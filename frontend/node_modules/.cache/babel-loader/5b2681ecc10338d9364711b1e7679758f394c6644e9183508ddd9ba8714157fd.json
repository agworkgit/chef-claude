{"ast":null,"code":"const taskData = {\n  datasets: [{\n    description: \"Microsoft Research Video to Text is a large-scale dataset for open domain video captioning\",\n    id: \"iejMac/CLIP-MSR-VTT\"\n  }, {\n    description: \"UCF101 Human Actions dataset consists of 13,320 video clips from YouTube, with 101 classes.\",\n    id: \"quchenyuan/UCF101-ZIP\"\n  }, {\n    description: \"A high-quality dataset for human action recognition in YouTube videos.\",\n    id: \"nateraw/kinetics\"\n  }, {\n    description: \"A dataset of video clips of humans performing pre-defined basic actions with everyday objects.\",\n    id: \"HuggingFaceM4/something_something_v2\"\n  }, {\n    description: \"This dataset consists of text-video pairs and contains noisy samples with irrelevant video descriptions\",\n    id: \"HuggingFaceM4/webvid\"\n  }, {\n    description: \"A dataset of short Flickr videos for the temporal localization of events with descriptions.\",\n    id: \"iejMac/CLIP-DiDeMo\"\n  }],\n  demo: {\n    inputs: [{\n      label: \"Input\",\n      content: \"Darth Vader is surfing on the waves.\",\n      type: \"text\"\n    }],\n    outputs: [{\n      filename: \"text-to-video-output.gif\",\n      type: \"img\"\n    }]\n  },\n  metrics: [{\n    description: \"Inception Score uses an image classification model that predicts class labels and evaluates how distinct and diverse the images are. A higher score indicates better video generation.\",\n    id: \"is\"\n  }, {\n    description: \"Frechet Inception Distance uses an image classification model to obtain image embeddings. The metric compares mean and standard deviation of the embeddings of real and generated images. A smaller score indicates better video generation.\",\n    id: \"fid\"\n  }, {\n    description: \"Frechet Video Distance uses a model that captures coherence for changes in frames and the quality of each frame. A smaller score indicates better video generation.\",\n    id: \"fvd\"\n  }, {\n    description: \"CLIPSIM measures similarity between video frames and text using an image-text similarity model. A higher score indicates better video generation.\",\n    id: \"clipsim\"\n  }],\n  models: [{\n    description: \"A strong model for consistent video generation.\",\n    id: \"tencent/HunyuanVideo\"\n  }, {\n    description: \"A text-to-video model with high fidelity motion and strong prompt adherence.\",\n    id: \"Lightricks/LTX-Video\"\n  }, {\n    description: \"A text-to-video model focusing on physics-aware applications like robotics.\",\n    id: \"nvidia/Cosmos-1.0-Diffusion-7B-Text2World\"\n  }, {\n    description: \"Very fast model for video generation.\",\n    id: \"Lightricks/LTX-Video-0.9.8-13B-distilled\"\n  }],\n  spaces: [{\n    description: \"An application that generates video from text.\",\n    id: \"VideoCrafter/VideoCrafter\"\n  }, {\n    description: \"Consistent video generation application.\",\n    id: \"Wan-AI/Wan2.1\"\n  }, {\n    description: \"A cutting edge video generation application.\",\n    id: \"Pyramid-Flow/pyramid-flow\"\n  }],\n  summary: \"Text-to-video models can be used in any application that requires generating consistent sequence of images from text. \",\n  widgetModels: [\"Wan-AI/Wan2.2-TI2V-5B\"],\n  youtubeId: undefined\n};\nexport default taskData;","map":{"version":3,"names":["taskData","datasets","description","id","demo","inputs","label","content","type","outputs","filename","metrics","models","spaces","summary","widgetModels","youtubeId","undefined"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/tasks/text-to-video/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Microsoft Research Video to Text is a large-scale dataset for open domain video captioning\",\n            id: \"iejMac/CLIP-MSR-VTT\",\n        },\n        {\n            description: \"UCF101 Human Actions dataset consists of 13,320 video clips from YouTube, with 101 classes.\",\n            id: \"quchenyuan/UCF101-ZIP\",\n        },\n        {\n            description: \"A high-quality dataset for human action recognition in YouTube videos.\",\n            id: \"nateraw/kinetics\",\n        },\n        {\n            description: \"A dataset of video clips of humans performing pre-defined basic actions with everyday objects.\",\n            id: \"HuggingFaceM4/something_something_v2\",\n        },\n        {\n            description: \"This dataset consists of text-video pairs and contains noisy samples with irrelevant video descriptions\",\n            id: \"HuggingFaceM4/webvid\",\n        },\n        {\n            description: \"A dataset of short Flickr videos for the temporal localization of events with descriptions.\",\n            id: \"iejMac/CLIP-DiDeMo\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"Darth Vader is surfing on the waves.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                filename: \"text-to-video-output.gif\",\n                type: \"img\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Inception Score uses an image classification model that predicts class labels and evaluates how distinct and diverse the images are. A higher score indicates better video generation.\",\n            id: \"is\",\n        },\n        {\n            description: \"Frechet Inception Distance uses an image classification model to obtain image embeddings. The metric compares mean and standard deviation of the embeddings of real and generated images. A smaller score indicates better video generation.\",\n            id: \"fid\",\n        },\n        {\n            description: \"Frechet Video Distance uses a model that captures coherence for changes in frames and the quality of each frame. A smaller score indicates better video generation.\",\n            id: \"fvd\",\n        },\n        {\n            description: \"CLIPSIM measures similarity between video frames and text using an image-text similarity model. A higher score indicates better video generation.\",\n            id: \"clipsim\",\n        },\n    ],\n    models: [\n        {\n            description: \"A strong model for consistent video generation.\",\n            id: \"tencent/HunyuanVideo\",\n        },\n        {\n            description: \"A text-to-video model with high fidelity motion and strong prompt adherence.\",\n            id: \"Lightricks/LTX-Video\",\n        },\n        {\n            description: \"A text-to-video model focusing on physics-aware applications like robotics.\",\n            id: \"nvidia/Cosmos-1.0-Diffusion-7B-Text2World\",\n        },\n        {\n            description: \"Very fast model for video generation.\",\n            id: \"Lightricks/LTX-Video-0.9.8-13B-distilled\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that generates video from text.\",\n            id: \"VideoCrafter/VideoCrafter\",\n        },\n        {\n            description: \"Consistent video generation application.\",\n            id: \"Wan-AI/Wan2.1\",\n        },\n        {\n            description: \"A cutting edge video generation application.\",\n            id: \"Pyramid-Flow/pyramid-flow\",\n        },\n    ],\n    summary: \"Text-to-video models can be used in any application that requires generating consistent sequence of images from text. \",\n    widgetModels: [\"Wan-AI/Wan2.2-TI2V-5B\"],\n    youtubeId: undefined,\n};\nexport default taskData;\n"],"mappings":"AAAA,MAAMA,QAAQ,GAAG;EACbC,QAAQ,EAAE,CACN;IACIC,WAAW,EAAE,4FAA4F;IACzGC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,6FAA6F;IAC1GC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,wEAAwE;IACrFC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,gGAAgG;IAC7GC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,yGAAyG;IACtHC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,6FAA6F;IAC1GC,EAAE,EAAE;EACR,CAAC,CACJ;EACDC,IAAI,EAAE;IACFC,MAAM,EAAE,CACJ;MACIC,KAAK,EAAE,OAAO;MACdC,OAAO,EAAE,sCAAsC;MAC/CC,IAAI,EAAE;IACV,CAAC,CACJ;IACDC,OAAO,EAAE,CACL;MACIC,QAAQ,EAAE,0BAA0B;MACpCF,IAAI,EAAE;IACV,CAAC;EAET,CAAC;EACDG,OAAO,EAAE,CACL;IACIT,WAAW,EAAE,wLAAwL;IACrMC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,8OAA8O;IAC3PC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,qKAAqK;IAClLC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,mJAAmJ;IAChKC,EAAE,EAAE;EACR,CAAC,CACJ;EACDS,MAAM,EAAE,CACJ;IACIV,WAAW,EAAE,iDAAiD;IAC9DC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,8EAA8E;IAC3FC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,6EAA6E;IAC1FC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,uCAAuC;IACpDC,EAAE,EAAE;EACR,CAAC,CACJ;EACDU,MAAM,EAAE,CACJ;IACIX,WAAW,EAAE,gDAAgD;IAC7DC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,0CAA0C;IACvDC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,8CAA8C;IAC3DC,EAAE,EAAE;EACR,CAAC,CACJ;EACDW,OAAO,EAAE,wHAAwH;EACjIC,YAAY,EAAE,CAAC,uBAAuB,CAAC;EACvCC,SAAS,EAAEC;AACf,CAAC;AACD,eAAejB,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}