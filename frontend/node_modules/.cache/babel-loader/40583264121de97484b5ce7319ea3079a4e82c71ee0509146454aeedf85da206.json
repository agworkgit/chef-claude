{"ast":null,"code":"const taskData = {\n  datasets: [{\n    description: \"A widely used dataset containing questions (with answers) about images.\",\n    id: \"Graphcore/vqa\"\n  }, {\n    description: \"A dataset to benchmark visual reasoning based on text in images.\",\n    id: \"facebook/textvqa\"\n  }],\n  demo: {\n    inputs: [{\n      filename: \"elephant.jpeg\",\n      type: \"img\"\n    }, {\n      label: \"Question\",\n      content: \"What is in this image?\",\n      type: \"text\"\n    }],\n    outputs: [{\n      type: \"chart\",\n      data: [{\n        label: \"elephant\",\n        score: 0.97\n      }, {\n        label: \"elephants\",\n        score: 0.06\n      }, {\n        label: \"animal\",\n        score: 0.003\n      }]\n    }]\n  },\n  isPlaceholder: false,\n  metrics: [{\n    description: \"\",\n    id: \"accuracy\"\n  }, {\n    description: \"Measures how much a predicted answer differs from the ground truth based on the difference in their semantic meaning.\",\n    id: \"wu-palmer similarity\"\n  }],\n  models: [{\n    description: \"A visual question answering model trained to convert charts and plots to text.\",\n    id: \"google/deplot\"\n  }, {\n    description: \"A visual question answering model trained for mathematical reasoning and chart derendering from images.\",\n    id: \"google/matcha-base\"\n  }, {\n    description: \"A strong visual question answering that answers questions from book covers.\",\n    id: \"google/pix2struct-ocrvqa-large\"\n  }],\n  spaces: [{\n    description: \"An application that compares visual question answering models across different tasks.\",\n    id: \"merve/pix2struct\"\n  }, {\n    description: \"An application that can answer questions based on images.\",\n    id: \"nielsr/vilt-vqa\"\n  }, {\n    description: \"An application that can caption images and answer questions about a given image. \",\n    id: \"Salesforce/BLIP\"\n  }, {\n    description: \"An application that can caption images and answer questions about a given image. \",\n    id: \"vumichien/Img2Prompt\"\n  }],\n  summary: \"Visual Question Answering is the task of answering open-ended questions based on an image. They output natural language responses to natural language questions.\",\n  widgetModels: [\"dandelin/vilt-b32-finetuned-vqa\"],\n  youtubeId: \"\"\n};\nexport default taskData;","map":{"version":3,"names":["taskData","datasets","description","id","demo","inputs","filename","type","label","content","outputs","data","score","isPlaceholder","metrics","models","spaces","summary","widgetModels","youtubeId"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/tasks/visual-question-answering/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A widely used dataset containing questions (with answers) about images.\",\n            id: \"Graphcore/vqa\",\n        },\n        {\n            description: \"A dataset to benchmark visual reasoning based on text in images.\",\n            id: \"facebook/textvqa\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"elephant.jpeg\",\n                type: \"img\",\n            },\n            {\n                label: \"Question\",\n                content: \"What is in this image?\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"elephant\",\n                        score: 0.97,\n                    },\n                    {\n                        label: \"elephants\",\n                        score: 0.06,\n                    },\n                    {\n                        label: \"animal\",\n                        score: 0.003,\n                    },\n                ],\n            },\n        ],\n    },\n    isPlaceholder: false,\n    metrics: [\n        {\n            description: \"\",\n            id: \"accuracy\",\n        },\n        {\n            description: \"Measures how much a predicted answer differs from the ground truth based on the difference in their semantic meaning.\",\n            id: \"wu-palmer similarity\",\n        },\n    ],\n    models: [\n        {\n            description: \"A visual question answering model trained to convert charts and plots to text.\",\n            id: \"google/deplot\",\n        },\n        {\n            description: \"A visual question answering model trained for mathematical reasoning and chart derendering from images.\",\n            id: \"google/matcha-base\",\n        },\n        {\n            description: \"A strong visual question answering that answers questions from book covers.\",\n            id: \"google/pix2struct-ocrvqa-large\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that compares visual question answering models across different tasks.\",\n            id: \"merve/pix2struct\",\n        },\n        {\n            description: \"An application that can answer questions based on images.\",\n            id: \"nielsr/vilt-vqa\",\n        },\n        {\n            description: \"An application that can caption images and answer questions about a given image. \",\n            id: \"Salesforce/BLIP\",\n        },\n        {\n            description: \"An application that can caption images and answer questions about a given image. \",\n            id: \"vumichien/Img2Prompt\",\n        },\n    ],\n    summary: \"Visual Question Answering is the task of answering open-ended questions based on an image. They output natural language responses to natural language questions.\",\n    widgetModels: [\"dandelin/vilt-b32-finetuned-vqa\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"mappings":"AAAA,MAAMA,QAAQ,GAAG;EACbC,QAAQ,EAAE,CACN;IACIC,WAAW,EAAE,yEAAyE;IACtFC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,kEAAkE;IAC/EC,EAAE,EAAE;EACR,CAAC,CACJ;EACDC,IAAI,EAAE;IACFC,MAAM,EAAE,CACJ;MACIC,QAAQ,EAAE,eAAe;MACzBC,IAAI,EAAE;IACV,CAAC,EACD;MACIC,KAAK,EAAE,UAAU;MACjBC,OAAO,EAAE,wBAAwB;MACjCF,IAAI,EAAE;IACV,CAAC,CACJ;IACDG,OAAO,EAAE,CACL;MACIH,IAAI,EAAE,OAAO;MACbI,IAAI,EAAE,CACF;QACIH,KAAK,EAAE,UAAU;QACjBI,KAAK,EAAE;MACX,CAAC,EACD;QACIJ,KAAK,EAAE,WAAW;QAClBI,KAAK,EAAE;MACX,CAAC,EACD;QACIJ,KAAK,EAAE,QAAQ;QACfI,KAAK,EAAE;MACX,CAAC;IAET,CAAC;EAET,CAAC;EACDC,aAAa,EAAE,KAAK;EACpBC,OAAO,EAAE,CACL;IACIZ,WAAW,EAAE,EAAE;IACfC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,uHAAuH;IACpIC,EAAE,EAAE;EACR,CAAC,CACJ;EACDY,MAAM,EAAE,CACJ;IACIb,WAAW,EAAE,gFAAgF;IAC7FC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,yGAAyG;IACtHC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,6EAA6E;IAC1FC,EAAE,EAAE;EACR,CAAC,CACJ;EACDa,MAAM,EAAE,CACJ;IACId,WAAW,EAAE,uFAAuF;IACpGC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,2DAA2D;IACxEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,mFAAmF;IAChGC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,mFAAmF;IAChGC,EAAE,EAAE;EACR,CAAC,CACJ;EACDc,OAAO,EAAE,kKAAkK;EAC3KC,YAAY,EAAE,CAAC,iCAAiC,CAAC;EACjDC,SAAS,EAAE;AACf,CAAC;AACD,eAAenB,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}