{"ast":null,"code":"const taskData = {\n  datasets: [{\n    description: \"A common dataset that is used to train models for many languages.\",\n    id: \"wikipedia\"\n  }, {\n    description: \"A large English dataset with text crawled from the web.\",\n    id: \"c4\"\n  }],\n  demo: {\n    inputs: [{\n      label: \"Input\",\n      content: \"The <mask> barked at me\",\n      type: \"text\"\n    }],\n    outputs: [{\n      type: \"chart\",\n      data: [{\n        label: \"wolf\",\n        score: 0.487\n      }, {\n        label: \"dog\",\n        score: 0.061\n      }, {\n        label: \"cat\",\n        score: 0.058\n      }, {\n        label: \"fox\",\n        score: 0.047\n      }, {\n        label: \"squirrel\",\n        score: 0.025\n      }]\n    }]\n  },\n  metrics: [{\n    description: \"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words\",\n    id: \"cross_entropy\"\n  }, {\n    description: \"Perplexity is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance\",\n    id: \"perplexity\"\n  }],\n  models: [{\n    description: \"State-of-the-art masked language model.\",\n    id: \"answerdotai/ModernBERT-large\"\n  }, {\n    description: \"A multilingual model trained on 100 languages.\",\n    id: \"FacebookAI/xlm-roberta-base\"\n  }],\n  spaces: [],\n  summary: \"Masked language modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. These models are useful when we want to get a statistical understanding of the language in which the model is trained in.\",\n  widgetModels: [\"distilroberta-base\"],\n  youtubeId: \"mqElG5QJWUg\"\n};\nexport default taskData;","map":{"version":3,"names":["taskData","datasets","description","id","demo","inputs","label","content","type","outputs","data","score","metrics","models","spaces","summary","widgetModels","youtubeId"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/tasks/fill-mask/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A common dataset that is used to train models for many languages.\",\n            id: \"wikipedia\",\n        },\n        {\n            description: \"A large English dataset with text crawled from the web.\",\n            id: \"c4\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Input\",\n                content: \"The <mask> barked at me\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"wolf\",\n                        score: 0.487,\n                    },\n                    {\n                        label: \"dog\",\n                        score: 0.061,\n                    },\n                    {\n                        label: \"cat\",\n                        score: 0.058,\n                    },\n                    {\n                        label: \"fox\",\n                        score: 0.047,\n                    },\n                    {\n                        label: \"squirrel\",\n                        score: 0.025,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Cross Entropy is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words\",\n            id: \"cross_entropy\",\n        },\n        {\n            description: \"Perplexity is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance\",\n            id: \"perplexity\",\n        },\n    ],\n    models: [\n        {\n            description: \"State-of-the-art masked language model.\",\n            id: \"answerdotai/ModernBERT-large\",\n        },\n        {\n            description: \"A multilingual model trained on 100 languages.\",\n            id: \"FacebookAI/xlm-roberta-base\",\n        },\n    ],\n    spaces: [],\n    summary: \"Masked language modeling is the task of masking some of the words in a sentence and predicting which words should replace those masks. These models are useful when we want to get a statistical understanding of the language in which the model is trained in.\",\n    widgetModels: [\"distilroberta-base\"],\n    youtubeId: \"mqElG5QJWUg\",\n};\nexport default taskData;\n"],"mappings":"AAAA,MAAMA,QAAQ,GAAG;EACbC,QAAQ,EAAE,CACN;IACIC,WAAW,EAAE,mEAAmE;IAChFC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,yDAAyD;IACtEC,EAAE,EAAE;EACR,CAAC,CACJ;EACDC,IAAI,EAAE;IACFC,MAAM,EAAE,CACJ;MACIC,KAAK,EAAE,OAAO;MACdC,OAAO,EAAE,yBAAyB;MAClCC,IAAI,EAAE;IACV,CAAC,CACJ;IACDC,OAAO,EAAE,CACL;MACID,IAAI,EAAE,OAAO;MACbE,IAAI,EAAE,CACF;QACIJ,KAAK,EAAE,MAAM;QACbK,KAAK,EAAE;MACX,CAAC,EACD;QACIL,KAAK,EAAE,KAAK;QACZK,KAAK,EAAE;MACX,CAAC,EACD;QACIL,KAAK,EAAE,KAAK;QACZK,KAAK,EAAE;MACX,CAAC,EACD;QACIL,KAAK,EAAE,KAAK;QACZK,KAAK,EAAE;MACX,CAAC,EACD;QACIL,KAAK,EAAE,UAAU;QACjBK,KAAK,EAAE;MACX,CAAC;IAET,CAAC;EAET,CAAC;EACDC,OAAO,EAAE,CACL;IACIV,WAAW,EAAE,sKAAsK;IACnLC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,+KAA+K;IAC5LC,EAAE,EAAE;EACR,CAAC,CACJ;EACDU,MAAM,EAAE,CACJ;IACIX,WAAW,EAAE,yCAAyC;IACtDC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,gDAAgD;IAC7DC,EAAE,EAAE;EACR,CAAC,CACJ;EACDW,MAAM,EAAE,EAAE;EACVC,OAAO,EAAE,kQAAkQ;EAC3QC,YAAY,EAAE,CAAC,oBAAoB,CAAC;EACpCC,SAAS,EAAE;AACf,CAAC;AACD,eAAejB,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}