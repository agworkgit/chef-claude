{"ast":null,"code":"const taskData = {\n  datasets: [{\n    description: \"Largest document understanding dataset.\",\n    id: \"HuggingFaceM4/Docmatix\"\n  }, {\n    description: \"Dataset from the 2020 DocVQA challenge. The documents are taken from the UCSF Industry Documents Library.\",\n    id: \"eliolio/docvqa\"\n  }],\n  demo: {\n    inputs: [{\n      label: \"Question\",\n      content: \"What is the idea behind the consumer relations efficiency team?\",\n      type: \"text\"\n    }, {\n      filename: \"document-question-answering-input.png\",\n      type: \"img\"\n    }],\n    outputs: [{\n      label: \"Answer\",\n      content: \"Balance cost efficiency with quality customer service\",\n      type: \"text\"\n    }]\n  },\n  metrics: [{\n    description: \"The evaluation metric for the DocVQA challenge is the Average Normalized Levenshtein Similarity (ANLS). This metric is flexible to character regognition errors and compares the predicted answer with the ground truth answer.\",\n    id: \"anls\"\n  }, {\n    description: \"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0\",\n    id: \"exact-match\"\n  }],\n  models: [{\n    description: \"A robust document question answering model.\",\n    id: \"impira/layoutlm-document-qa\"\n  }, {\n    description: \"A document question answering model specialized in invoices.\",\n    id: \"impira/layoutlm-invoices\"\n  }, {\n    description: \"A special model for OCR-free document question answering.\",\n    id: \"microsoft/udop-large\"\n  }, {\n    description: \"A powerful model for document question answering.\",\n    id: \"google/pix2struct-docvqa-large\"\n  }],\n  spaces: [{\n    description: \"A robust document question answering application.\",\n    id: \"impira/docquery\"\n  }, {\n    description: \"An application that can answer questions from invoices.\",\n    id: \"impira/invoices\"\n  }, {\n    description: \"An application to compare different document question answering models.\",\n    id: \"merve/compare_docvqa_models\"\n  }],\n  summary: \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\",\n  widgetModels: [\"impira/layoutlm-invoices\"],\n  youtubeId: \"\"\n};\nexport default taskData;","map":{"version":3,"names":["taskData","datasets","description","id","demo","inputs","label","content","type","filename","outputs","metrics","models","spaces","summary","widgetModels","youtubeId"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/tasks/document-question-answering/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Largest document understanding dataset.\",\n            id: \"HuggingFaceM4/Docmatix\",\n        },\n        {\n            description: \"Dataset from the 2020 DocVQA challenge. The documents are taken from the UCSF Industry Documents Library.\",\n            id: \"eliolio/docvqa\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Question\",\n                content: \"What is the idea behind the consumer relations efficiency team?\",\n                type: \"text\",\n            },\n            {\n                filename: \"document-question-answering-input.png\",\n                type: \"img\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Answer\",\n                content: \"Balance cost efficiency with quality customer service\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"The evaluation metric for the DocVQA challenge is the Average Normalized Levenshtein Similarity (ANLS). This metric is flexible to character regognition errors and compares the predicted answer with the ground truth answer.\",\n            id: \"anls\",\n        },\n        {\n            description: \"Exact Match is a metric based on the strict character match of the predicted answer and the right answer. For answers predicted correctly, the Exact Match will be 1. Even if only one character is different, Exact Match will be 0\",\n            id: \"exact-match\",\n        },\n    ],\n    models: [\n        {\n            description: \"A robust document question answering model.\",\n            id: \"impira/layoutlm-document-qa\",\n        },\n        {\n            description: \"A document question answering model specialized in invoices.\",\n            id: \"impira/layoutlm-invoices\",\n        },\n        {\n            description: \"A special model for OCR-free document question answering.\",\n            id: \"microsoft/udop-large\",\n        },\n        {\n            description: \"A powerful model for document question answering.\",\n            id: \"google/pix2struct-docvqa-large\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"A robust document question answering application.\",\n            id: \"impira/docquery\",\n        },\n        {\n            description: \"An application that can answer questions from invoices.\",\n            id: \"impira/invoices\",\n        },\n        {\n            description: \"An application to compare different document question answering models.\",\n            id: \"merve/compare_docvqa_models\",\n        },\n    ],\n    summary: \"Document Question Answering (also known as Document Visual Question Answering) is the task of answering questions on document images. Document question answering models take a (document, question) pair as input and return an answer in natural language. Models usually rely on multi-modal features, combining text, position of words (bounding-boxes) and image.\",\n    widgetModels: [\"impira/layoutlm-invoices\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"mappings":"AAAA,MAAMA,QAAQ,GAAG;EACbC,QAAQ,EAAE,CACN;IACIC,WAAW,EAAE,yCAAyC;IACtDC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,2GAA2G;IACxHC,EAAE,EAAE;EACR,CAAC,CACJ;EACDC,IAAI,EAAE;IACFC,MAAM,EAAE,CACJ;MACIC,KAAK,EAAE,UAAU;MACjBC,OAAO,EAAE,iEAAiE;MAC1EC,IAAI,EAAE;IACV,CAAC,EACD;MACIC,QAAQ,EAAE,uCAAuC;MACjDD,IAAI,EAAE;IACV,CAAC,CACJ;IACDE,OAAO,EAAE,CACL;MACIJ,KAAK,EAAE,QAAQ;MACfC,OAAO,EAAE,uDAAuD;MAChEC,IAAI,EAAE;IACV,CAAC;EAET,CAAC;EACDG,OAAO,EAAE,CACL;IACIT,WAAW,EAAE,iOAAiO;IAC9OC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,sOAAsO;IACnPC,EAAE,EAAE;EACR,CAAC,CACJ;EACDS,MAAM,EAAE,CACJ;IACIV,WAAW,EAAE,6CAA6C;IAC1DC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,8DAA8D;IAC3EC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,2DAA2D;IACxEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,mDAAmD;IAChEC,EAAE,EAAE;EACR,CAAC,CACJ;EACDU,MAAM,EAAE,CACJ;IACIX,WAAW,EAAE,mDAAmD;IAChEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,yDAAyD;IACtEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,yEAAyE;IACtFC,EAAE,EAAE;EACR,CAAC,CACJ;EACDW,OAAO,EAAE,yWAAyW;EAClXC,YAAY,EAAE,CAAC,0BAA0B,CAAC;EAC1CC,SAAS,EAAE;AACf,CAAC;AACD,eAAehB,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}