{"ast":null,"code":"import { LIBRARY_TASK_MAPPING } from \"./library-to-tasks.js\";\nimport { getModelInputSnippet } from \"./snippets/inputs.js\";\nimport { stringifyMessages } from \"./snippets/common.js\";\nconst TAG_CUSTOM_CODE = \"custom_code\";\nfunction nameWithoutNamespace(modelId) {\n  const splitted = modelId.split(\"/\");\n  return splitted.length === 1 ? splitted[0] : splitted[1];\n}\nconst escapeStringForJson = str => JSON.stringify(str).slice(1, -1); // slice is needed to remove surrounding quotes added by JSON.stringify\n//#region snippets\nexport const adapters = model => [`from adapters import AutoAdapterModel\n\nmodel = AutoAdapterModel.from_pretrained(\"${model.config?.adapter_transformers?.model_name}\")\nmodel.load_adapter(\"${model.id}\", set_active=True)`];\nconst allennlpUnknown = model => [`import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path(\"hf://${model.id}\")`];\nconst allennlpQuestionAnswering = model => [`import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path(\"hf://${model.id}\")\npredictor_input = {\"passage\": \"My name is Wolfgang and I live in Berlin\", \"question\": \"Where do I live?\"}\npredictions = predictor.predict_json(predictor_input)`];\nexport const allennlp = model => {\n  if (model.tags.includes(\"question-answering\")) {\n    return allennlpQuestionAnswering(model);\n  }\n  return allennlpUnknown(model);\n};\nexport const araclip = model => [`from araclip import AraClip\n\nmodel = AraClip.from_pretrained(\"${model.id}\")`];\nexport const asteroid = model => [`from asteroid.models import BaseModel\n\nmodel = BaseModel.from_pretrained(\"${model.id}\")`];\nexport const audioseal = model => {\n  const watermarkSnippet = `# Watermark Generator\nfrom audioseal import AudioSeal\n\nmodel = AudioSeal.load_generator(\"${model.id}\")\n# pass a tensor (tensor_wav) of shape (batch, channels, samples) and a sample rate\nwav, sr = tensor_wav, 16000\n\t\nwatermark = model.get_watermark(wav, sr)\nwatermarked_audio = wav + watermark`;\n  const detectorSnippet = `# Watermark Detector\nfrom audioseal import AudioSeal\n\ndetector = AudioSeal.load_detector(\"${model.id}\")\n\t\nresult, message = detector.detect_watermark(watermarked_audio, sr)`;\n  return [watermarkSnippet, detectorSnippet];\n};\nfunction get_base_diffusers_model(model) {\n  return model.cardData?.base_model?.toString() ?? \"fill-in-base-model\";\n}\nfunction get_prompt_from_diffusers_model(model) {\n  const prompt = model.widgetData?.[0]?.text ?? model.cardData?.instance_prompt;\n  if (prompt) {\n    return escapeStringForJson(prompt);\n  }\n}\nexport const ben2 = model => [`import requests\nfrom PIL import Image\nfrom ben2 import AutoModel\n\nurl = \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = AutoModel.from_pretrained(\"${model.id}\")\nmodel.to(\"cuda\").eval()\nforeground = model.inference(image)\n`];\nexport const bertopic = model => [`from bertopic import BERTopic\n\nmodel = BERTopic.load(\"${model.id}\")`];\nexport const bm25s = model => [`from bm25s.hf import BM25HF\n\nretriever = BM25HF.load_from_hub(\"${model.id}\")`];\nexport const chatterbox = () => [`# pip install chatterbox-tts\nimport torchaudio as ta\nfrom chatterbox.tts import ChatterboxTTS\n\nmodel = ChatterboxTTS.from_pretrained(device=\"cuda\")\n\ntext = \"Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill.\"\nwav = model.generate(text)\nta.save(\"test-1.wav\", wav, model.sr)\n\n# If you want to synthesize with a different voice, specify the audio prompt\nAUDIO_PROMPT_PATH=\"YOUR_FILE.wav\"\nwav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)\nta.save(\"test-2.wav\", wav, model.sr)`];\nexport const contexttab = () => {\n  const installSnippet = `pip install git+https://github.com/SAP-samples/contexttab`;\n  const classificationSnippet = `# Run a classification task\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contexttab import ConTextTabClassifier\n\n# Load sample data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Initialize a classifier\n# You can omit checkpoint and checkpoint_revision to use the default model\nclf = ConTextTabClassifier(checkpoint=\"l2/base.pt\", checkpoint_revision=\"v1.0.0\", bagging=1, max_context_size=2048)\n\nclf.fit(X_train, y_train)\n\n# Predict probabilities\nprediction_probabilities = clf.predict_proba(X_test)\n# Predict labels\npredictions = clf.predict(X_test)\nprint(\"Accuracy\", accuracy_score(y_test, predictions))`;\n  const regressionsSnippet = `# Run a regression task\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contexttab import ConTextTabRegressor\n\n\n# Load sample data\ndf = fetch_openml(data_id=531, as_frame=True)\nX = df.data\ny = df.target.astype(float)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Initialize the regressor\n# You can omit checkpoint and checkpoint_revision to use the default model\nregressor = ConTextTabRegressor(checkpoint=\"l2/base.pt\", checkpoint_revision=\"v1.0.0\", bagging=1, max_context_size=2048)\n\nregressor.fit(X_train, y_train)\n\n# Predict on the test set\npredictions = regressor.predict(X_test)\n\nr2 = r2_score(y_test, predictions)\nprint(\"R² Score:\", r2)`;\n  return [installSnippet, classificationSnippet, regressionsSnippet];\n};\nexport const cxr_foundation = () => [`# pip install git+https://github.com/Google-Health/cxr-foundation.git#subdirectory=python\n\n# Load image as grayscale (Stillwaterising, CC0, via Wikimedia Commons)\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimg = Image.open(requests.get(image_url, headers={'User-Agent': 'Demo'}, stream=True).raw).convert('L')\n\n# Run inference\nfrom clientside.clients import make_hugging_face_client\ncxr_client = make_hugging_face_client('cxr_model')\nprint(cxr_client.get_image_embeddings_from_images([img]))`];\nexport const depth_anything_v2 = model => {\n  let encoder;\n  let features;\n  let out_channels;\n  encoder = \"<ENCODER>\";\n  features = \"<NUMBER_OF_FEATURES>\";\n  out_channels = \"<OUT_CHANNELS>\";\n  if (model.id === \"depth-anything/Depth-Anything-V2-Small\") {\n    encoder = \"vits\";\n    features = \"64\";\n    out_channels = \"[48, 96, 192, 384]\";\n  } else if (model.id === \"depth-anything/Depth-Anything-V2-Base\") {\n    encoder = \"vitb\";\n    features = \"128\";\n    out_channels = \"[96, 192, 384, 768]\";\n  } else if (model.id === \"depth-anything/Depth-Anything-V2-Large\") {\n    encoder = \"vitl\";\n    features = \"256\";\n    out_channels = \"[256, 512, 1024, 1024\";\n  }\n  return [`\n# Install from https://github.com/DepthAnything/Depth-Anything-V2\n\n# Load the model and infer depth from an image\nimport cv2\nimport torch\n\nfrom depth_anything_v2.dpt import DepthAnythingV2\n\n# instantiate the model\nmodel = DepthAnythingV2(encoder=\"${encoder}\", features=${features}, out_channels=${out_channels})\n\n# load the weights\nfilepath = hf_hub_download(repo_id=\"${model.id}\", filename=\"depth_anything_v2_${encoder}.pth\", repo_type=\"model\")\nstate_dict = torch.load(filepath, map_location=\"cpu\")\nmodel.load_state_dict(state_dict).eval()\n\nraw_img = cv2.imread(\"your/image/path\")\ndepth = model.infer_image(raw_img) # HxW raw depth map in numpy\n    `];\n};\nexport const depth_pro = model => {\n  const installSnippet = `# Download checkpoint\npip install huggingface-hub\nhuggingface-cli download --local-dir checkpoints ${model.id}`;\n  const inferenceSnippet = `import depth_pro\n\n# Load model and preprocessing transform\nmodel, transform = depth_pro.create_model_and_transforms()\nmodel.eval()\n\n# Load and preprocess an image.\nimage, _, f_px = depth_pro.load_rgb(\"example.png\")\nimage = transform(image)\n\n# Run inference.\nprediction = model.infer(image, f_px=f_px)\n\n# Results: 1. Depth in meters\ndepth = prediction[\"depth\"]\n# Results: 2. Focal length in pixels\nfocallength_px = prediction[\"focallength_px\"]`;\n  return [installSnippet, inferenceSnippet];\n};\nexport const derm_foundation = () => [`from huggingface_hub import from_pretrained_keras\nimport tensorflow as tf, requests\n\n# Load and format input\nIMAGE_URL = \"https://storage.googleapis.com/dx-scin-public-data/dataset/images/3445096909671059178.png\"\ninput_tensor = tf.train.Example(\n    features=tf.train.Features(\n        feature={\n            \"image/encoded\": tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[requests.get(IMAGE_URL, stream=True).content])\n            )\n        }\n    )\n).SerializeToString()\n\n# Load model and run inference\nloaded_model = from_pretrained_keras(\"google/derm-foundation\")\ninfer = loaded_model.signatures[\"serving_default\"]\nprint(infer(inputs=tf.constant([input_tensor])))`];\nexport const dia = model => [`import soundfile as sf\nfrom dia.model import Dia\n\nmodel = Dia.from_pretrained(\"${model.id}\")\ntext = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\noutput = model.generate(text)\n\nsf.write(\"simple.mp3\", output, 44100)`];\nexport const describe_anything = model => [`# pip install git+https://github.com/NVlabs/describe-anything\nfrom huggingface_hub import snapshot_download\nfrom dam import DescribeAnythingModel\n\nsnapshot_download(${model.id}, local_dir=\"checkpoints\")\n\ndam = DescribeAnythingModel(\n\tmodel_path=\"checkpoints\",\n\tconv_mode=\"v1\",\n\tprompt_mode=\"focal_prompt\",\n)`];\nconst diffusers_install = \"pip install -U diffusers transformers\";\nconst diffusersDefaultPrompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\";\nconst diffusersImg2ImgDefaultPrompt = \"Turn this cat into a dog\";\nconst diffusersVideoDefaultPrompt = \"A man with short gray hair plays a red electric guitar.\";\nconst diffusers_default = model => [`from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersDefaultPrompt}\"\nimage = pipe(prompt).images[0]`];\nconst diffusers_image_to_image = model => [`from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersImg2ImgDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n\nimage = pipe(image=input_image, prompt=prompt).images[0]`];\nconst diffusers_image_to_video = model => [`import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png\"\n)\n\noutput = pipe(image=image, prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`];\nconst diffusers_controlnet = model => [`from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n\ncontrolnet = ControlNetModel.from_pretrained(\"${model.id}\")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\t\"${get_base_diffusers_model(model)}\", controlnet=controlnet\n)`];\nconst diffusers_lora = model => [`from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersDefaultPrompt}\"\nimage = pipe(prompt).images[0]`];\nconst diffusers_lora_image_to_image = model => [`from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersImg2ImgDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n\nimage = pipe(image=input_image, prompt=prompt).images[0]`];\nconst diffusers_lora_text_to_video = model => [`from diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\n\noutput = pipe(prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`];\nconst diffusers_lora_image_to_video = model => [`from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png\")\n\nimage = pipe(image=input_image, prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`];\nconst diffusers_textual_inversion = model => [`from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_textual_inversion(\"${model.id}\")`];\nconst diffusers_flux_fill = model => [`import torch\nfrom diffusers import FluxFillPipeline\nfrom diffusers.utils import load_image\n\nimage = load_image(\"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup.png\")\nmask = load_image(\"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup_mask.png\")\n\npipe = FluxFillPipeline.from_pretrained(\"${model.id}\", torch_dtype=torch.bfloat16).to(\"cuda\")\nimage = pipe(\n    prompt=\"a white paper cup\",\n    image=image,\n    mask_image=mask,\n    height=1632,\n    width=1232,\n    guidance_scale=30,\n    num_inference_steps=50,\n    max_sequence_length=512,\n    generator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"flux-fill-dev.png\")`];\nconst diffusers_inpainting = model => [`import torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\n\npipe = AutoPipelineForInpainting.from_pretrained(\"${model.id}\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\nimage = load_image(img_url).resize((1024, 1024))\nmask_image = load_image(mask_url).resize((1024, 1024))\n\nprompt = \"a tiger sitting on a park bench\"\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\n\nimage = pipe(\n  prompt=prompt,\n  image=image,\n  mask_image=mask_image,\n  guidance_scale=8.0,\n  num_inference_steps=20,  # steps between 15 and 30 work well for us\n  strength=0.99,  # make sure to use \\`strength\\` below 1.0\n  generator=generator,\n).images[0]`];\nexport const diffusers = model => {\n  let codeSnippets;\n  if (model.tags.includes(\"StableDiffusionInpaintPipeline\") || model.tags.includes(\"StableDiffusionXLInpaintPipeline\")) {\n    codeSnippets = diffusers_inpainting(model);\n  } else if (model.tags.includes(\"controlnet\")) {\n    codeSnippets = diffusers_controlnet(model);\n  } else if (model.tags.includes(\"lora\")) {\n    if (model.pipeline_tag === \"image-to-image\") {\n      codeSnippets = diffusers_lora_image_to_image(model);\n    } else if (model.pipeline_tag === \"image-to-video\") {\n      codeSnippets = diffusers_lora_image_to_video(model);\n    } else if (model.pipeline_tag === \"text-to-video\") {\n      codeSnippets = diffusers_lora_text_to_video(model);\n    } else {\n      codeSnippets = diffusers_lora(model);\n    }\n  } else if (model.tags.includes(\"textual_inversion\")) {\n    codeSnippets = diffusers_textual_inversion(model);\n  } else if (model.tags.includes(\"FluxFillPipeline\")) {\n    codeSnippets = diffusers_flux_fill(model);\n  } else if (model.pipeline_tag === \"image-to-video\") {\n    codeSnippets = diffusers_image_to_video(model);\n  } else if (model.pipeline_tag === \"image-to-image\") {\n    codeSnippets = diffusers_image_to_image(model);\n  } else {\n    codeSnippets = diffusers_default(model);\n  }\n  return [diffusers_install, ...codeSnippets];\n};\nexport const diffusionkit = model => {\n  const sd3Snippet = `# Pipeline for Stable Diffusion 3\nfrom diffusionkit.mlx import DiffusionPipeline\n\npipeline = DiffusionPipeline(\n\tshift=3.0,\n\tuse_t5=False,\n\tmodel_version=${model.id},\n\tlow_memory_mode=True,\n\ta16=True,\n\tw16=True,\n)`;\n  const fluxSnippet = `# Pipeline for Flux\nfrom diffusionkit.mlx import FluxPipeline\n\npipeline = FluxPipeline(\n  shift=1.0,\n  model_version=${model.id},\n  low_memory_mode=True,\n  a16=True,\n  w16=True,\n)`;\n  const generateSnippet = `# Image Generation\nHEIGHT = 512\nWIDTH = 512\nNUM_STEPS = ${model.tags.includes(\"flux\") ? 4 : 50}\nCFG_WEIGHT = ${model.tags.includes(\"flux\") ? 0 : 5}\n\nimage, _ = pipeline.generate_image(\n  \"a photo of a cat\",\n  cfg_weight=CFG_WEIGHT,\n  num_steps=NUM_STEPS,\n  latent_size=(HEIGHT // 8, WIDTH // 8),\n)`;\n  const pipelineSnippet = model.tags.includes(\"flux\") ? fluxSnippet : sd3Snippet;\n  return [pipelineSnippet, generateSnippet];\n};\nexport const cartesia_pytorch = model => [`# pip install --no-binary :all: cartesia-pytorch\nfrom cartesia_pytorch import ReneLMHeadModel\nfrom transformers import AutoTokenizer\n\nmodel = ReneLMHeadModel.from_pretrained(\"${model.id}\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")\n\nin_message = [\"Rene Descartes was\"]\ninputs = tokenizer(in_message, return_tensors=\"pt\")\n\noutputs = model.generate(inputs.input_ids, max_length=50, top_k=100, top_p=0.99)\nout_message = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n\nprint(out_message)\n)`];\nexport const cartesia_mlx = model => [`import mlx.core as mx\nimport cartesia_mlx as cmx\n\nmodel = cmx.from_pretrained(\"${model.id}\")\nmodel.set_dtype(mx.float32)   \n\nprompt = \"Rene Descartes was\"\n\nfor text in model.generate(\n    prompt,\n    max_tokens=500,\n    eval_every_n=5,\n    verbose=True,\n    top_p=0.99,\n    temperature=0.85,\n):\n    print(text, end=\"\", flush=True)\n`];\nexport const edsnlp = model => {\n  const packageName = nameWithoutNamespace(model.id).replaceAll(\"-\", \"_\");\n  return [`# Load it from the Hub directly\nimport edsnlp\nnlp = edsnlp.load(\"${model.id}\")\n`, `# Or install it as a package\n!pip install git+https://huggingface.co/${model.id}\n\n# and import it as a module\nimport ${packageName}\n\nnlp = ${packageName}.load()  # or edsnlp.load(\"${packageName}\")\n`];\n};\nexport const espnetTTS = model => [`from espnet2.bin.tts_inference import Text2Speech\n\nmodel = Text2Speech.from_pretrained(\"${model.id}\")\n\nspeech, *_ = model(\"text to generate speech from\")`];\nexport const espnetASR = model => [`from espnet2.bin.asr_inference import Speech2Text\n\nmodel = Speech2Text.from_pretrained(\n  \"${model.id}\"\n)\n\nspeech, rate = soundfile.read(\"speech.wav\")\ntext, *_ = model(speech)[0]`];\nconst espnetUnknown = () => [`unknown model type (must be text-to-speech or automatic-speech-recognition)`];\nexport const espnet = model => {\n  if (model.tags.includes(\"text-to-speech\")) {\n    return espnetTTS(model);\n  } else if (model.tags.includes(\"automatic-speech-recognition\")) {\n    return espnetASR(model);\n  }\n  return espnetUnknown();\n};\nexport const fairseq = model => [`from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\n\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n    \"${model.id}\"\n)`];\nexport const flair = model => [`from flair.models import SequenceTagger\n\ntagger = SequenceTagger.load(\"${model.id}\")`];\nexport const gliner = model => [`from gliner import GLiNER\n\nmodel = GLiNER.from_pretrained(\"${model.id}\")`];\nexport const indextts = model => [`# Download model\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(${model.id}, local_dir=\"checkpoints\")\n\nfrom indextts.infer import IndexTTS\n\n# Ensure config.yaml is present in the checkpoints directory\ntts = IndexTTS(model_dir=\"checkpoints\", cfg_path=\"checkpoints/config.yaml\")\n\nvoice = \"path/to/your/reference_voice.wav\"  # Path to the voice reference audio file\ntext = \"Hello, how are you?\"\noutput_path = \"output_index.wav\"\n\ntts.infer(voice, text, output_path)`];\nexport const htrflow = model => [`# CLI usage\n# see docs: https://ai-riksarkivet.github.io/htrflow/latest/getting_started/quick_start.html\nhtrflow pipeline <path/to/pipeline.yaml> <path/to/image>`, `# Python usage\nfrom htrflow.pipeline.pipeline import Pipeline\nfrom htrflow.pipeline.steps import Task\nfrom htrflow.models.framework.model import ModelClass\n\npipeline = Pipeline(\n    [\n        Task(\n            ModelClass, {\"model\": \"${model.id}\"}, {}\n        ),\n    ])`];\nexport const keras = model => [`# Available backend options are: \"jax\", \"torch\", \"tensorflow\".\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\t\nimport keras\n\nmodel = keras.saving.load_model(\"hf://${model.id}\")\n`];\nconst _keras_hub_causal_lm = modelId => `\nimport keras_hub\n\n# Load CausalLM model (optional: use half precision for inference)\ncausal_lm = keras_hub.models.CausalLM.from_preset(\"hf://${modelId}\", dtype=\"bfloat16\")\ncausal_lm.compile(sampler=\"greedy\")  # (optional) specify a sampler\n\n# Generate text\ncausal_lm.generate(\"Keras: deep learning for\", max_length=64)\n`;\nconst _keras_hub_text_to_image = modelId => `\nimport keras_hub\n\n# Load TextToImage model (optional: use half precision for inference)\ntext_to_image = keras_hub.models.TextToImage.from_preset(\"hf://${modelId}\", dtype=\"bfloat16\")\n\n# Generate images with a TextToImage model.\ntext_to_image.generate(\"Astronaut in a jungle\")\n`;\nconst _keras_hub_text_classifier = modelId => `\nimport keras_hub\n\n# Load TextClassifier model\ntext_classifier = keras_hub.models.TextClassifier.from_preset(\n    \"hf://${modelId}\",\n    num_classes=2,\n)\n# Fine-tune\ntext_classifier.fit(x=[\"Thilling adventure!\", \"Total snoozefest.\"], y=[1, 0])\n# Classify text\ntext_classifier.predict([\"Not my cup of tea.\"])\n`;\nconst _keras_hub_image_classifier = modelId => `\nimport keras_hub\nimport keras\n\n# Load ImageClassifier model\nimage_classifier = keras_hub.models.ImageClassifier.from_preset(\n    \"hf://${modelId}\",\n    num_classes=2,\n)\n# Fine-tune\nimage_classifier.fit(\n    x=keras.random.randint((32, 64, 64, 3), 0, 256),\n    y=keras.random.randint((32, 1), 0, 2),\n)\n# Classify image\nimage_classifier.predict(keras.random.randint((1, 64, 64, 3), 0, 256))\n`;\nconst _keras_hub_tasks_with_example = {\n  CausalLM: _keras_hub_causal_lm,\n  TextToImage: _keras_hub_text_to_image,\n  TextClassifier: _keras_hub_text_classifier,\n  ImageClassifier: _keras_hub_image_classifier\n};\nconst _keras_hub_task_without_example = (task, modelId) => `\nimport keras_hub\n\n# Create a ${task} model\ntask = keras_hub.models.${task}.from_preset(\"hf://${modelId}\")\n`;\nconst _keras_hub_generic_backbone = modelId => `\nimport keras_hub\n\n# Create a Backbone model unspecialized for any task\nbackbone = keras_hub.models.Backbone.from_preset(\"hf://${modelId}\")\n`;\nexport const keras_hub = model => {\n  const modelId = model.id;\n  const tasks = model.config?.keras_hub?.tasks ?? [];\n  const snippets = [];\n  // First, generate tasks with examples\n  for (const [task, snippet] of Object.entries(_keras_hub_tasks_with_example)) {\n    if (tasks.includes(task)) {\n      snippets.push(snippet(modelId));\n    }\n  }\n  // Then, add remaining tasks\n  for (const task of tasks) {\n    if (!Object.keys(_keras_hub_tasks_with_example).includes(task)) {\n      snippets.push(_keras_hub_task_without_example(task, modelId));\n    }\n  }\n  // Finally, add generic backbone snippet\n  snippets.push(_keras_hub_generic_backbone(modelId));\n  return snippets;\n};\nexport const kimi_audio = model => [`# Example usage for KimiAudio\n# pip install git+https://github.com/MoonshotAI/Kimi-Audio.git\n\nfrom kimia_infer.api.kimia import KimiAudio\n\nmodel = KimiAudio(model_path=\"${model.id}\", load_detokenizer=True)\n\nsampling_params = {\n    \"audio_temperature\": 0.8,\n    \"audio_top_k\": 10,\n    \"text_temperature\": 0.0,\n    \"text_top_k\": 5,\n}\n\n# For ASR\nasr_audio = \"asr_example.wav\"\nmessages_asr = [\n    {\"role\": \"user\", \"message_type\": \"text\", \"content\": \"Please transcribe the following audio:\"},\n    {\"role\": \"user\", \"message_type\": \"audio\", \"content\": asr_audio}\n]\n_, text = model.generate(messages_asr, **sampling_params, output_type=\"text\")\nprint(text)\n\n# For Q&A\nqa_audio = \"qa_example.wav\"\nmessages_conv = [{\"role\": \"user\", \"message_type\": \"audio\", \"content\": qa_audio}]\nwav, text = model.generate(messages_conv, **sampling_params, output_type=\"both\")\nsf.write(\"output_audio.wav\", wav.cpu().view(-1).numpy(), 24000)\nprint(text)\n`];\nexport const kittentts = model => [`from kittentts import KittenTTS\nm = KittenTTS(\"${model.id}\")\n\naudio = m.generate(\"This high quality TTS model works without a GPU\")\n\n# Save the audio\nimport soundfile as sf\nsf.write('output.wav', audio, 24000)`];\nexport const lightning_ir = model => {\n  if (model.tags.includes(\"bi-encoder\")) {\n    return [`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule\nmodel = BiEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`];\n  } else if (model.tags.includes(\"cross-encoder\")) {\n    return [`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import CrossEncoderModule\nmodel = CrossEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`];\n  }\n  return [`#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule, CrossEncoderModule\n\n# depending on the model type, use either BiEncoderModule or CrossEncoderModule\nmodel = BiEncoderModule(\"${model.id}\") \n# model = CrossEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`];\n};\nexport const llama_cpp_python = model => {\n  const snippets = [`# !pip install llama-cpp-python\n\nfrom llama_cpp import Llama\n\nllm = Llama.from_pretrained(\n\trepo_id=\"${model.id}\",\n\tfilename=\"{{GGUF_FILE}}\",\n)\n`];\n  if (model.tags.includes(\"conversational\")) {\n    const messages = getModelInputSnippet(model);\n    snippets.push(`llm.create_chat_completion(\n\tmessages = ${stringifyMessages(messages, {\n      attributeKeyQuotes: true,\n      indent: \"\\t\"\n    })}\n)`);\n  } else {\n    snippets.push(`output = llm(\n\t\"Once upon a time,\",\n\tmax_tokens=512,\n\techo=True\n)\nprint(output)`);\n  }\n  return snippets;\n};\nexport const lerobot = model => {\n  if (model.tags.includes(\"smolvla\")) {\n    const smolvlaSnippets = [\n    // Installation snippet\n    `# See https://github.com/huggingface/lerobot?tab=readme-ov-file#installation for more details\ngit clone https://github.com/huggingface/lerobot.git\ncd lerobot\npip install -e .[smolvla]`,\n    // Finetune snippet\n    `# Launch finetuning on your dataset\npython lerobot/scripts/train.py \\\\\n--policy.path=${model.id} \\\\\n--dataset.repo_id=lerobot/svla_so101_pickplace \\\\ \n--batch_size=64 \\\\\n--steps=20000 \\\\\n--output_dir=outputs/train/my_smolvla \\\\\n--job_name=my_smolvla_training \\\\\n--policy.device=cuda \\\\\n--wandb.enable=true`];\n    if (model.id !== \"lerobot/smolvla_base\") {\n      // Inference snippet (only if not base model)\n      smolvlaSnippets.push(`# Run the policy using the record function\t\npython -m lerobot.record \\\\\n  --robot.type=so101_follower \\\\\n  --robot.port=/dev/ttyACM0 \\\\ # <- Use your port\n  --robot.id=my_blue_follower_arm \\\\ # <- Use your robot id\n  --robot.cameras=\"{ front: {type: opencv, index_or_path: 8, width: 640, height: 480, fps: 30}}\" \\\\ # <- Use your cameras\n  --dataset.single_task=\"Grasp a lego block and put it in the bin.\" \\\\ # <- Use the same task description you used in your dataset recording\n  --dataset.repo_id=HF_USER/dataset_name \\\\  # <- This will be the dataset name on HF Hub\n  --dataset.episode_time_s=50 \\\\\n  --dataset.num_episodes=10 \\\\\n  --policy.path=${model.id}`);\n    }\n    return smolvlaSnippets;\n  }\n  return [];\n};\nexport const tf_keras = model => [`# Note: 'keras<3.x' or 'tf_keras' must be installed (legacy)\n# See https://github.com/keras-team/tf-keras for more details.\nfrom huggingface_hub import from_pretrained_keras\n\nmodel = from_pretrained_keras(\"${model.id}\")\n`];\nexport const mamba_ssm = model => [`from mamba_ssm import MambaLMHeadModel\n\nmodel = MambaLMHeadModel.from_pretrained(\"${model.id}\")`];\nexport const mars5_tts = model => [`# Install from https://github.com/Camb-ai/MARS5-TTS\n\nfrom inference import Mars5TTS\nmars5 = Mars5TTS.from_pretrained(\"${model.id}\")`];\nexport const matanyone = model => [`# Install from https://github.com/pq-yang/MatAnyone.git\n\nfrom matanyone.model.matanyone import MatAnyone\nmodel = MatAnyone.from_pretrained(\"${model.id}\")`, `\nfrom matanyone import InferenceCore\nprocessor = InferenceCore(\"${model.id}\")`];\nexport const mesh_anything = () => [`# Install from https://github.com/buaacyw/MeshAnything.git\n\nfrom MeshAnything.models.meshanything import MeshAnything\n\n# refer to https://github.com/buaacyw/MeshAnything/blob/main/main.py#L91 on how to define args\n# and https://github.com/buaacyw/MeshAnything/blob/main/app.py regarding usage\nmodel = MeshAnything(args)`];\nexport const open_clip = model => [`import open_clip\n\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:${model.id}')\ntokenizer = open_clip.get_tokenizer('hf-hub:${model.id}')`];\nexport const paddlenlp = model => {\n  if (model.config?.architectures?.[0]) {\n    const architecture = model.config.architectures[0];\n    return [[`from paddlenlp.transformers import AutoTokenizer, ${architecture}`, \"\", `tokenizer = AutoTokenizer.from_pretrained(\"${model.id}\", from_hf_hub=True)`, `model = ${architecture}.from_pretrained(\"${model.id}\", from_hf_hub=True)`].join(\"\\n\")];\n  } else {\n    return [[`# ⚠️ Type of model unknown`, `from paddlenlp.transformers import AutoTokenizer, AutoModel`, \"\", `tokenizer = AutoTokenizer.from_pretrained(\"${model.id}\", from_hf_hub=True)`, `model = AutoModel.from_pretrained(\"${model.id}\", from_hf_hub=True)`].join(\"\\n\")];\n  }\n};\nexport const paddleocr = model => {\n  const mapping = {\n    textline_detection: {\n      className: \"TextDetection\"\n    },\n    textline_recognition: {\n      className: \"TextRecognition\"\n    },\n    seal_text_detection: {\n      className: \"SealTextDetection\"\n    },\n    doc_img_unwarping: {\n      className: \"TextImageUnwarping\"\n    },\n    doc_img_orientation_classification: {\n      className: \"DocImgOrientationClassification\"\n    },\n    textline_orientation_classification: {\n      className: \"TextLineOrientationClassification\"\n    },\n    chart_parsing: {\n      className: \"ChartParsing\"\n    },\n    formula_recognition: {\n      className: \"FormulaRecognition\"\n    },\n    layout_detection: {\n      className: \"LayoutDetection\"\n    },\n    table_cells_detection: {\n      className: \"TableCellsDetection\"\n    },\n    wired_table_classification: {\n      className: \"TableClassification\"\n    },\n    table_structure_recognition: {\n      className: \"TableStructureRecognition\"\n    }\n  };\n  if (model.tags.includes(\"doc_vlm\")) {\n    return [`# 1. See https://www.paddlepaddle.org.cn/en/install to install paddlepaddle\n# 2. pip install paddleocr\n\nfrom paddleocr import DocVLM\nmodel = DocVLM(model_name=\"${nameWithoutNamespace(model.id)}\")\noutput = model.predict(\n    input={\"image\": \"path/to/image.png\", \"query\": \"Parsing this image and output the content in Markdown format.\"},\n    batch_size=1\n)\nfor res in output:\n    res.print()\n    res.save_to_json(save_path=\"./output/res.json\")`];\n  }\n  if (model.tags.includes(\"document-parse\")) {\n    return [`# See https://www.paddleocr.ai/latest/version3.x/pipeline_usage/PaddleOCR-VL.html to installation\n\nfrom paddleocr import PaddleOCRVL\npipeline = PaddleOCRVL()\noutput = pipeline.predict(\"path/to/document_image.png\")\nfor res in output:\n\tres.print()\n\tres.save_to_json(save_path=\"output\")\n\tres.save_to_markdown(save_path=\"output\")`];\n  }\n  for (const tag of model.tags) {\n    if (tag in mapping) {\n      const {\n        className\n      } = mapping[tag];\n      return [`# 1. See https://www.paddlepaddle.org.cn/en/install to install paddlepaddle\n# 2. pip install paddleocr\n\nfrom paddleocr import ${className}\nmodel = ${className}(model_name=\"${nameWithoutNamespace(model.id)}\")\noutput = model.predict(input=\"path/to/image.png\", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")`];\n    }\n  }\n  return [`# Please refer to the document for information on how to use the model. \n# https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/module_usage/module_overview.html`];\n};\nexport const perception_encoder = model => {\n  const clip_model = `# Use PE-Core models as CLIP models\nimport core.vision_encoder.pe as pe\n\nmodel = pe.CLIP.from_config(\"${model.id}\", pretrained=True)`;\n  const vision_encoder = `# Use any PE model as a vision encoder\nimport core.vision_encoder.pe as pe\n\nmodel = pe.VisionTransformer.from_config(\"${model.id}\", pretrained=True)`;\n  if (model.id.includes(\"Core\")) {\n    return [clip_model, vision_encoder];\n  } else {\n    return [vision_encoder];\n  }\n};\nexport const phantom_wan = model => [`from huggingface_hub import snapshot_download\nfrom phantom_wan import WANI2V, configs\n\ncheckpoint_dir = snapshot_download(\"${model.id}\")\nwan_i2v = WanI2V(\n            config=configs.WAN_CONFIGS['i2v-14B'],\n            checkpoint_dir=checkpoint_dir,\n        )\n video = wan_i2v.generate(text_prompt, image_prompt)`];\nexport const pyannote_audio_pipeline = model => [`from pyannote.audio import Pipeline\n  \npipeline = Pipeline.from_pretrained(\"${model.id}\")\n\n# inference on the whole file\npipeline(\"file.wav\")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\n\nfrom pyannote.audio import Audio\nwaveform, sample_rate = Audio().crop(\"file.wav\", excerpt)\npipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})`];\nconst pyannote_audio_model = model => [`from pyannote.audio import Model, Inference\n\nmodel = Model.from_pretrained(\"${model.id}\")\ninference = Inference(model)\n\n# inference on the whole file\ninference(\"file.wav\")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\ninference.crop(\"file.wav\", excerpt)`];\nexport const pyannote_audio = model => {\n  if (model.tags.includes(\"pyannote-audio-pipeline\")) {\n    return pyannote_audio_pipeline(model);\n  }\n  return pyannote_audio_model(model);\n};\nexport const relik = model => [`from relik import Relik\n \nrelik = Relik.from_pretrained(\"${model.id}\")`];\nexport const renderformer = model => [`# Install from https://github.com/microsoft/renderformer\n\nfrom renderformer import RenderFormerRenderingPipeline\npipeline = RenderFormerRenderingPipeline.from_pretrained(\"${model.id}\")`];\nconst tensorflowttsTextToMel = model => [`from tensorflow_tts.inference import AutoProcessor, TFAutoModel\n\nprocessor = AutoProcessor.from_pretrained(\"${model.id}\")\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\n`];\nconst tensorflowttsMelToWav = model => [`from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\naudios = model.inference(mels)\n`];\nconst tensorflowttsUnknown = model => [`from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\n`];\nexport const tensorflowtts = model => {\n  if (model.tags.includes(\"text-to-mel\")) {\n    return tensorflowttsTextToMel(model);\n  } else if (model.tags.includes(\"mel-to-wav\")) {\n    return tensorflowttsMelToWav(model);\n  }\n  return tensorflowttsUnknown(model);\n};\nexport const timm = model => [`import timm\n\nmodel = timm.create_model(\"hf_hub:${model.id}\", pretrained=True)`];\nexport const saelens = (/* model: ModelData */\n) => [`# pip install sae-lens\nfrom sae_lens import SAE\n\nsae, cfg_dict, sparsity = SAE.from_pretrained(\n    release = \"RELEASE_ID\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n    sae_id = \"SAE_ID\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n)`];\nexport const seed_story = () => [`# seed_story_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/agent_7b_sft.yaml'\n# llm_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/llama2chat7b_lora.yaml'\nfrom omegaconf import OmegaConf\nimport hydra\n\n# load Llama2\nllm_cfg = OmegaConf.load(llm_cfg_path)\nllm = hydra.utils.instantiate(llm_cfg, torch_dtype=\"fp16\")\n\n# initialize seed_story\nseed_story_cfg = OmegaConf.load(seed_story_cfg_path)\nseed_story = hydra.utils.instantiate(seed_story_cfg, llm=llm) `];\nconst skopsPickle = (model, modelFile) => {\n  return [`import joblib\nfrom skops.hub_utils import download\ndownload(\"${model.id}\", \"path_to_folder\")\nmodel = joblib.load(\n\t\"${modelFile}\"\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`];\n};\nconst skopsFormat = (model, modelFile) => {\n  return [`from skops.hub_utils import download\nfrom skops.io import load\ndownload(\"${model.id}\", \"path_to_folder\")\n# make sure model file is in skops format\n# if model is a pickle file, make sure it's from a source you trust\nmodel = load(\"path_to_folder/${modelFile}\")`];\n};\nconst skopsJobLib = model => {\n  return [`from huggingface_hub import hf_hub_download\nimport joblib\nmodel = joblib.load(\n\thf_hub_download(\"${model.id}\", \"sklearn_model.joblib\")\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`];\n};\nexport const sklearn = model => {\n  if (model.tags.includes(\"skops\")) {\n    const skopsmodelFile = model.config?.sklearn?.model?.file;\n    const skopssaveFormat = model.config?.sklearn?.model_format;\n    if (!skopsmodelFile) {\n      return [`# ⚠️ Model filename not specified in config.json`];\n    }\n    if (skopssaveFormat === \"pickle\") {\n      return skopsPickle(model, skopsmodelFile);\n    } else {\n      return skopsFormat(model, skopsmodelFile);\n    }\n  } else {\n    return skopsJobLib(model);\n  }\n};\nexport const stable_audio_tools = model => [`import torch\nimport torchaudio\nfrom einops import rearrange\nfrom stable_audio_tools import get_pretrained_model\nfrom stable_audio_tools.inference.generation import generate_diffusion_cond\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Download model\nmodel, model_config = get_pretrained_model(\"${model.id}\")\nsample_rate = model_config[\"sample_rate\"]\nsample_size = model_config[\"sample_size\"]\n\nmodel = model.to(device)\n\n# Set up text and timing conditioning\nconditioning = [{\n\t\"prompt\": \"128 BPM tech house drum loop\",\n}]\n\n# Generate stereo audio\noutput = generate_diffusion_cond(\n\tmodel,\n\tconditioning=conditioning,\n\tsample_size=sample_size,\n\tdevice=device\n)\n\n# Rearrange audio batch to a single sequence\noutput = rearrange(output, \"b d n -> d (b n)\")\n\n# Peak normalize, clip, convert to int16, and save to file\noutput = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\ntorchaudio.save(\"output.wav\", output, sample_rate)`];\nexport const fastai = model => [`from huggingface_hub import from_pretrained_fastai\n\nlearn = from_pretrained_fastai(\"${model.id}\")`];\nexport const sam2 = model => {\n  const image_predictor = `# Use SAM2 with images\nimport torch\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\npredictor = SAM2ImagePredictor.from_pretrained(${model.id})\n\nwith torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    predictor.set_image(<your_image>)\n    masks, _, _ = predictor.predict(<input_prompts>)`;\n  const video_predictor = `# Use SAM2 with videos\nimport torch\nfrom sam2.sam2_video_predictor import SAM2VideoPredictor\n\t\npredictor = SAM2VideoPredictor.from_pretrained(${model.id})\n\nwith torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    state = predictor.init_state(<your_video>)\n\n    # add new prompts and instantly get the output on the same frame\n    frame_idx, object_ids, masks = predictor.add_new_points(state, <your_prompts>):\n\n    # propagate the prompts to get masklets throughout the video\n    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):\n        ...`;\n  return [image_predictor, video_predictor];\n};\nexport const sampleFactory = model => [`python -m sample_factory.huggingface.load_from_hub -r ${model.id} -d ./train_dir`];\nfunction get_widget_examples_from_st_model(model) {\n  const widgetExample = model.widgetData?.[0];\n  if (widgetExample?.source_sentence && widgetExample?.sentences?.length) {\n    return [widgetExample.source_sentence, ...widgetExample.sentences];\n  }\n}\nexport const sentenceTransformers = model => {\n  const remote_code_snippet = model.tags.includes(TAG_CUSTOM_CODE) ? \", trust_remote_code=True\" : \"\";\n  if (model.tags.includes(\"PyLate\")) {\n    return [`from pylate import models\n\nqueries = [\n    \"Which planet is known as the Red Planet?\",\n    \"What is the largest planet in our solar system?\",\n]\n\ndocuments = [\n    [\"Mars is the Red Planet.\", \"Venus is Earth's twin.\"],\n    [\"Jupiter is the largest planet.\", \"Saturn has rings.\"],\n]\n\nmodel = models.ColBERT(model_name_or_path=\"${model.id}\")\n\nqueries_emb = model.encode(queries, is_query=True)\ndocs_emb = model.encode(documents, is_query=False)`];\n  }\n  if (model.tags.includes(\"cross-encoder\") || model.pipeline_tag == \"text-ranking\") {\n    return [`from sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\"${model.id}\"${remote_code_snippet})\n\nquery = \"Which planet is known as the Red Planet?\"\npassages = [\n\t\"Venus is often called Earth's twin because of its similar size and proximity.\",\n\t\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n\t\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n\t\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n]\n\nscores = model.predict([(query, passage) for passage in passages])\nprint(scores)`];\n  }\n  const exampleSentences = get_widget_examples_from_st_model(model) ?? [\"The weather is lovely today.\", \"It's so sunny outside!\", \"He drove to the stadium.\"];\n  return [`from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"${model.id}\"${remote_code_snippet})\n\nsentences = ${JSON.stringify(exampleSentences, null, 4)}\nembeddings = model.encode(sentences)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [${exampleSentences.length}, ${exampleSentences.length}]`];\n};\nexport const setfit = model => [`from setfit import SetFitModel\n\nmodel = SetFitModel.from_pretrained(\"${model.id}\")`];\nexport const spacy = model => [`!pip install https://huggingface.co/${model.id}/resolve/main/${nameWithoutNamespace(model.id)}-any-py3-none-any.whl\n\n# Using spacy.load().\nimport spacy\nnlp = spacy.load(\"${nameWithoutNamespace(model.id)}\")\n\n# Importing as module.\nimport ${nameWithoutNamespace(model.id)}\nnlp = ${nameWithoutNamespace(model.id)}.load()`];\nexport const span_marker = model => [`from span_marker import SpanMarkerModel\n\nmodel = SpanMarkerModel.from_pretrained(\"${model.id}\")`];\nexport const stanza = model => [`import stanza\n\nstanza.download(\"${nameWithoutNamespace(model.id).replace(\"stanza-\", \"\")}\")\nnlp = stanza.Pipeline(\"${nameWithoutNamespace(model.id).replace(\"stanza-\", \"\")}\")`];\nconst speechBrainMethod = speechbrainInterface => {\n  switch (speechbrainInterface) {\n    case \"EncoderClassifier\":\n      return \"classify_file\";\n    case \"EncoderDecoderASR\":\n    case \"EncoderASR\":\n      return \"transcribe_file\";\n    case \"SpectralMaskEnhancement\":\n      return \"enhance_file\";\n    case \"SepformerSeparation\":\n      return \"separate_file\";\n    default:\n      return undefined;\n  }\n};\nexport const speechbrain = model => {\n  const speechbrainInterface = model.config?.speechbrain?.speechbrain_interface;\n  if (speechbrainInterface === undefined) {\n    return [`# interface not specified in config.json`];\n  }\n  const speechbrainMethod = speechBrainMethod(speechbrainInterface);\n  if (speechbrainMethod === undefined) {\n    return [`# interface in config.json invalid`];\n  }\n  return [`from speechbrain.pretrained import ${speechbrainInterface}\nmodel = ${speechbrainInterface}.from_hparams(\n  \"${model.id}\"\n)\nmodel.${speechbrainMethod}(\"file.wav\")`];\n};\nexport const terratorch = model => [`from terratorch.registry import BACKBONE_REGISTRY\n\nmodel = BACKBONE_REGISTRY.build(\"${model.id}\")`];\nconst hasChatTemplate = model => model.config?.tokenizer_config?.chat_template !== undefined || model.config?.processor_config?.chat_template !== undefined || model.config?.chat_template_jinja !== undefined;\nexport const transformers = model => {\n  const info = model.transformersInfo;\n  if (!info) {\n    return [`# ⚠️ Type of model unknown`];\n  }\n  const remote_code_snippet = model.tags.includes(TAG_CUSTOM_CODE) ? \", trust_remote_code=True\" : \"\";\n  const autoSnippet = [];\n  if (info.processor) {\n    const processorVarName = info.processor === \"AutoTokenizer\" ? \"tokenizer\" : info.processor === \"AutoFeatureExtractor\" ? \"extractor\" : \"processor\";\n    autoSnippet.push(\"# Load model directly\", `from transformers import ${info.processor}, ${info.auto_model}`, \"\", `${processorVarName} = ${info.processor}.from_pretrained(\"${model.id}\"` + remote_code_snippet + \")\", `model = ${info.auto_model}.from_pretrained(\"${model.id}\"` + remote_code_snippet + \")\");\n    if (model.tags.includes(\"conversational\") && hasChatTemplate(model)) {\n      if (model.tags.includes(\"image-text-to-text\")) {\n        autoSnippet.push(\"messages = [\", [\"    {\", '        \"role\": \"user\",', '        \"content\": [', '            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},', '            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}', \"        ]\", \"    },\"].join(\"\\n\"), \"]\");\n      } else {\n        autoSnippet.push(\"messages = [\", '    {\"role\": \"user\", \"content\": \"Who are you?\"},', \"]\");\n      }\n      autoSnippet.push(`inputs = ${processorVarName}.apply_chat_template(`, \"\tmessages,\", \"\tadd_generation_prompt=True,\", \"\ttokenize=True,\", \"\treturn_dict=True,\", '\treturn_tensors=\"pt\",', \").to(model.device)\", \"\", \"outputs = model.generate(**inputs, max_new_tokens=40)\", `print(${processorVarName}.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))`);\n    }\n  } else {\n    autoSnippet.push(\"# Load model directly\", `from transformers import ${info.auto_model}`, `model = ${info.auto_model}.from_pretrained(\"${model.id}\"` + remote_code_snippet + ', torch_dtype=\"auto\")');\n  }\n  if (model.pipeline_tag && LIBRARY_TASK_MAPPING.transformers?.includes(model.pipeline_tag)) {\n    const pipelineSnippet = [\"# Use a pipeline as a high-level helper\", \"from transformers import pipeline\", \"\", `pipe = pipeline(\"${model.pipeline_tag}\", model=\"${model.id}\"` + remote_code_snippet + \")\"];\n    if (model.tags.includes(\"conversational\")) {\n      if (model.tags.includes(\"image-text-to-text\")) {\n        pipelineSnippet.push(\"messages = [\", [\"    {\", '        \"role\": \"user\",', '        \"content\": [', '            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},', '            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}', \"        ]\", \"    },\"].join(\"\\n\"), \"]\");\n        pipelineSnippet.push(\"pipe(text=messages)\");\n      } else {\n        pipelineSnippet.push(\"messages = [\", '    {\"role\": \"user\", \"content\": \"Who are you?\"},', \"]\");\n        pipelineSnippet.push(\"pipe(messages)\");\n      }\n    } else if (model.pipeline_tag === \"zero-shot-image-classification\") {\n      pipelineSnippet.push(\"pipe(\", '    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\",', '    candidate_labels=[\"animals\", \"humans\", \"landscape\"],', \")\");\n    } else if (model.pipeline_tag === \"image-classification\") {\n      pipelineSnippet.push('pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")');\n    }\n    return [pipelineSnippet.join(\"\\n\"), autoSnippet.join(\"\\n\")];\n  }\n  return [autoSnippet.join(\"\\n\")];\n};\nexport const transformersJS = model => {\n  if (!model.pipeline_tag) {\n    return [`// ⚠️ Unknown pipeline tag`];\n  }\n  const libName = \"@huggingface/transformers\";\n  return [`// npm i ${libName}\nimport { pipeline } from '${libName}';\n\n// Allocate pipeline\nconst pipe = await pipeline('${model.pipeline_tag}', '${model.id}');`];\n};\nconst peftTask = peftTaskType => {\n  switch (peftTaskType) {\n    case \"CAUSAL_LM\":\n      return \"CausalLM\";\n    case \"SEQ_2_SEQ_LM\":\n      return \"Seq2SeqLM\";\n    case \"TOKEN_CLS\":\n      return \"TokenClassification\";\n    case \"SEQ_CLS\":\n      return \"SequenceClassification\";\n    default:\n      return undefined;\n  }\n};\nexport const peft = model => {\n  const {\n    base_model_name_or_path: peftBaseModel,\n    task_type: peftTaskType\n  } = model.config?.peft ?? {};\n  const pefttask = peftTask(peftTaskType);\n  if (!pefttask) {\n    return [`Task type is invalid.`];\n  }\n  if (!peftBaseModel) {\n    return [`Base model is not found.`];\n  }\n  return [`from peft import PeftModel\nfrom transformers import AutoModelFor${pefttask}\n\nbase_model = AutoModelFor${pefttask}.from_pretrained(\"${peftBaseModel}\")\nmodel = PeftModel.from_pretrained(base_model, \"${model.id}\")`];\n};\nexport const fasttext = model => [`from huggingface_hub import hf_hub_download\nimport fasttext\n\nmodel = fasttext.load_model(hf_hub_download(\"${model.id}\", \"model.bin\"))`];\nexport const stableBaselines3 = model => [`from huggingface_sb3 import load_from_hub\ncheckpoint = load_from_hub(\n\trepo_id=\"${model.id}\",\n\tfilename=\"{MODEL FILENAME}.zip\",\n)`];\nconst nemoDomainResolver = (domain, model) => {\n  switch (domain) {\n    case \"ASR\":\n      return [`import nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(\"${model.id}\")\n\ntranscriptions = asr_model.transcribe([\"file.wav\"])`];\n    default:\n      return undefined;\n  }\n};\nexport const mlAgents = model => [`mlagents-load-from-hf --repo-id=\"${model.id}\" --local-dir=\"./download: string[]s\"`];\nexport const sentis = (/* model: ModelData */\n) => [`string modelName = \"[Your model name here].sentis\";\nModel model = ModelLoader.Load(Application.streamingAssetsPath + \"/\" + modelName);\nIWorker engine = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);\n// Please see provided C# file for more details\n`];\nexport const sana = model => [`\n# Load the model and infer image from text\nimport torch\nfrom app.sana_pipeline import SanaPipeline\nfrom torchvision.utils import save_image\n\nsana = SanaPipeline(\"configs/sana_config/1024ms/Sana_1600M_img1024.yaml\")\nsana.from_pretrained(\"hf://${model.id}\")\n\nimage = sana(\n    prompt='a cyberpunk cat with a neon sign that says \"Sana\"',\n    height=1024,\n    width=1024,\n    guidance_scale=5.0,\n    pag_guidance_scale=2.0,\n    num_inference_steps=18,\n) `];\nexport const vibevoice = model => [`import torch, soundfile as sf, librosa, numpy as np\nfrom vibevoice.processor.vibevoice_processor import VibeVoiceProcessor\nfrom vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference\n\n# Load voice sample (should be 24kHz mono)\nvoice, sr = sf.read(\"path/to/voice_sample.wav\")\nif voice.ndim > 1: voice = voice.mean(axis=1)\nif sr != 24000: voice = librosa.resample(voice, sr, 24000)\n\nprocessor = VibeVoiceProcessor.from_pretrained(\"${model.id}\")\nmodel = VibeVoiceForConditionalGenerationInference.from_pretrained(\n    \"${model.id}\", torch_dtype=torch.bfloat16\n).to(\"cuda\").eval()\nmodel.set_ddpm_inference_steps(5)\n\ninputs = processor(text=[\"Speaker 0: Hello!\\\\nSpeaker 1: Hi there!\"],\n                   voice_samples=[[voice]], return_tensors=\"pt\")\naudio = model.generate(**inputs, cfg_scale=1.3,\n                       tokenizer=processor.tokenizer).speech_outputs[0]\nsf.write(\"output.wav\", audio.cpu().numpy().squeeze(), 24000)`];\nexport const videoprism = model => [`# Install from https://github.com/google-deepmind/videoprism\nimport jax\nfrom videoprism import models as vp\n\nflax_model = vp.get_model(\"${model.id}\")\nloaded_state = vp.load_pretrained_weights(\"${model.id}\")\n\n@jax.jit\ndef forward_fn(inputs, train=False):\n  return flax_model.apply(loaded_state, inputs, train=train)`];\nexport const vfimamba = model => [`from Trainer_finetune import Model\n\nmodel = Model.from_pretrained(\"${model.id}\")`];\nexport const lvface = model => [`from huggingface_hub import hf_hub_download\n\t from inference_onnx import LVFaceONNXInferencer\n\nmodel_path = hf_hub_download(\"${model.id}\", \"LVFace-L_Glint360K/LVFace-L_Glint360K.onnx\")\ninferencer = LVFaceONNXInferencer(model_path, use_gpu=True, timeout=300)\nimg_path = 'path/to/image1.jpg'\nembedding = inferencer.infer_from_image(img_path)`];\nexport const voicecraft = model => [`from voicecraft import VoiceCraft\n\nmodel = VoiceCraft.from_pretrained(\"${model.id}\")`];\nexport const voxcpm = model => [`import soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"${model.id}\")\n\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")`];\nexport const vui = () => [`# !pip install git+https://github.com/fluxions-ai/vui\n\nimport torchaudio\n\nfrom vui.inference import render\nfrom vui.model import Vui,\n\nmodel = Vui.from_pretrained().cuda()\nwaveform = render(\n    model,\n    \"Hey, here is some random stuff, usually something quite long as the shorter the text the less likely the model can cope!\",\n)\nprint(waveform.shape)\ntorchaudio.save(\"out.opus\", waveform[0], 22050)\n`];\nexport const chattts = () => [`import ChatTTS\nimport torchaudio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = [\"PUT YOUR TEXT HERE\",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)`];\nexport const ultralytics = model => {\n  // ultralytics models must have a version tag (e.g. `yolov8`)\n  const versionTag = model.tags.find(tag => tag.match(/^yolov\\d+$/));\n  const className = versionTag ? `YOLOv${versionTag.slice(4)}` : \"YOLOvXX\";\n  const prefix = versionTag ? \"\" : `# Couldn't find a valid YOLO version tag.\\n# Replace XX with the correct version.\\n`;\n  return [prefix + `from ultralytics import ${className}\n\nmodel = ${className}.from_pretrained(\"${model.id}\")\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)`];\n};\nexport const birefnet = model => [`# Option 1: use with transformers\n\nfrom transformers import AutoModelForImageSegmentation\nbirefnet = AutoModelForImageSegmentation.from_pretrained(\"${model.id}\", trust_remote_code=True)\n`, `# Option 2: use with BiRefNet\n\n# Install from https://github.com/ZhengPeng7/BiRefNet\n\nfrom models.birefnet import BiRefNet\nmodel = BiRefNet.from_pretrained(\"${model.id}\")`];\nexport const swarmformer = model => [`from swarmformer import SwarmFormerModel\n\nmodel = SwarmFormerModel.from_pretrained(\"${model.id}\")\n`];\nexport const univa = model => [`# Follow installation instructions at https://github.com/PKU-YuanGroup/UniWorld-V1\n\nfrom univa.models.qwen2p5vl.modeling_univa_qwen2p5vl import UnivaQwen2p5VLForConditionalGeneration\n\tmodel = UnivaQwen2p5VLForConditionalGeneration.from_pretrained(\n        \"${model.id}\",\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    ).to(\"cuda\")\n\tprocessor = AutoProcessor.from_pretrained(\"${model.id}\")\n`];\nconst mlx_unknown = model => [`# Download the model from the Hub\npip install huggingface_hub[hf_xet]\n\nhuggingface-cli download --local-dir ${nameWithoutNamespace(model.id)} ${model.id}`];\nconst mlxlm = model => [`# Make sure mlx-lm is installed\n# pip install --upgrade mlx-lm\n# if on a CUDA device, also pip install mlx[cuda]\n\n# Generate text with mlx-lm\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"${model.id}\")\n\nprompt = \"Once upon a time in\"\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)`];\nconst mlxchat = model => [`# Make sure mlx-lm is installed\n# pip install --upgrade mlx-lm\n\n# Generate text with mlx-lm\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"${model.id}\")\n\nprompt = \"Write a story about Einstein\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\n\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)`];\nconst mlxvlm = model => [`# Make sure mlx-vlm is installed\n# pip install --upgrade mlx-vlm\n\nfrom mlx_vlm import load, generate\nfrom mlx_vlm.prompt_utils import apply_chat_template\nfrom mlx_vlm.utils import load_config\n\n# Load the model\nmodel, processor = load(\"${model.id}\")\nconfig = load_config(\"${model.id}\")\n\n# Prepare input\nimage = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nprompt = \"Describe this image.\"\n\n# Apply chat template\nformatted_prompt = apply_chat_template(\n    processor, config, prompt, num_images=1\n)\n\n# Generate output\noutput = generate(model, processor, formatted_prompt, image)\nprint(output)`];\nexport const mlxim = model => [`from mlxim.model import create_model\n\nmodel = create_model(${model.id})`];\nexport const mlx = model => {\n  if (model.pipeline_tag === \"image-text-to-text\") {\n    return mlxvlm(model);\n  }\n  if (model.pipeline_tag === \"text-generation\") {\n    if (model.tags.includes(\"conversational\")) {\n      return mlxchat(model);\n    } else {\n      return mlxlm(model);\n    }\n  }\n  return mlx_unknown(model);\n};\nexport const model2vec = model => [`from model2vec import StaticModel\n\nmodel = StaticModel.from_pretrained(\"${model.id}\")`];\nexport const pruna = model => {\n  let snippets;\n  if (model.tags.includes(\"diffusers\")) {\n    snippets = pruna_diffusers(model);\n  } else if (model.tags.includes(\"transformers\")) {\n    snippets = pruna_transformers(model);\n  } else {\n    snippets = pruna_default(model);\n  }\n  const ensurePrunaModelImport = snippet => {\n    if (!/^from pruna import PrunaModel/m.test(snippet)) {\n      return `from pruna import PrunaModel\\n${snippet}`;\n    }\n    return snippet;\n  };\n  snippets = snippets.map(ensurePrunaModelImport);\n  if (model.tags.includes(\"pruna_pro-ai\")) {\n    return snippets.map(snippet => snippet.replace(/\\bpruna\\b/g, \"pruna_pro\").replace(/\\bPrunaModel\\b/g, \"PrunaProModel\"));\n  }\n  return snippets;\n};\nconst pruna_diffusers = model => {\n  const diffusersSnippets = diffusers(model);\n  return diffusersSnippets.map(snippet => snippet\n  // Replace pipeline classes with PrunaModel\n  .replace(/\\b\\w*Pipeline\\w*\\b/g, \"PrunaModel\")\n  // Clean up diffusers imports containing PrunaModel\n  .replace(/from diffusers import ([^,\\n]*PrunaModel[^,\\n]*)/g, \"\").replace(/from diffusers import ([^,\\n]+),?\\s*([^,\\n]*PrunaModel[^,\\n]*)/g, \"from diffusers import $1\").replace(/from diffusers import\\s*(\\n|$)/g, \"\")\n  // Fix PrunaModel imports\n  .replace(/from diffusers import PrunaModel/g, \"from pruna import PrunaModel\").replace(/from diffusers import ([^,\\n]+), PrunaModel/g, \"from diffusers import $1\").replace(/from diffusers import PrunaModel, ([^,\\n]+)/g, \"from diffusers import $1\")\n  // Clean up whitespace\n  .replace(/\\n\\n+/g, \"\\n\").trim());\n};\nconst pruna_transformers = model => {\n  const info = model.transformersInfo;\n  const transformersSnippets = transformers(model);\n  // Replace pipeline with PrunaModel\n  let processedSnippets = transformersSnippets.map(snippet => snippet.replace(/from transformers import pipeline/g, \"from pruna import PrunaModel\").replace(/pipeline\\([^)]*\\)/g, `PrunaModel.from_pretrained(\"${model.id}\")`));\n  // Additional cleanup if auto_model info is available\n  if (info?.auto_model) {\n    processedSnippets = processedSnippets.map(snippet => snippet.replace(new RegExp(`from transformers import ${info.auto_model}\\n?`, \"g\"), \"\").replace(new RegExp(`${info.auto_model}.from_pretrained`, \"g\"), \"PrunaModel.from_pretrained\").replace(new RegExp(`^.*from.*import.*(, *${info.auto_model})+.*$`, \"gm\"), line => line.replace(new RegExp(`, *${info.auto_model}`, \"g\"), \"\")));\n  }\n  return processedSnippets;\n};\nconst pruna_default = model => [`from pruna import PrunaModel\nmodel = PrunaModel.from_pretrained(\"${model.id}\")\n`];\nexport const nemo = model => {\n  let command = undefined;\n  // Resolve the tag to a nemo domain/sub-domain\n  if (model.tags.includes(\"automatic-speech-recognition\")) {\n    command = nemoDomainResolver(\"ASR\", model);\n  }\n  return command ?? [`# tag did not correspond to a valid NeMo domain.`];\n};\nexport const outetts = model => {\n  // Don’t show this block on GGUF / ONNX mirrors\n  const t = model.tags ?? [];\n  if (t.includes(\"gguf\") || t.includes(\"onnx\")) return [];\n  // v1.0 HF → minimal runnable snippet\n  return [`\n  import outetts\n  \n  enum = outetts.Models(\"${model.id}\".split(\"/\", 1)[1])       # VERSION_1_0_SIZE_1B\n  cfg  = outetts.ModelConfig.auto_config(enum, outetts.Backend.HF)\n  tts  = outetts.Interface(cfg)\n  \n  speaker = tts.load_default_speaker(\"EN-FEMALE-1-NEUTRAL\")\n  tts.generate(\n\t  outetts.GenerationConfig(\n\t\t  text=\"Hello there, how are you doing?\",\n\t\t  speaker=speaker,\n\t  )\n  ).save(\"output.wav\")\n  `];\n};\nexport const pxia = model => [`from pxia import AutoModel\n\nmodel = AutoModel.from_pretrained(\"${model.id}\")`];\nexport const pythae = model => [`from pythae.models import AutoModel\n\nmodel = AutoModel.load_from_hf_hub(\"${model.id}\")`];\nconst musicgen = model => [`from audiocraft.models import MusicGen\n\nmodel = MusicGen.get_pretrained(\"${model.id}\")\n\ndescriptions = ['happy rock', 'energetic EDM', 'sad jazz']\nwav = model.generate(descriptions)  # generates 3 samples.`];\nconst magnet = model => [`from audiocraft.models import MAGNeT\n\t\nmodel = MAGNeT.get_pretrained(\"${model.id}\")\n\ndescriptions = ['disco beat', 'energetic EDM', 'funky groove']\nwav = model.generate(descriptions)  # generates 3 samples.`];\nconst audiogen = model => [`from audiocraft.models import AudioGen\n\t\nmodel = AudioGen.get_pretrained(\"${model.id}\")\nmodel.set_generation_params(duration=5)  # generate 5 seconds.\ndescriptions = ['dog barking', 'sirene of an emergency vehicle', 'footsteps in a corridor']\nwav = model.generate(descriptions)  # generates 3 samples.`];\nexport const anemoi = model => [`from anemoi.inference.runners.default import DefaultRunner\nfrom anemoi.inference.config.run import RunConfiguration\n# Create Configuration\nconfig = RunConfiguration(checkpoint = {\"huggingface\":\"${model.id}\"})\n# Load Runner\nrunner = DefaultRunner(config)`];\nexport const audiocraft = model => {\n  if (model.tags.includes(\"musicgen\")) {\n    return musicgen(model);\n  } else if (model.tags.includes(\"audiogen\")) {\n    return audiogen(model);\n  } else if (model.tags.includes(\"magnet\")) {\n    return magnet(model);\n  } else {\n    return [`# Type of model unknown.`];\n  }\n};\nexport const whisperkit = () => [`# Install CLI with Homebrew on macOS device\nbrew install whisperkit-cli\n\n# View all available inference options\nwhisperkit-cli transcribe --help\n\t\n# Download and run inference using whisper base model\nwhisperkit-cli transcribe --audio-path /path/to/audio.mp3\n\n# Or use your preferred model variant\nwhisperkit-cli transcribe --model \"large-v3\" --model-prefix \"distil\" --audio-path /path/to/audio.mp3 --verbose`];\nexport const threedtopia_xl = model => [`from threedtopia_xl.models import threedtopia_xl\n\nmodel = threedtopia_xl.from_pretrained(\"${model.id}\")\nmodel.generate(cond=\"path/to/image.png\")`];\nexport const hezar = model => [`from hezar import Model\n\nmodel = Model.load(\"${model.id}\")`];\nexport const zonos = model => [`# pip install git+https://github.com/Zyphra/Zonos.git\nimport torchaudio\nfrom zonos.model import Zonos\nfrom zonos.conditioning import make_cond_dict\n\nmodel = Zonos.from_pretrained(\"${model.id}\", device=\"cuda\")\n\nwav, sr = torchaudio.load(\"speaker.wav\")           # 5-10s reference clip\nspeaker = model.make_speaker_embedding(wav, sr)\n\ncond  = make_cond_dict(text=\"Hello, world!\", speaker=speaker, language=\"en-us\")\ncodes = model.generate(model.prepare_conditioning(cond))\n\naudio = model.autoencoder.decode(codes)[0].cpu()\ntorchaudio.save(\"sample.wav\", audio, model.autoencoder.sampling_rate)\n`];\n//#endregion","map":{"version":3,"names":["LIBRARY_TASK_MAPPING","getModelInputSnippet","stringifyMessages","TAG_CUSTOM_CODE","nameWithoutNamespace","modelId","splitted","split","length","escapeStringForJson","str","JSON","stringify","slice","adapters","model","config","adapter_transformers","model_name","id","allennlpUnknown","allennlpQuestionAnswering","allennlp","tags","includes","araclip","asteroid","audioseal","watermarkSnippet","detectorSnippet","get_base_diffusers_model","cardData","base_model","toString","get_prompt_from_diffusers_model","prompt","widgetData","text","instance_prompt","ben2","bertopic","bm25s","chatterbox","contexttab","installSnippet","classificationSnippet","regressionsSnippet","cxr_foundation","depth_anything_v2","encoder","features","out_channels","depth_pro","inferenceSnippet","derm_foundation","dia","describe_anything","diffusers_install","diffusersDefaultPrompt","diffusersImg2ImgDefaultPrompt","diffusersVideoDefaultPrompt","diffusers_default","diffusers_image_to_image","diffusers_image_to_video","diffusers_controlnet","diffusers_lora","diffusers_lora_image_to_image","diffusers_lora_text_to_video","diffusers_lora_image_to_video","diffusers_textual_inversion","diffusers_flux_fill","diffusers_inpainting","diffusers","codeSnippets","pipeline_tag","diffusionkit","sd3Snippet","fluxSnippet","generateSnippet","pipelineSnippet","cartesia_pytorch","cartesia_mlx","edsnlp","packageName","replaceAll","espnetTTS","espnetASR","espnetUnknown","espnet","fairseq","flair","gliner","indextts","htrflow","keras","_keras_hub_causal_lm","_keras_hub_text_to_image","_keras_hub_text_classifier","_keras_hub_image_classifier","_keras_hub_tasks_with_example","CausalLM","TextToImage","TextClassifier","ImageClassifier","_keras_hub_task_without_example","task","_keras_hub_generic_backbone","keras_hub","tasks","snippets","snippet","Object","entries","push","keys","kimi_audio","kittentts","lightning_ir","llama_cpp_python","messages","attributeKeyQuotes","indent","lerobot","smolvlaSnippets","tf_keras","mamba_ssm","mars5_tts","matanyone","mesh_anything","open_clip","paddlenlp","architectures","architecture","join","paddleocr","mapping","textline_detection","className","textline_recognition","seal_text_detection","doc_img_unwarping","doc_img_orientation_classification","textline_orientation_classification","chart_parsing","formula_recognition","layout_detection","table_cells_detection","wired_table_classification","table_structure_recognition","tag","perception_encoder","clip_model","vision_encoder","phantom_wan","pyannote_audio_pipeline","pyannote_audio_model","pyannote_audio","relik","renderformer","tensorflowttsTextToMel","tensorflowttsMelToWav","tensorflowttsUnknown","tensorflowtts","timm","saelens","seed_story","skopsPickle","modelFile","skopsFormat","skopsJobLib","sklearn","skopsmodelFile","file","skopssaveFormat","model_format","stable_audio_tools","fastai","sam2","image_predictor","video_predictor","sampleFactory","get_widget_examples_from_st_model","widgetExample","source_sentence","sentences","sentenceTransformers","remote_code_snippet","exampleSentences","setfit","spacy","span_marker","stanza","replace","speechBrainMethod","speechbrainInterface","undefined","speechbrain","speechbrain_interface","speechbrainMethod","terratorch","hasChatTemplate","tokenizer_config","chat_template","processor_config","chat_template_jinja","transformers","info","transformersInfo","autoSnippet","processor","processorVarName","auto_model","transformersJS","libName","peftTask","peftTaskType","peft","base_model_name_or_path","peftBaseModel","task_type","pefttask","fasttext","stableBaselines3","nemoDomainResolver","domain","mlAgents","sentis","sana","vibevoice","videoprism","vfimamba","lvface","voicecraft","voxcpm","vui","chattts","ultralytics","versionTag","find","match","prefix","birefnet","swarmformer","univa","mlx_unknown","mlxlm","mlxchat","mlxvlm","mlxim","mlx","model2vec","pruna","pruna_diffusers","pruna_transformers","pruna_default","ensurePrunaModelImport","test","map","diffusersSnippets","trim","transformersSnippets","processedSnippets","RegExp","line","nemo","command","outetts","t","pxia","pythae","musicgen","magnet","audiogen","anemoi","audiocraft","whisperkit","threedtopia_xl","hezar","zonos"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/model-libraries-snippets.js"],"sourcesContent":["import { LIBRARY_TASK_MAPPING } from \"./library-to-tasks.js\";\nimport { getModelInputSnippet } from \"./snippets/inputs.js\";\nimport { stringifyMessages } from \"./snippets/common.js\";\nconst TAG_CUSTOM_CODE = \"custom_code\";\nfunction nameWithoutNamespace(modelId) {\n    const splitted = modelId.split(\"/\");\n    return splitted.length === 1 ? splitted[0] : splitted[1];\n}\nconst escapeStringForJson = (str) => JSON.stringify(str).slice(1, -1); // slice is needed to remove surrounding quotes added by JSON.stringify\n//#region snippets\nexport const adapters = (model) => [\n    `from adapters import AutoAdapterModel\n\nmodel = AutoAdapterModel.from_pretrained(\"${model.config?.adapter_transformers?.model_name}\")\nmodel.load_adapter(\"${model.id}\", set_active=True)`,\n];\nconst allennlpUnknown = (model) => [\n    `import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path(\"hf://${model.id}\")`,\n];\nconst allennlpQuestionAnswering = (model) => [\n    `import allennlp_models\nfrom allennlp.predictors.predictor import Predictor\n\npredictor = Predictor.from_path(\"hf://${model.id}\")\npredictor_input = {\"passage\": \"My name is Wolfgang and I live in Berlin\", \"question\": \"Where do I live?\"}\npredictions = predictor.predict_json(predictor_input)`,\n];\nexport const allennlp = (model) => {\n    if (model.tags.includes(\"question-answering\")) {\n        return allennlpQuestionAnswering(model);\n    }\n    return allennlpUnknown(model);\n};\nexport const araclip = (model) => [\n    `from araclip import AraClip\n\nmodel = AraClip.from_pretrained(\"${model.id}\")`,\n];\nexport const asteroid = (model) => [\n    `from asteroid.models import BaseModel\n\nmodel = BaseModel.from_pretrained(\"${model.id}\")`,\n];\nexport const audioseal = (model) => {\n    const watermarkSnippet = `# Watermark Generator\nfrom audioseal import AudioSeal\n\nmodel = AudioSeal.load_generator(\"${model.id}\")\n# pass a tensor (tensor_wav) of shape (batch, channels, samples) and a sample rate\nwav, sr = tensor_wav, 16000\n\t\nwatermark = model.get_watermark(wav, sr)\nwatermarked_audio = wav + watermark`;\n    const detectorSnippet = `# Watermark Detector\nfrom audioseal import AudioSeal\n\ndetector = AudioSeal.load_detector(\"${model.id}\")\n\t\nresult, message = detector.detect_watermark(watermarked_audio, sr)`;\n    return [watermarkSnippet, detectorSnippet];\n};\nfunction get_base_diffusers_model(model) {\n    return model.cardData?.base_model?.toString() ?? \"fill-in-base-model\";\n}\nfunction get_prompt_from_diffusers_model(model) {\n    const prompt = model.widgetData?.[0]?.text ?? model.cardData?.instance_prompt;\n    if (prompt) {\n        return escapeStringForJson(prompt);\n    }\n}\nexport const ben2 = (model) => [\n    `import requests\nfrom PIL import Image\nfrom ben2 import AutoModel\n\nurl = \"https://huggingface.co/datasets/mishig/sample_images/resolve/main/teapot.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n\nmodel = AutoModel.from_pretrained(\"${model.id}\")\nmodel.to(\"cuda\").eval()\nforeground = model.inference(image)\n`,\n];\nexport const bertopic = (model) => [\n    `from bertopic import BERTopic\n\nmodel = BERTopic.load(\"${model.id}\")`,\n];\nexport const bm25s = (model) => [\n    `from bm25s.hf import BM25HF\n\nretriever = BM25HF.load_from_hub(\"${model.id}\")`,\n];\nexport const chatterbox = () => [\n    `# pip install chatterbox-tts\nimport torchaudio as ta\nfrom chatterbox.tts import ChatterboxTTS\n\nmodel = ChatterboxTTS.from_pretrained(device=\"cuda\")\n\ntext = \"Ezreal and Jinx teamed up with Ahri, Yasuo, and Teemo to take down the enemy's Nexus in an epic late-game pentakill.\"\nwav = model.generate(text)\nta.save(\"test-1.wav\", wav, model.sr)\n\n# If you want to synthesize with a different voice, specify the audio prompt\nAUDIO_PROMPT_PATH=\"YOUR_FILE.wav\"\nwav = model.generate(text, audio_prompt_path=AUDIO_PROMPT_PATH)\nta.save(\"test-2.wav\", wav, model.sr)`,\n];\nexport const contexttab = () => {\n    const installSnippet = `pip install git+https://github.com/SAP-samples/contexttab`;\n    const classificationSnippet = `# Run a classification task\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contexttab import ConTextTabClassifier\n\n# Load sample data\nX, y = load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Initialize a classifier\n# You can omit checkpoint and checkpoint_revision to use the default model\nclf = ConTextTabClassifier(checkpoint=\"l2/base.pt\", checkpoint_revision=\"v1.0.0\", bagging=1, max_context_size=2048)\n\nclf.fit(X_train, y_train)\n\n# Predict probabilities\nprediction_probabilities = clf.predict_proba(X_test)\n# Predict labels\npredictions = clf.predict(X_test)\nprint(\"Accuracy\", accuracy_score(y_test, predictions))`;\n    const regressionsSnippet = `# Run a regression task\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\n\nfrom contexttab import ConTextTabRegressor\n\n\n# Load sample data\ndf = fetch_openml(data_id=531, as_frame=True)\nX = df.data\ny = df.target.astype(float)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n# Initialize the regressor\n# You can omit checkpoint and checkpoint_revision to use the default model\nregressor = ConTextTabRegressor(checkpoint=\"l2/base.pt\", checkpoint_revision=\"v1.0.0\", bagging=1, max_context_size=2048)\n\nregressor.fit(X_train, y_train)\n\n# Predict on the test set\npredictions = regressor.predict(X_test)\n\nr2 = r2_score(y_test, predictions)\nprint(\"R² Score:\", r2)`;\n    return [installSnippet, classificationSnippet, regressionsSnippet];\n};\nexport const cxr_foundation = () => [\n    `# pip install git+https://github.com/Google-Health/cxr-foundation.git#subdirectory=python\n\n# Load image as grayscale (Stillwaterising, CC0, via Wikimedia Commons)\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nimage_url = \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\"\nimg = Image.open(requests.get(image_url, headers={'User-Agent': 'Demo'}, stream=True).raw).convert('L')\n\n# Run inference\nfrom clientside.clients import make_hugging_face_client\ncxr_client = make_hugging_face_client('cxr_model')\nprint(cxr_client.get_image_embeddings_from_images([img]))`,\n];\nexport const depth_anything_v2 = (model) => {\n    let encoder;\n    let features;\n    let out_channels;\n    encoder = \"<ENCODER>\";\n    features = \"<NUMBER_OF_FEATURES>\";\n    out_channels = \"<OUT_CHANNELS>\";\n    if (model.id === \"depth-anything/Depth-Anything-V2-Small\") {\n        encoder = \"vits\";\n        features = \"64\";\n        out_channels = \"[48, 96, 192, 384]\";\n    }\n    else if (model.id === \"depth-anything/Depth-Anything-V2-Base\") {\n        encoder = \"vitb\";\n        features = \"128\";\n        out_channels = \"[96, 192, 384, 768]\";\n    }\n    else if (model.id === \"depth-anything/Depth-Anything-V2-Large\") {\n        encoder = \"vitl\";\n        features = \"256\";\n        out_channels = \"[256, 512, 1024, 1024\";\n    }\n    return [\n        `\n# Install from https://github.com/DepthAnything/Depth-Anything-V2\n\n# Load the model and infer depth from an image\nimport cv2\nimport torch\n\nfrom depth_anything_v2.dpt import DepthAnythingV2\n\n# instantiate the model\nmodel = DepthAnythingV2(encoder=\"${encoder}\", features=${features}, out_channels=${out_channels})\n\n# load the weights\nfilepath = hf_hub_download(repo_id=\"${model.id}\", filename=\"depth_anything_v2_${encoder}.pth\", repo_type=\"model\")\nstate_dict = torch.load(filepath, map_location=\"cpu\")\nmodel.load_state_dict(state_dict).eval()\n\nraw_img = cv2.imread(\"your/image/path\")\ndepth = model.infer_image(raw_img) # HxW raw depth map in numpy\n    `,\n    ];\n};\nexport const depth_pro = (model) => {\n    const installSnippet = `# Download checkpoint\npip install huggingface-hub\nhuggingface-cli download --local-dir checkpoints ${model.id}`;\n    const inferenceSnippet = `import depth_pro\n\n# Load model and preprocessing transform\nmodel, transform = depth_pro.create_model_and_transforms()\nmodel.eval()\n\n# Load and preprocess an image.\nimage, _, f_px = depth_pro.load_rgb(\"example.png\")\nimage = transform(image)\n\n# Run inference.\nprediction = model.infer(image, f_px=f_px)\n\n# Results: 1. Depth in meters\ndepth = prediction[\"depth\"]\n# Results: 2. Focal length in pixels\nfocallength_px = prediction[\"focallength_px\"]`;\n    return [installSnippet, inferenceSnippet];\n};\nexport const derm_foundation = () => [\n    `from huggingface_hub import from_pretrained_keras\nimport tensorflow as tf, requests\n\n# Load and format input\nIMAGE_URL = \"https://storage.googleapis.com/dx-scin-public-data/dataset/images/3445096909671059178.png\"\ninput_tensor = tf.train.Example(\n    features=tf.train.Features(\n        feature={\n            \"image/encoded\": tf.train.Feature(\n                bytes_list=tf.train.BytesList(value=[requests.get(IMAGE_URL, stream=True).content])\n            )\n        }\n    )\n).SerializeToString()\n\n# Load model and run inference\nloaded_model = from_pretrained_keras(\"google/derm-foundation\")\ninfer = loaded_model.signatures[\"serving_default\"]\nprint(infer(inputs=tf.constant([input_tensor])))`,\n];\nexport const dia = (model) => [\n    `import soundfile as sf\nfrom dia.model import Dia\n\nmodel = Dia.from_pretrained(\"${model.id}\")\ntext = \"[S1] Dia is an open weights text to dialogue model. [S2] You get full control over scripts and voices. [S1] Wow. Amazing. (laughs) [S2] Try it now on Git hub or Hugging Face.\"\noutput = model.generate(text)\n\nsf.write(\"simple.mp3\", output, 44100)`,\n];\nexport const describe_anything = (model) => [\n    `# pip install git+https://github.com/NVlabs/describe-anything\nfrom huggingface_hub import snapshot_download\nfrom dam import DescribeAnythingModel\n\nsnapshot_download(${model.id}, local_dir=\"checkpoints\")\n\ndam = DescribeAnythingModel(\n\tmodel_path=\"checkpoints\",\n\tconv_mode=\"v1\",\n\tprompt_mode=\"focal_prompt\",\n)`,\n];\nconst diffusers_install = \"pip install -U diffusers transformers\";\nconst diffusersDefaultPrompt = \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\";\nconst diffusersImg2ImgDefaultPrompt = \"Turn this cat into a dog\";\nconst diffusersVideoDefaultPrompt = \"A man with short gray hair plays a red electric guitar.\";\nconst diffusers_default = (model) => [\n    `from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersDefaultPrompt}\"\nimage = pipe(prompt).images[0]`,\n];\nconst diffusers_image_to_image = (model) => [\n    `from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersImg2ImgDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n\nimage = pipe(image=input_image, prompt=prompt).images[0]`,\n];\nconst diffusers_image_to_video = (model) => [\n    `import torch\nfrom diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${model.id}\", torch_dtype=torch.float16)\npipe.to(\"cuda\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\nimage = load_image(\n    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png\"\n)\n\noutput = pipe(image=image, prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`,\n];\nconst diffusers_controlnet = (model) => [\n    `from diffusers import ControlNetModel, StableDiffusionControlNetPipeline\n\ncontrolnet = ControlNetModel.from_pretrained(\"${model.id}\")\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n\t\"${get_base_diffusers_model(model)}\", controlnet=controlnet\n)`,\n];\nconst diffusers_lora = (model) => [\n    `from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersDefaultPrompt}\"\nimage = pipe(prompt).images[0]`,\n];\nconst diffusers_lora_image_to_image = (model) => [\n    `from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersImg2ImgDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/cat.png\")\n\nimage = pipe(image=input_image, prompt=prompt).images[0]`,\n];\nconst diffusers_lora_text_to_video = (model) => [\n    `from diffusers import DiffusionPipeline\nfrom diffusers.utils import export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\n\noutput = pipe(prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`,\n];\nconst diffusers_lora_image_to_video = (model) => [\n    `from diffusers import DiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_lora_weights(\"${model.id}\")\n\nprompt = \"${get_prompt_from_diffusers_model(model) ?? diffusersVideoDefaultPrompt}\"\ninput_image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/guitar-man.png\")\n\nimage = pipe(image=input_image, prompt=prompt).frames[0]\nexport_to_video(output, \"output.mp4\")`,\n];\nconst diffusers_textual_inversion = (model) => [\n    `from diffusers import DiffusionPipeline\n\npipe = DiffusionPipeline.from_pretrained(\"${get_base_diffusers_model(model)}\")\npipe.load_textual_inversion(\"${model.id}\")`,\n];\nconst diffusers_flux_fill = (model) => [\n    `import torch\nfrom diffusers import FluxFillPipeline\nfrom diffusers.utils import load_image\n\nimage = load_image(\"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup.png\")\nmask = load_image(\"https://huggingface.co/datasets/diffusers/diffusers-images-docs/resolve/main/cup_mask.png\")\n\npipe = FluxFillPipeline.from_pretrained(\"${model.id}\", torch_dtype=torch.bfloat16).to(\"cuda\")\nimage = pipe(\n    prompt=\"a white paper cup\",\n    image=image,\n    mask_image=mask,\n    height=1632,\n    width=1232,\n    guidance_scale=30,\n    num_inference_steps=50,\n    max_sequence_length=512,\n    generator=torch.Generator(\"cpu\").manual_seed(0)\n).images[0]\nimage.save(f\"flux-fill-dev.png\")`,\n];\nconst diffusers_inpainting = (model) => [\n    `import torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image\n\npipe = AutoPipelineForInpainting.from_pretrained(\"${model.id}\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\nimage = load_image(img_url).resize((1024, 1024))\nmask_image = load_image(mask_url).resize((1024, 1024))\n\nprompt = \"a tiger sitting on a park bench\"\ngenerator = torch.Generator(device=\"cuda\").manual_seed(0)\n\nimage = pipe(\n  prompt=prompt,\n  image=image,\n  mask_image=mask_image,\n  guidance_scale=8.0,\n  num_inference_steps=20,  # steps between 15 and 30 work well for us\n  strength=0.99,  # make sure to use \\`strength\\` below 1.0\n  generator=generator,\n).images[0]`,\n];\nexport const diffusers = (model) => {\n    let codeSnippets;\n    if (model.tags.includes(\"StableDiffusionInpaintPipeline\") ||\n        model.tags.includes(\"StableDiffusionXLInpaintPipeline\")) {\n        codeSnippets = diffusers_inpainting(model);\n    }\n    else if (model.tags.includes(\"controlnet\")) {\n        codeSnippets = diffusers_controlnet(model);\n    }\n    else if (model.tags.includes(\"lora\")) {\n        if (model.pipeline_tag === \"image-to-image\") {\n            codeSnippets = diffusers_lora_image_to_image(model);\n        }\n        else if (model.pipeline_tag === \"image-to-video\") {\n            codeSnippets = diffusers_lora_image_to_video(model);\n        }\n        else if (model.pipeline_tag === \"text-to-video\") {\n            codeSnippets = diffusers_lora_text_to_video(model);\n        }\n        else {\n            codeSnippets = diffusers_lora(model);\n        }\n    }\n    else if (model.tags.includes(\"textual_inversion\")) {\n        codeSnippets = diffusers_textual_inversion(model);\n    }\n    else if (model.tags.includes(\"FluxFillPipeline\")) {\n        codeSnippets = diffusers_flux_fill(model);\n    }\n    else if (model.pipeline_tag === \"image-to-video\") {\n        codeSnippets = diffusers_image_to_video(model);\n    }\n    else if (model.pipeline_tag === \"image-to-image\") {\n        codeSnippets = diffusers_image_to_image(model);\n    }\n    else {\n        codeSnippets = diffusers_default(model);\n    }\n    return [diffusers_install, ...codeSnippets];\n};\nexport const diffusionkit = (model) => {\n    const sd3Snippet = `# Pipeline for Stable Diffusion 3\nfrom diffusionkit.mlx import DiffusionPipeline\n\npipeline = DiffusionPipeline(\n\tshift=3.0,\n\tuse_t5=False,\n\tmodel_version=${model.id},\n\tlow_memory_mode=True,\n\ta16=True,\n\tw16=True,\n)`;\n    const fluxSnippet = `# Pipeline for Flux\nfrom diffusionkit.mlx import FluxPipeline\n\npipeline = FluxPipeline(\n  shift=1.0,\n  model_version=${model.id},\n  low_memory_mode=True,\n  a16=True,\n  w16=True,\n)`;\n    const generateSnippet = `# Image Generation\nHEIGHT = 512\nWIDTH = 512\nNUM_STEPS = ${model.tags.includes(\"flux\") ? 4 : 50}\nCFG_WEIGHT = ${model.tags.includes(\"flux\") ? 0 : 5}\n\nimage, _ = pipeline.generate_image(\n  \"a photo of a cat\",\n  cfg_weight=CFG_WEIGHT,\n  num_steps=NUM_STEPS,\n  latent_size=(HEIGHT // 8, WIDTH // 8),\n)`;\n    const pipelineSnippet = model.tags.includes(\"flux\") ? fluxSnippet : sd3Snippet;\n    return [pipelineSnippet, generateSnippet];\n};\nexport const cartesia_pytorch = (model) => [\n    `# pip install --no-binary :all: cartesia-pytorch\nfrom cartesia_pytorch import ReneLMHeadModel\nfrom transformers import AutoTokenizer\n\nmodel = ReneLMHeadModel.from_pretrained(\"${model.id}\")\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMo-1B-hf\")\n\nin_message = [\"Rene Descartes was\"]\ninputs = tokenizer(in_message, return_tensors=\"pt\")\n\noutputs = model.generate(inputs.input_ids, max_length=50, top_k=100, top_p=0.99)\nout_message = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n\nprint(out_message)\n)`,\n];\nexport const cartesia_mlx = (model) => [\n    `import mlx.core as mx\nimport cartesia_mlx as cmx\n\nmodel = cmx.from_pretrained(\"${model.id}\")\nmodel.set_dtype(mx.float32)   \n\nprompt = \"Rene Descartes was\"\n\nfor text in model.generate(\n    prompt,\n    max_tokens=500,\n    eval_every_n=5,\n    verbose=True,\n    top_p=0.99,\n    temperature=0.85,\n):\n    print(text, end=\"\", flush=True)\n`,\n];\nexport const edsnlp = (model) => {\n    const packageName = nameWithoutNamespace(model.id).replaceAll(\"-\", \"_\");\n    return [\n        `# Load it from the Hub directly\nimport edsnlp\nnlp = edsnlp.load(\"${model.id}\")\n`,\n        `# Or install it as a package\n!pip install git+https://huggingface.co/${model.id}\n\n# and import it as a module\nimport ${packageName}\n\nnlp = ${packageName}.load()  # or edsnlp.load(\"${packageName}\")\n`,\n    ];\n};\nexport const espnetTTS = (model) => [\n    `from espnet2.bin.tts_inference import Text2Speech\n\nmodel = Text2Speech.from_pretrained(\"${model.id}\")\n\nspeech, *_ = model(\"text to generate speech from\")`,\n];\nexport const espnetASR = (model) => [\n    `from espnet2.bin.asr_inference import Speech2Text\n\nmodel = Speech2Text.from_pretrained(\n  \"${model.id}\"\n)\n\nspeech, rate = soundfile.read(\"speech.wav\")\ntext, *_ = model(speech)[0]`,\n];\nconst espnetUnknown = () => [`unknown model type (must be text-to-speech or automatic-speech-recognition)`];\nexport const espnet = (model) => {\n    if (model.tags.includes(\"text-to-speech\")) {\n        return espnetTTS(model);\n    }\n    else if (model.tags.includes(\"automatic-speech-recognition\")) {\n        return espnetASR(model);\n    }\n    return espnetUnknown();\n};\nexport const fairseq = (model) => [\n    `from fairseq.checkpoint_utils import load_model_ensemble_and_task_from_hf_hub\n\nmodels, cfg, task = load_model_ensemble_and_task_from_hf_hub(\n    \"${model.id}\"\n)`,\n];\nexport const flair = (model) => [\n    `from flair.models import SequenceTagger\n\ntagger = SequenceTagger.load(\"${model.id}\")`,\n];\nexport const gliner = (model) => [\n    `from gliner import GLiNER\n\nmodel = GLiNER.from_pretrained(\"${model.id}\")`,\n];\nexport const indextts = (model) => [\n    `# Download model\nfrom huggingface_hub import snapshot_download\n\nsnapshot_download(${model.id}, local_dir=\"checkpoints\")\n\nfrom indextts.infer import IndexTTS\n\n# Ensure config.yaml is present in the checkpoints directory\ntts = IndexTTS(model_dir=\"checkpoints\", cfg_path=\"checkpoints/config.yaml\")\n\nvoice = \"path/to/your/reference_voice.wav\"  # Path to the voice reference audio file\ntext = \"Hello, how are you?\"\noutput_path = \"output_index.wav\"\n\ntts.infer(voice, text, output_path)`,\n];\nexport const htrflow = (model) => [\n    `# CLI usage\n# see docs: https://ai-riksarkivet.github.io/htrflow/latest/getting_started/quick_start.html\nhtrflow pipeline <path/to/pipeline.yaml> <path/to/image>`,\n    `# Python usage\nfrom htrflow.pipeline.pipeline import Pipeline\nfrom htrflow.pipeline.steps import Task\nfrom htrflow.models.framework.model import ModelClass\n\npipeline = Pipeline(\n    [\n        Task(\n            ModelClass, {\"model\": \"${model.id}\"}, {}\n        ),\n    ])`,\n];\nexport const keras = (model) => [\n    `# Available backend options are: \"jax\", \"torch\", \"tensorflow\".\nimport os\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\t\nimport keras\n\nmodel = keras.saving.load_model(\"hf://${model.id}\")\n`,\n];\nconst _keras_hub_causal_lm = (modelId) => `\nimport keras_hub\n\n# Load CausalLM model (optional: use half precision for inference)\ncausal_lm = keras_hub.models.CausalLM.from_preset(\"hf://${modelId}\", dtype=\"bfloat16\")\ncausal_lm.compile(sampler=\"greedy\")  # (optional) specify a sampler\n\n# Generate text\ncausal_lm.generate(\"Keras: deep learning for\", max_length=64)\n`;\nconst _keras_hub_text_to_image = (modelId) => `\nimport keras_hub\n\n# Load TextToImage model (optional: use half precision for inference)\ntext_to_image = keras_hub.models.TextToImage.from_preset(\"hf://${modelId}\", dtype=\"bfloat16\")\n\n# Generate images with a TextToImage model.\ntext_to_image.generate(\"Astronaut in a jungle\")\n`;\nconst _keras_hub_text_classifier = (modelId) => `\nimport keras_hub\n\n# Load TextClassifier model\ntext_classifier = keras_hub.models.TextClassifier.from_preset(\n    \"hf://${modelId}\",\n    num_classes=2,\n)\n# Fine-tune\ntext_classifier.fit(x=[\"Thilling adventure!\", \"Total snoozefest.\"], y=[1, 0])\n# Classify text\ntext_classifier.predict([\"Not my cup of tea.\"])\n`;\nconst _keras_hub_image_classifier = (modelId) => `\nimport keras_hub\nimport keras\n\n# Load ImageClassifier model\nimage_classifier = keras_hub.models.ImageClassifier.from_preset(\n    \"hf://${modelId}\",\n    num_classes=2,\n)\n# Fine-tune\nimage_classifier.fit(\n    x=keras.random.randint((32, 64, 64, 3), 0, 256),\n    y=keras.random.randint((32, 1), 0, 2),\n)\n# Classify image\nimage_classifier.predict(keras.random.randint((1, 64, 64, 3), 0, 256))\n`;\nconst _keras_hub_tasks_with_example = {\n    CausalLM: _keras_hub_causal_lm,\n    TextToImage: _keras_hub_text_to_image,\n    TextClassifier: _keras_hub_text_classifier,\n    ImageClassifier: _keras_hub_image_classifier,\n};\nconst _keras_hub_task_without_example = (task, modelId) => `\nimport keras_hub\n\n# Create a ${task} model\ntask = keras_hub.models.${task}.from_preset(\"hf://${modelId}\")\n`;\nconst _keras_hub_generic_backbone = (modelId) => `\nimport keras_hub\n\n# Create a Backbone model unspecialized for any task\nbackbone = keras_hub.models.Backbone.from_preset(\"hf://${modelId}\")\n`;\nexport const keras_hub = (model) => {\n    const modelId = model.id;\n    const tasks = model.config?.keras_hub?.tasks ?? [];\n    const snippets = [];\n    // First, generate tasks with examples\n    for (const [task, snippet] of Object.entries(_keras_hub_tasks_with_example)) {\n        if (tasks.includes(task)) {\n            snippets.push(snippet(modelId));\n        }\n    }\n    // Then, add remaining tasks\n    for (const task of tasks) {\n        if (!Object.keys(_keras_hub_tasks_with_example).includes(task)) {\n            snippets.push(_keras_hub_task_without_example(task, modelId));\n        }\n    }\n    // Finally, add generic backbone snippet\n    snippets.push(_keras_hub_generic_backbone(modelId));\n    return snippets;\n};\nexport const kimi_audio = (model) => [\n    `# Example usage for KimiAudio\n# pip install git+https://github.com/MoonshotAI/Kimi-Audio.git\n\nfrom kimia_infer.api.kimia import KimiAudio\n\nmodel = KimiAudio(model_path=\"${model.id}\", load_detokenizer=True)\n\nsampling_params = {\n    \"audio_temperature\": 0.8,\n    \"audio_top_k\": 10,\n    \"text_temperature\": 0.0,\n    \"text_top_k\": 5,\n}\n\n# For ASR\nasr_audio = \"asr_example.wav\"\nmessages_asr = [\n    {\"role\": \"user\", \"message_type\": \"text\", \"content\": \"Please transcribe the following audio:\"},\n    {\"role\": \"user\", \"message_type\": \"audio\", \"content\": asr_audio}\n]\n_, text = model.generate(messages_asr, **sampling_params, output_type=\"text\")\nprint(text)\n\n# For Q&A\nqa_audio = \"qa_example.wav\"\nmessages_conv = [{\"role\": \"user\", \"message_type\": \"audio\", \"content\": qa_audio}]\nwav, text = model.generate(messages_conv, **sampling_params, output_type=\"both\")\nsf.write(\"output_audio.wav\", wav.cpu().view(-1).numpy(), 24000)\nprint(text)\n`,\n];\nexport const kittentts = (model) => [\n    `from kittentts import KittenTTS\nm = KittenTTS(\"${model.id}\")\n\naudio = m.generate(\"This high quality TTS model works without a GPU\")\n\n# Save the audio\nimport soundfile as sf\nsf.write('output.wav', audio, 24000)`,\n];\nexport const lightning_ir = (model) => {\n    if (model.tags.includes(\"bi-encoder\")) {\n        return [\n            `#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule\nmodel = BiEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`,\n        ];\n    }\n    else if (model.tags.includes(\"cross-encoder\")) {\n        return [\n            `#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import CrossEncoderModule\nmodel = CrossEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`,\n        ];\n    }\n    return [\n        `#install from https://github.com/webis-de/lightning-ir\n\nfrom lightning_ir import BiEncoderModule, CrossEncoderModule\n\n# depending on the model type, use either BiEncoderModule or CrossEncoderModule\nmodel = BiEncoderModule(\"${model.id}\") \n# model = CrossEncoderModule(\"${model.id}\")\n\nmodel.score(\"query\", [\"doc1\", \"doc2\", \"doc3\"])`,\n    ];\n};\nexport const llama_cpp_python = (model) => {\n    const snippets = [\n        `# !pip install llama-cpp-python\n\nfrom llama_cpp import Llama\n\nllm = Llama.from_pretrained(\n\trepo_id=\"${model.id}\",\n\tfilename=\"{{GGUF_FILE}}\",\n)\n`,\n    ];\n    if (model.tags.includes(\"conversational\")) {\n        const messages = getModelInputSnippet(model);\n        snippets.push(`llm.create_chat_completion(\n\tmessages = ${stringifyMessages(messages, { attributeKeyQuotes: true, indent: \"\\t\" })}\n)`);\n    }\n    else {\n        snippets.push(`output = llm(\n\t\"Once upon a time,\",\n\tmax_tokens=512,\n\techo=True\n)\nprint(output)`);\n    }\n    return snippets;\n};\nexport const lerobot = (model) => {\n    if (model.tags.includes(\"smolvla\")) {\n        const smolvlaSnippets = [\n            // Installation snippet\n            `# See https://github.com/huggingface/lerobot?tab=readme-ov-file#installation for more details\ngit clone https://github.com/huggingface/lerobot.git\ncd lerobot\npip install -e .[smolvla]`,\n            // Finetune snippet\n            `# Launch finetuning on your dataset\npython lerobot/scripts/train.py \\\\\n--policy.path=${model.id} \\\\\n--dataset.repo_id=lerobot/svla_so101_pickplace \\\\ \n--batch_size=64 \\\\\n--steps=20000 \\\\\n--output_dir=outputs/train/my_smolvla \\\\\n--job_name=my_smolvla_training \\\\\n--policy.device=cuda \\\\\n--wandb.enable=true`,\n        ];\n        if (model.id !== \"lerobot/smolvla_base\") {\n            // Inference snippet (only if not base model)\n            smolvlaSnippets.push(`# Run the policy using the record function\t\npython -m lerobot.record \\\\\n  --robot.type=so101_follower \\\\\n  --robot.port=/dev/ttyACM0 \\\\ # <- Use your port\n  --robot.id=my_blue_follower_arm \\\\ # <- Use your robot id\n  --robot.cameras=\"{ front: {type: opencv, index_or_path: 8, width: 640, height: 480, fps: 30}}\" \\\\ # <- Use your cameras\n  --dataset.single_task=\"Grasp a lego block and put it in the bin.\" \\\\ # <- Use the same task description you used in your dataset recording\n  --dataset.repo_id=HF_USER/dataset_name \\\\  # <- This will be the dataset name on HF Hub\n  --dataset.episode_time_s=50 \\\\\n  --dataset.num_episodes=10 \\\\\n  --policy.path=${model.id}`);\n        }\n        return smolvlaSnippets;\n    }\n    return [];\n};\nexport const tf_keras = (model) => [\n    `# Note: 'keras<3.x' or 'tf_keras' must be installed (legacy)\n# See https://github.com/keras-team/tf-keras for more details.\nfrom huggingface_hub import from_pretrained_keras\n\nmodel = from_pretrained_keras(\"${model.id}\")\n`,\n];\nexport const mamba_ssm = (model) => [\n    `from mamba_ssm import MambaLMHeadModel\n\nmodel = MambaLMHeadModel.from_pretrained(\"${model.id}\")`,\n];\nexport const mars5_tts = (model) => [\n    `# Install from https://github.com/Camb-ai/MARS5-TTS\n\nfrom inference import Mars5TTS\nmars5 = Mars5TTS.from_pretrained(\"${model.id}\")`,\n];\nexport const matanyone = (model) => [\n    `# Install from https://github.com/pq-yang/MatAnyone.git\n\nfrom matanyone.model.matanyone import MatAnyone\nmodel = MatAnyone.from_pretrained(\"${model.id}\")`,\n    `\nfrom matanyone import InferenceCore\nprocessor = InferenceCore(\"${model.id}\")`,\n];\nexport const mesh_anything = () => [\n    `# Install from https://github.com/buaacyw/MeshAnything.git\n\nfrom MeshAnything.models.meshanything import MeshAnything\n\n# refer to https://github.com/buaacyw/MeshAnything/blob/main/main.py#L91 on how to define args\n# and https://github.com/buaacyw/MeshAnything/blob/main/app.py regarding usage\nmodel = MeshAnything(args)`,\n];\nexport const open_clip = (model) => [\n    `import open_clip\n\nmodel, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:${model.id}')\ntokenizer = open_clip.get_tokenizer('hf-hub:${model.id}')`,\n];\nexport const paddlenlp = (model) => {\n    if (model.config?.architectures?.[0]) {\n        const architecture = model.config.architectures[0];\n        return [\n            [\n                `from paddlenlp.transformers import AutoTokenizer, ${architecture}`,\n                \"\",\n                `tokenizer = AutoTokenizer.from_pretrained(\"${model.id}\", from_hf_hub=True)`,\n                `model = ${architecture}.from_pretrained(\"${model.id}\", from_hf_hub=True)`,\n            ].join(\"\\n\"),\n        ];\n    }\n    else {\n        return [\n            [\n                `# ⚠️ Type of model unknown`,\n                `from paddlenlp.transformers import AutoTokenizer, AutoModel`,\n                \"\",\n                `tokenizer = AutoTokenizer.from_pretrained(\"${model.id}\", from_hf_hub=True)`,\n                `model = AutoModel.from_pretrained(\"${model.id}\", from_hf_hub=True)`,\n            ].join(\"\\n\"),\n        ];\n    }\n};\nexport const paddleocr = (model) => {\n    const mapping = {\n        textline_detection: { className: \"TextDetection\" },\n        textline_recognition: { className: \"TextRecognition\" },\n        seal_text_detection: { className: \"SealTextDetection\" },\n        doc_img_unwarping: { className: \"TextImageUnwarping\" },\n        doc_img_orientation_classification: { className: \"DocImgOrientationClassification\" },\n        textline_orientation_classification: { className: \"TextLineOrientationClassification\" },\n        chart_parsing: { className: \"ChartParsing\" },\n        formula_recognition: { className: \"FormulaRecognition\" },\n        layout_detection: { className: \"LayoutDetection\" },\n        table_cells_detection: { className: \"TableCellsDetection\" },\n        wired_table_classification: { className: \"TableClassification\" },\n        table_structure_recognition: { className: \"TableStructureRecognition\" },\n    };\n    if (model.tags.includes(\"doc_vlm\")) {\n        return [\n            `# 1. See https://www.paddlepaddle.org.cn/en/install to install paddlepaddle\n# 2. pip install paddleocr\n\nfrom paddleocr import DocVLM\nmodel = DocVLM(model_name=\"${nameWithoutNamespace(model.id)}\")\noutput = model.predict(\n    input={\"image\": \"path/to/image.png\", \"query\": \"Parsing this image and output the content in Markdown format.\"},\n    batch_size=1\n)\nfor res in output:\n    res.print()\n    res.save_to_json(save_path=\"./output/res.json\")`,\n        ];\n    }\n    if (model.tags.includes(\"document-parse\")) {\n        return [\n            `# See https://www.paddleocr.ai/latest/version3.x/pipeline_usage/PaddleOCR-VL.html to installation\n\nfrom paddleocr import PaddleOCRVL\npipeline = PaddleOCRVL()\noutput = pipeline.predict(\"path/to/document_image.png\")\nfor res in output:\n\tres.print()\n\tres.save_to_json(save_path=\"output\")\n\tres.save_to_markdown(save_path=\"output\")`,\n        ];\n    }\n    for (const tag of model.tags) {\n        if (tag in mapping) {\n            const { className } = mapping[tag];\n            return [\n                `# 1. See https://www.paddlepaddle.org.cn/en/install to install paddlepaddle\n# 2. pip install paddleocr\n\nfrom paddleocr import ${className}\nmodel = ${className}(model_name=\"${nameWithoutNamespace(model.id)}\")\noutput = model.predict(input=\"path/to/image.png\", batch_size=1)\nfor res in output:\n    res.print()\n    res.save_to_img(save_path=\"./output/\")\n    res.save_to_json(save_path=\"./output/res.json\")`,\n            ];\n        }\n    }\n    return [\n        `# Please refer to the document for information on how to use the model. \n# https://paddlepaddle.github.io/PaddleOCR/latest/en/version3.x/module_usage/module_overview.html`,\n    ];\n};\nexport const perception_encoder = (model) => {\n    const clip_model = `# Use PE-Core models as CLIP models\nimport core.vision_encoder.pe as pe\n\nmodel = pe.CLIP.from_config(\"${model.id}\", pretrained=True)`;\n    const vision_encoder = `# Use any PE model as a vision encoder\nimport core.vision_encoder.pe as pe\n\nmodel = pe.VisionTransformer.from_config(\"${model.id}\", pretrained=True)`;\n    if (model.id.includes(\"Core\")) {\n        return [clip_model, vision_encoder];\n    }\n    else {\n        return [vision_encoder];\n    }\n};\nexport const phantom_wan = (model) => [\n    `from huggingface_hub import snapshot_download\nfrom phantom_wan import WANI2V, configs\n\ncheckpoint_dir = snapshot_download(\"${model.id}\")\nwan_i2v = WanI2V(\n            config=configs.WAN_CONFIGS['i2v-14B'],\n            checkpoint_dir=checkpoint_dir,\n        )\n video = wan_i2v.generate(text_prompt, image_prompt)`,\n];\nexport const pyannote_audio_pipeline = (model) => [\n    `from pyannote.audio import Pipeline\n  \npipeline = Pipeline.from_pretrained(\"${model.id}\")\n\n# inference on the whole file\npipeline(\"file.wav\")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\n\nfrom pyannote.audio import Audio\nwaveform, sample_rate = Audio().crop(\"file.wav\", excerpt)\npipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})`,\n];\nconst pyannote_audio_model = (model) => [\n    `from pyannote.audio import Model, Inference\n\nmodel = Model.from_pretrained(\"${model.id}\")\ninference = Inference(model)\n\n# inference on the whole file\ninference(\"file.wav\")\n\n# inference on an excerpt\nfrom pyannote.core import Segment\nexcerpt = Segment(start=2.0, end=5.0)\ninference.crop(\"file.wav\", excerpt)`,\n];\nexport const pyannote_audio = (model) => {\n    if (model.tags.includes(\"pyannote-audio-pipeline\")) {\n        return pyannote_audio_pipeline(model);\n    }\n    return pyannote_audio_model(model);\n};\nexport const relik = (model) => [\n    `from relik import Relik\n \nrelik = Relik.from_pretrained(\"${model.id}\")`,\n];\nexport const renderformer = (model) => [\n    `# Install from https://github.com/microsoft/renderformer\n\nfrom renderformer import RenderFormerRenderingPipeline\npipeline = RenderFormerRenderingPipeline.from_pretrained(\"${model.id}\")`,\n];\nconst tensorflowttsTextToMel = (model) => [\n    `from tensorflow_tts.inference import AutoProcessor, TFAutoModel\n\nprocessor = AutoProcessor.from_pretrained(\"${model.id}\")\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\n`,\n];\nconst tensorflowttsMelToWav = (model) => [\n    `from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\naudios = model.inference(mels)\n`,\n];\nconst tensorflowttsUnknown = (model) => [\n    `from tensorflow_tts.inference import TFAutoModel\n\nmodel = TFAutoModel.from_pretrained(\"${model.id}\")\n`,\n];\nexport const tensorflowtts = (model) => {\n    if (model.tags.includes(\"text-to-mel\")) {\n        return tensorflowttsTextToMel(model);\n    }\n    else if (model.tags.includes(\"mel-to-wav\")) {\n        return tensorflowttsMelToWav(model);\n    }\n    return tensorflowttsUnknown(model);\n};\nexport const timm = (model) => [\n    `import timm\n\nmodel = timm.create_model(\"hf_hub:${model.id}\", pretrained=True)`,\n];\nexport const saelens = ( /* model: ModelData */) => [\n    `# pip install sae-lens\nfrom sae_lens import SAE\n\nsae, cfg_dict, sparsity = SAE.from_pretrained(\n    release = \"RELEASE_ID\", # e.g., \"gpt2-small-res-jb\". See other options in https://github.com/jbloomAus/SAELens/blob/main/sae_lens/pretrained_saes.yaml\n    sae_id = \"SAE_ID\", # e.g., \"blocks.8.hook_resid_pre\". Won't always be a hook point\n)`,\n];\nexport const seed_story = () => [\n    `# seed_story_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/agent_7b_sft.yaml'\n# llm_cfg_path refers to 'https://github.com/TencentARC/SEED-Story/blob/master/configs/clm_models/llama2chat7b_lora.yaml'\nfrom omegaconf import OmegaConf\nimport hydra\n\n# load Llama2\nllm_cfg = OmegaConf.load(llm_cfg_path)\nllm = hydra.utils.instantiate(llm_cfg, torch_dtype=\"fp16\")\n\n# initialize seed_story\nseed_story_cfg = OmegaConf.load(seed_story_cfg_path)\nseed_story = hydra.utils.instantiate(seed_story_cfg, llm=llm) `,\n];\nconst skopsPickle = (model, modelFile) => {\n    return [\n        `import joblib\nfrom skops.hub_utils import download\ndownload(\"${model.id}\", \"path_to_folder\")\nmodel = joblib.load(\n\t\"${modelFile}\"\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`,\n    ];\n};\nconst skopsFormat = (model, modelFile) => {\n    return [\n        `from skops.hub_utils import download\nfrom skops.io import load\ndownload(\"${model.id}\", \"path_to_folder\")\n# make sure model file is in skops format\n# if model is a pickle file, make sure it's from a source you trust\nmodel = load(\"path_to_folder/${modelFile}\")`,\n    ];\n};\nconst skopsJobLib = (model) => {\n    return [\n        `from huggingface_hub import hf_hub_download\nimport joblib\nmodel = joblib.load(\n\thf_hub_download(\"${model.id}\", \"sklearn_model.joblib\")\n)\n# only load pickle files from sources you trust\n# read more about it here https://skops.readthedocs.io/en/stable/persistence.html`,\n    ];\n};\nexport const sklearn = (model) => {\n    if (model.tags.includes(\"skops\")) {\n        const skopsmodelFile = model.config?.sklearn?.model?.file;\n        const skopssaveFormat = model.config?.sklearn?.model_format;\n        if (!skopsmodelFile) {\n            return [`# ⚠️ Model filename not specified in config.json`];\n        }\n        if (skopssaveFormat === \"pickle\") {\n            return skopsPickle(model, skopsmodelFile);\n        }\n        else {\n            return skopsFormat(model, skopsmodelFile);\n        }\n    }\n    else {\n        return skopsJobLib(model);\n    }\n};\nexport const stable_audio_tools = (model) => [\n    `import torch\nimport torchaudio\nfrom einops import rearrange\nfrom stable_audio_tools import get_pretrained_model\nfrom stable_audio_tools.inference.generation import generate_diffusion_cond\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Download model\nmodel, model_config = get_pretrained_model(\"${model.id}\")\nsample_rate = model_config[\"sample_rate\"]\nsample_size = model_config[\"sample_size\"]\n\nmodel = model.to(device)\n\n# Set up text and timing conditioning\nconditioning = [{\n\t\"prompt\": \"128 BPM tech house drum loop\",\n}]\n\n# Generate stereo audio\noutput = generate_diffusion_cond(\n\tmodel,\n\tconditioning=conditioning,\n\tsample_size=sample_size,\n\tdevice=device\n)\n\n# Rearrange audio batch to a single sequence\noutput = rearrange(output, \"b d n -> d (b n)\")\n\n# Peak normalize, clip, convert to int16, and save to file\noutput = output.to(torch.float32).div(torch.max(torch.abs(output))).clamp(-1, 1).mul(32767).to(torch.int16).cpu()\ntorchaudio.save(\"output.wav\", output, sample_rate)`,\n];\nexport const fastai = (model) => [\n    `from huggingface_hub import from_pretrained_fastai\n\nlearn = from_pretrained_fastai(\"${model.id}\")`,\n];\nexport const sam2 = (model) => {\n    const image_predictor = `# Use SAM2 with images\nimport torch\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor\n\npredictor = SAM2ImagePredictor.from_pretrained(${model.id})\n\nwith torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    predictor.set_image(<your_image>)\n    masks, _, _ = predictor.predict(<input_prompts>)`;\n    const video_predictor = `# Use SAM2 with videos\nimport torch\nfrom sam2.sam2_video_predictor import SAM2VideoPredictor\n\t\npredictor = SAM2VideoPredictor.from_pretrained(${model.id})\n\nwith torch.inference_mode(), torch.autocast(\"cuda\", dtype=torch.bfloat16):\n    state = predictor.init_state(<your_video>)\n\n    # add new prompts and instantly get the output on the same frame\n    frame_idx, object_ids, masks = predictor.add_new_points(state, <your_prompts>):\n\n    # propagate the prompts to get masklets throughout the video\n    for frame_idx, object_ids, masks in predictor.propagate_in_video(state):\n        ...`;\n    return [image_predictor, video_predictor];\n};\nexport const sampleFactory = (model) => [\n    `python -m sample_factory.huggingface.load_from_hub -r ${model.id} -d ./train_dir`,\n];\nfunction get_widget_examples_from_st_model(model) {\n    const widgetExample = model.widgetData?.[0];\n    if (widgetExample?.source_sentence && widgetExample?.sentences?.length) {\n        return [widgetExample.source_sentence, ...widgetExample.sentences];\n    }\n}\nexport const sentenceTransformers = (model) => {\n    const remote_code_snippet = model.tags.includes(TAG_CUSTOM_CODE) ? \", trust_remote_code=True\" : \"\";\n    if (model.tags.includes(\"PyLate\")) {\n        return [\n            `from pylate import models\n\nqueries = [\n    \"Which planet is known as the Red Planet?\",\n    \"What is the largest planet in our solar system?\",\n]\n\ndocuments = [\n    [\"Mars is the Red Planet.\", \"Venus is Earth's twin.\"],\n    [\"Jupiter is the largest planet.\", \"Saturn has rings.\"],\n]\n\nmodel = models.ColBERT(model_name_or_path=\"${model.id}\")\n\nqueries_emb = model.encode(queries, is_query=True)\ndocs_emb = model.encode(documents, is_query=False)`,\n        ];\n    }\n    if (model.tags.includes(\"cross-encoder\") || model.pipeline_tag == \"text-ranking\") {\n        return [\n            `from sentence_transformers import CrossEncoder\n\nmodel = CrossEncoder(\"${model.id}\"${remote_code_snippet})\n\nquery = \"Which planet is known as the Red Planet?\"\npassages = [\n\t\"Venus is often called Earth's twin because of its similar size and proximity.\",\n\t\"Mars, known for its reddish appearance, is often referred to as the Red Planet.\",\n\t\"Jupiter, the largest planet in our solar system, has a prominent red spot.\",\n\t\"Saturn, famous for its rings, is sometimes mistaken for the Red Planet.\"\n]\n\nscores = model.predict([(query, passage) for passage in passages])\nprint(scores)`,\n        ];\n    }\n    const exampleSentences = get_widget_examples_from_st_model(model) ?? [\n        \"The weather is lovely today.\",\n        \"It's so sunny outside!\",\n        \"He drove to the stadium.\",\n    ];\n    return [\n        `from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"${model.id}\"${remote_code_snippet})\n\nsentences = ${JSON.stringify(exampleSentences, null, 4)}\nembeddings = model.encode(sentences)\n\nsimilarities = model.similarity(embeddings, embeddings)\nprint(similarities.shape)\n# [${exampleSentences.length}, ${exampleSentences.length}]`,\n    ];\n};\nexport const setfit = (model) => [\n    `from setfit import SetFitModel\n\nmodel = SetFitModel.from_pretrained(\"${model.id}\")`,\n];\nexport const spacy = (model) => [\n    `!pip install https://huggingface.co/${model.id}/resolve/main/${nameWithoutNamespace(model.id)}-any-py3-none-any.whl\n\n# Using spacy.load().\nimport spacy\nnlp = spacy.load(\"${nameWithoutNamespace(model.id)}\")\n\n# Importing as module.\nimport ${nameWithoutNamespace(model.id)}\nnlp = ${nameWithoutNamespace(model.id)}.load()`,\n];\nexport const span_marker = (model) => [\n    `from span_marker import SpanMarkerModel\n\nmodel = SpanMarkerModel.from_pretrained(\"${model.id}\")`,\n];\nexport const stanza = (model) => [\n    `import stanza\n\nstanza.download(\"${nameWithoutNamespace(model.id).replace(\"stanza-\", \"\")}\")\nnlp = stanza.Pipeline(\"${nameWithoutNamespace(model.id).replace(\"stanza-\", \"\")}\")`,\n];\nconst speechBrainMethod = (speechbrainInterface) => {\n    switch (speechbrainInterface) {\n        case \"EncoderClassifier\":\n            return \"classify_file\";\n        case \"EncoderDecoderASR\":\n        case \"EncoderASR\":\n            return \"transcribe_file\";\n        case \"SpectralMaskEnhancement\":\n            return \"enhance_file\";\n        case \"SepformerSeparation\":\n            return \"separate_file\";\n        default:\n            return undefined;\n    }\n};\nexport const speechbrain = (model) => {\n    const speechbrainInterface = model.config?.speechbrain?.speechbrain_interface;\n    if (speechbrainInterface === undefined) {\n        return [`# interface not specified in config.json`];\n    }\n    const speechbrainMethod = speechBrainMethod(speechbrainInterface);\n    if (speechbrainMethod === undefined) {\n        return [`# interface in config.json invalid`];\n    }\n    return [\n        `from speechbrain.pretrained import ${speechbrainInterface}\nmodel = ${speechbrainInterface}.from_hparams(\n  \"${model.id}\"\n)\nmodel.${speechbrainMethod}(\"file.wav\")`,\n    ];\n};\nexport const terratorch = (model) => [\n    `from terratorch.registry import BACKBONE_REGISTRY\n\nmodel = BACKBONE_REGISTRY.build(\"${model.id}\")`,\n];\nconst hasChatTemplate = (model) => model.config?.tokenizer_config?.chat_template !== undefined ||\n    model.config?.processor_config?.chat_template !== undefined ||\n    model.config?.chat_template_jinja !== undefined;\nexport const transformers = (model) => {\n    const info = model.transformersInfo;\n    if (!info) {\n        return [`# ⚠️ Type of model unknown`];\n    }\n    const remote_code_snippet = model.tags.includes(TAG_CUSTOM_CODE) ? \", trust_remote_code=True\" : \"\";\n    const autoSnippet = [];\n    if (info.processor) {\n        const processorVarName = info.processor === \"AutoTokenizer\"\n            ? \"tokenizer\"\n            : info.processor === \"AutoFeatureExtractor\"\n                ? \"extractor\"\n                : \"processor\";\n        autoSnippet.push(\"# Load model directly\", `from transformers import ${info.processor}, ${info.auto_model}`, \"\", `${processorVarName} = ${info.processor}.from_pretrained(\"${model.id}\"` + remote_code_snippet + \")\", `model = ${info.auto_model}.from_pretrained(\"${model.id}\"` + remote_code_snippet + \")\");\n        if (model.tags.includes(\"conversational\") && hasChatTemplate(model)) {\n            if (model.tags.includes(\"image-text-to-text\")) {\n                autoSnippet.push(\"messages = [\", [\n                    \"    {\",\n                    '        \"role\": \"user\",',\n                    '        \"content\": [',\n                    '            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},',\n                    '            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}',\n                    \"        ]\",\n                    \"    },\",\n                ].join(\"\\n\"), \"]\");\n            }\n            else {\n                autoSnippet.push(\"messages = [\", '    {\"role\": \"user\", \"content\": \"Who are you?\"},', \"]\");\n            }\n            autoSnippet.push(`inputs = ${processorVarName}.apply_chat_template(`, \"\tmessages,\", \"\tadd_generation_prompt=True,\", \"\ttokenize=True,\", \"\treturn_dict=True,\", '\treturn_tensors=\"pt\",', \").to(model.device)\", \"\", \"outputs = model.generate(**inputs, max_new_tokens=40)\", `print(${processorVarName}.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))`);\n        }\n    }\n    else {\n        autoSnippet.push(\"# Load model directly\", `from transformers import ${info.auto_model}`, `model = ${info.auto_model}.from_pretrained(\"${model.id}\"` + remote_code_snippet + ', torch_dtype=\"auto\")');\n    }\n    if (model.pipeline_tag && LIBRARY_TASK_MAPPING.transformers?.includes(model.pipeline_tag)) {\n        const pipelineSnippet = [\n            \"# Use a pipeline as a high-level helper\",\n            \"from transformers import pipeline\",\n            \"\",\n            `pipe = pipeline(\"${model.pipeline_tag}\", model=\"${model.id}\"` + remote_code_snippet + \")\",\n        ];\n        if (model.tags.includes(\"conversational\")) {\n            if (model.tags.includes(\"image-text-to-text\")) {\n                pipelineSnippet.push(\"messages = [\", [\n                    \"    {\",\n                    '        \"role\": \"user\",',\n                    '        \"content\": [',\n                    '            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},',\n                    '            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}',\n                    \"        ]\",\n                    \"    },\",\n                ].join(\"\\n\"), \"]\");\n                pipelineSnippet.push(\"pipe(text=messages)\");\n            }\n            else {\n                pipelineSnippet.push(\"messages = [\", '    {\"role\": \"user\", \"content\": \"Who are you?\"},', \"]\");\n                pipelineSnippet.push(\"pipe(messages)\");\n            }\n        }\n        else if (model.pipeline_tag === \"zero-shot-image-classification\") {\n            pipelineSnippet.push(\"pipe(\", '    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\",', '    candidate_labels=[\"animals\", \"humans\", \"landscape\"],', \")\");\n        }\n        else if (model.pipeline_tag === \"image-classification\") {\n            pipelineSnippet.push('pipe(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/parrots.png\")');\n        }\n        return [pipelineSnippet.join(\"\\n\"), autoSnippet.join(\"\\n\")];\n    }\n    return [autoSnippet.join(\"\\n\")];\n};\nexport const transformersJS = (model) => {\n    if (!model.pipeline_tag) {\n        return [`// ⚠️ Unknown pipeline tag`];\n    }\n    const libName = \"@huggingface/transformers\";\n    return [\n        `// npm i ${libName}\nimport { pipeline } from '${libName}';\n\n// Allocate pipeline\nconst pipe = await pipeline('${model.pipeline_tag}', '${model.id}');`,\n    ];\n};\nconst peftTask = (peftTaskType) => {\n    switch (peftTaskType) {\n        case \"CAUSAL_LM\":\n            return \"CausalLM\";\n        case \"SEQ_2_SEQ_LM\":\n            return \"Seq2SeqLM\";\n        case \"TOKEN_CLS\":\n            return \"TokenClassification\";\n        case \"SEQ_CLS\":\n            return \"SequenceClassification\";\n        default:\n            return undefined;\n    }\n};\nexport const peft = (model) => {\n    const { base_model_name_or_path: peftBaseModel, task_type: peftTaskType } = model.config?.peft ?? {};\n    const pefttask = peftTask(peftTaskType);\n    if (!pefttask) {\n        return [`Task type is invalid.`];\n    }\n    if (!peftBaseModel) {\n        return [`Base model is not found.`];\n    }\n    return [\n        `from peft import PeftModel\nfrom transformers import AutoModelFor${pefttask}\n\nbase_model = AutoModelFor${pefttask}.from_pretrained(\"${peftBaseModel}\")\nmodel = PeftModel.from_pretrained(base_model, \"${model.id}\")`,\n    ];\n};\nexport const fasttext = (model) => [\n    `from huggingface_hub import hf_hub_download\nimport fasttext\n\nmodel = fasttext.load_model(hf_hub_download(\"${model.id}\", \"model.bin\"))`,\n];\nexport const stableBaselines3 = (model) => [\n    `from huggingface_sb3 import load_from_hub\ncheckpoint = load_from_hub(\n\trepo_id=\"${model.id}\",\n\tfilename=\"{MODEL FILENAME}.zip\",\n)`,\n];\nconst nemoDomainResolver = (domain, model) => {\n    switch (domain) {\n        case \"ASR\":\n            return [\n                `import nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(\"${model.id}\")\n\ntranscriptions = asr_model.transcribe([\"file.wav\"])`,\n            ];\n        default:\n            return undefined;\n    }\n};\nexport const mlAgents = (model) => [\n    `mlagents-load-from-hf --repo-id=\"${model.id}\" --local-dir=\"./download: string[]s\"`,\n];\nexport const sentis = ( /* model: ModelData */) => [\n    `string modelName = \"[Your model name here].sentis\";\nModel model = ModelLoader.Load(Application.streamingAssetsPath + \"/\" + modelName);\nIWorker engine = WorkerFactory.CreateWorker(BackendType.GPUCompute, model);\n// Please see provided C# file for more details\n`,\n];\nexport const sana = (model) => [\n    `\n# Load the model and infer image from text\nimport torch\nfrom app.sana_pipeline import SanaPipeline\nfrom torchvision.utils import save_image\n\nsana = SanaPipeline(\"configs/sana_config/1024ms/Sana_1600M_img1024.yaml\")\nsana.from_pretrained(\"hf://${model.id}\")\n\nimage = sana(\n    prompt='a cyberpunk cat with a neon sign that says \"Sana\"',\n    height=1024,\n    width=1024,\n    guidance_scale=5.0,\n    pag_guidance_scale=2.0,\n    num_inference_steps=18,\n) `,\n];\nexport const vibevoice = (model) => [\n    `import torch, soundfile as sf, librosa, numpy as np\nfrom vibevoice.processor.vibevoice_processor import VibeVoiceProcessor\nfrom vibevoice.modular.modeling_vibevoice_inference import VibeVoiceForConditionalGenerationInference\n\n# Load voice sample (should be 24kHz mono)\nvoice, sr = sf.read(\"path/to/voice_sample.wav\")\nif voice.ndim > 1: voice = voice.mean(axis=1)\nif sr != 24000: voice = librosa.resample(voice, sr, 24000)\n\nprocessor = VibeVoiceProcessor.from_pretrained(\"${model.id}\")\nmodel = VibeVoiceForConditionalGenerationInference.from_pretrained(\n    \"${model.id}\", torch_dtype=torch.bfloat16\n).to(\"cuda\").eval()\nmodel.set_ddpm_inference_steps(5)\n\ninputs = processor(text=[\"Speaker 0: Hello!\\\\nSpeaker 1: Hi there!\"],\n                   voice_samples=[[voice]], return_tensors=\"pt\")\naudio = model.generate(**inputs, cfg_scale=1.3,\n                       tokenizer=processor.tokenizer).speech_outputs[0]\nsf.write(\"output.wav\", audio.cpu().numpy().squeeze(), 24000)`,\n];\nexport const videoprism = (model) => [\n    `# Install from https://github.com/google-deepmind/videoprism\nimport jax\nfrom videoprism import models as vp\n\nflax_model = vp.get_model(\"${model.id}\")\nloaded_state = vp.load_pretrained_weights(\"${model.id}\")\n\n@jax.jit\ndef forward_fn(inputs, train=False):\n  return flax_model.apply(loaded_state, inputs, train=train)`,\n];\nexport const vfimamba = (model) => [\n    `from Trainer_finetune import Model\n\nmodel = Model.from_pretrained(\"${model.id}\")`,\n];\nexport const lvface = (model) => [\n    `from huggingface_hub import hf_hub_download\n\t from inference_onnx import LVFaceONNXInferencer\n\nmodel_path = hf_hub_download(\"${model.id}\", \"LVFace-L_Glint360K/LVFace-L_Glint360K.onnx\")\ninferencer = LVFaceONNXInferencer(model_path, use_gpu=True, timeout=300)\nimg_path = 'path/to/image1.jpg'\nembedding = inferencer.infer_from_image(img_path)`,\n];\nexport const voicecraft = (model) => [\n    `from voicecraft import VoiceCraft\n\nmodel = VoiceCraft.from_pretrained(\"${model.id}\")`,\n];\nexport const voxcpm = (model) => [\n    `import soundfile as sf\nfrom voxcpm import VoxCPM\n\nmodel = VoxCPM.from_pretrained(\"${model.id}\")\n\nwav = model.generate(\n    text=\"VoxCPM is an innovative end-to-end TTS model from ModelBest, designed to generate highly expressive speech.\",\n    prompt_wav_path=None,      # optional: path to a prompt speech for voice cloning\n    prompt_text=None,          # optional: reference text\n    cfg_value=2.0,             # LM guidance on LocDiT, higher for better adherence to the prompt, but maybe worse\n    inference_timesteps=10,   # LocDiT inference timesteps, higher for better result, lower for fast speed\n    normalize=True,           # enable external TN tool\n    denoise=True,             # enable external Denoise tool\n    retry_badcase=True,        # enable retrying mode for some bad cases (unstoppable)\n    retry_badcase_max_times=3,  # maximum retrying times\n    retry_badcase_ratio_threshold=6.0, # maximum length restriction for bad case detection (simple but effective), it could be adjusted for slow pace speech\n)\n\nsf.write(\"output.wav\", wav, 16000)\nprint(\"saved: output.wav\")`,\n];\nexport const vui = () => [\n    `# !pip install git+https://github.com/fluxions-ai/vui\n\nimport torchaudio\n\nfrom vui.inference import render\nfrom vui.model import Vui,\n\nmodel = Vui.from_pretrained().cuda()\nwaveform = render(\n    model,\n    \"Hey, here is some random stuff, usually something quite long as the shorter the text the less likely the model can cope!\",\n)\nprint(waveform.shape)\ntorchaudio.save(\"out.opus\", waveform[0], 22050)\n`,\n];\nexport const chattts = () => [\n    `import ChatTTS\nimport torchaudio\n\nchat = ChatTTS.Chat()\nchat.load_models(compile=False) # Set to True for better performance\n\ntexts = [\"PUT YOUR TEXT HERE\",]\n\nwavs = chat.infer(texts, )\n\ntorchaudio.save(\"output1.wav\", torch.from_numpy(wavs[0]), 24000)`,\n];\nexport const ultralytics = (model) => {\n    // ultralytics models must have a version tag (e.g. `yolov8`)\n    const versionTag = model.tags.find((tag) => tag.match(/^yolov\\d+$/));\n    const className = versionTag ? `YOLOv${versionTag.slice(4)}` : \"YOLOvXX\";\n    const prefix = versionTag\n        ? \"\"\n        : `# Couldn't find a valid YOLO version tag.\\n# Replace XX with the correct version.\\n`;\n    return [\n        prefix +\n            `from ultralytics import ${className}\n\nmodel = ${className}.from_pretrained(\"${model.id}\")\nsource = 'http://images.cocodataset.org/val2017/000000039769.jpg'\nmodel.predict(source=source, save=True)`,\n    ];\n};\nexport const birefnet = (model) => [\n    `# Option 1: use with transformers\n\nfrom transformers import AutoModelForImageSegmentation\nbirefnet = AutoModelForImageSegmentation.from_pretrained(\"${model.id}\", trust_remote_code=True)\n`,\n    `# Option 2: use with BiRefNet\n\n# Install from https://github.com/ZhengPeng7/BiRefNet\n\nfrom models.birefnet import BiRefNet\nmodel = BiRefNet.from_pretrained(\"${model.id}\")`,\n];\nexport const swarmformer = (model) => [\n    `from swarmformer import SwarmFormerModel\n\nmodel = SwarmFormerModel.from_pretrained(\"${model.id}\")\n`,\n];\nexport const univa = (model) => [\n    `# Follow installation instructions at https://github.com/PKU-YuanGroup/UniWorld-V1\n\nfrom univa.models.qwen2p5vl.modeling_univa_qwen2p5vl import UnivaQwen2p5VLForConditionalGeneration\n\tmodel = UnivaQwen2p5VLForConditionalGeneration.from_pretrained(\n        \"${model.id}\",\n        torch_dtype=torch.bfloat16,\n        attn_implementation=\"flash_attention_2\",\n    ).to(\"cuda\")\n\tprocessor = AutoProcessor.from_pretrained(\"${model.id}\")\n`,\n];\nconst mlx_unknown = (model) => [\n    `# Download the model from the Hub\npip install huggingface_hub[hf_xet]\n\nhuggingface-cli download --local-dir ${nameWithoutNamespace(model.id)} ${model.id}`,\n];\nconst mlxlm = (model) => [\n    `# Make sure mlx-lm is installed\n# pip install --upgrade mlx-lm\n# if on a CUDA device, also pip install mlx[cuda]\n\n# Generate text with mlx-lm\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"${model.id}\")\n\nprompt = \"Once upon a time in\"\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)`,\n];\nconst mlxchat = (model) => [\n    `# Make sure mlx-lm is installed\n# pip install --upgrade mlx-lm\n\n# Generate text with mlx-lm\nfrom mlx_lm import load, generate\n\nmodel, tokenizer = load(\"${model.id}\")\n\nprompt = \"Write a story about Einstein\"\nmessages = [{\"role\": \"user\", \"content\": prompt}]\nprompt = tokenizer.apply_chat_template(\n    messages, add_generation_prompt=True\n)\n\ntext = generate(model, tokenizer, prompt=prompt, verbose=True)`,\n];\nconst mlxvlm = (model) => [\n    `# Make sure mlx-vlm is installed\n# pip install --upgrade mlx-vlm\n\nfrom mlx_vlm import load, generate\nfrom mlx_vlm.prompt_utils import apply_chat_template\nfrom mlx_vlm.utils import load_config\n\n# Load the model\nmodel, processor = load(\"${model.id}\")\nconfig = load_config(\"${model.id}\")\n\n# Prepare input\nimage = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nprompt = \"Describe this image.\"\n\n# Apply chat template\nformatted_prompt = apply_chat_template(\n    processor, config, prompt, num_images=1\n)\n\n# Generate output\noutput = generate(model, processor, formatted_prompt, image)\nprint(output)`,\n];\nexport const mlxim = (model) => [\n    `from mlxim.model import create_model\n\nmodel = create_model(${model.id})`,\n];\nexport const mlx = (model) => {\n    if (model.pipeline_tag === \"image-text-to-text\") {\n        return mlxvlm(model);\n    }\n    if (model.pipeline_tag === \"text-generation\") {\n        if (model.tags.includes(\"conversational\")) {\n            return mlxchat(model);\n        }\n        else {\n            return mlxlm(model);\n        }\n    }\n    return mlx_unknown(model);\n};\nexport const model2vec = (model) => [\n    `from model2vec import StaticModel\n\nmodel = StaticModel.from_pretrained(\"${model.id}\")`,\n];\nexport const pruna = (model) => {\n    let snippets;\n    if (model.tags.includes(\"diffusers\")) {\n        snippets = pruna_diffusers(model);\n    }\n    else if (model.tags.includes(\"transformers\")) {\n        snippets = pruna_transformers(model);\n    }\n    else {\n        snippets = pruna_default(model);\n    }\n    const ensurePrunaModelImport = (snippet) => {\n        if (!/^from pruna import PrunaModel/m.test(snippet)) {\n            return `from pruna import PrunaModel\\n${snippet}`;\n        }\n        return snippet;\n    };\n    snippets = snippets.map(ensurePrunaModelImport);\n    if (model.tags.includes(\"pruna_pro-ai\")) {\n        return snippets.map((snippet) => snippet.replace(/\\bpruna\\b/g, \"pruna_pro\").replace(/\\bPrunaModel\\b/g, \"PrunaProModel\"));\n    }\n    return snippets;\n};\nconst pruna_diffusers = (model) => {\n    const diffusersSnippets = diffusers(model);\n    return diffusersSnippets.map((snippet) => snippet\n        // Replace pipeline classes with PrunaModel\n        .replace(/\\b\\w*Pipeline\\w*\\b/g, \"PrunaModel\")\n        // Clean up diffusers imports containing PrunaModel\n        .replace(/from diffusers import ([^,\\n]*PrunaModel[^,\\n]*)/g, \"\")\n        .replace(/from diffusers import ([^,\\n]+),?\\s*([^,\\n]*PrunaModel[^,\\n]*)/g, \"from diffusers import $1\")\n        .replace(/from diffusers import\\s*(\\n|$)/g, \"\")\n        // Fix PrunaModel imports\n        .replace(/from diffusers import PrunaModel/g, \"from pruna import PrunaModel\")\n        .replace(/from diffusers import ([^,\\n]+), PrunaModel/g, \"from diffusers import $1\")\n        .replace(/from diffusers import PrunaModel, ([^,\\n]+)/g, \"from diffusers import $1\")\n        // Clean up whitespace\n        .replace(/\\n\\n+/g, \"\\n\")\n        .trim());\n};\nconst pruna_transformers = (model) => {\n    const info = model.transformersInfo;\n    const transformersSnippets = transformers(model);\n    // Replace pipeline with PrunaModel\n    let processedSnippets = transformersSnippets.map((snippet) => snippet\n        .replace(/from transformers import pipeline/g, \"from pruna import PrunaModel\")\n        .replace(/pipeline\\([^)]*\\)/g, `PrunaModel.from_pretrained(\"${model.id}\")`));\n    // Additional cleanup if auto_model info is available\n    if (info?.auto_model) {\n        processedSnippets = processedSnippets.map((snippet) => snippet\n            .replace(new RegExp(`from transformers import ${info.auto_model}\\n?`, \"g\"), \"\")\n            .replace(new RegExp(`${info.auto_model}.from_pretrained`, \"g\"), \"PrunaModel.from_pretrained\")\n            .replace(new RegExp(`^.*from.*import.*(, *${info.auto_model})+.*$`, \"gm\"), (line) => line.replace(new RegExp(`, *${info.auto_model}`, \"g\"), \"\")));\n    }\n    return processedSnippets;\n};\nconst pruna_default = (model) => [\n    `from pruna import PrunaModel\nmodel = PrunaModel.from_pretrained(\"${model.id}\")\n`,\n];\nexport const nemo = (model) => {\n    let command = undefined;\n    // Resolve the tag to a nemo domain/sub-domain\n    if (model.tags.includes(\"automatic-speech-recognition\")) {\n        command = nemoDomainResolver(\"ASR\", model);\n    }\n    return command ?? [`# tag did not correspond to a valid NeMo domain.`];\n};\nexport const outetts = (model) => {\n    // Don’t show this block on GGUF / ONNX mirrors\n    const t = model.tags ?? [];\n    if (t.includes(\"gguf\") || t.includes(\"onnx\"))\n        return [];\n    // v1.0 HF → minimal runnable snippet\n    return [\n        `\n  import outetts\n  \n  enum = outetts.Models(\"${model.id}\".split(\"/\", 1)[1])       # VERSION_1_0_SIZE_1B\n  cfg  = outetts.ModelConfig.auto_config(enum, outetts.Backend.HF)\n  tts  = outetts.Interface(cfg)\n  \n  speaker = tts.load_default_speaker(\"EN-FEMALE-1-NEUTRAL\")\n  tts.generate(\n\t  outetts.GenerationConfig(\n\t\t  text=\"Hello there, how are you doing?\",\n\t\t  speaker=speaker,\n\t  )\n  ).save(\"output.wav\")\n  `,\n    ];\n};\nexport const pxia = (model) => [\n    `from pxia import AutoModel\n\nmodel = AutoModel.from_pretrained(\"${model.id}\")`,\n];\nexport const pythae = (model) => [\n    `from pythae.models import AutoModel\n\nmodel = AutoModel.load_from_hf_hub(\"${model.id}\")`,\n];\nconst musicgen = (model) => [\n    `from audiocraft.models import MusicGen\n\nmodel = MusicGen.get_pretrained(\"${model.id}\")\n\ndescriptions = ['happy rock', 'energetic EDM', 'sad jazz']\nwav = model.generate(descriptions)  # generates 3 samples.`,\n];\nconst magnet = (model) => [\n    `from audiocraft.models import MAGNeT\n\t\nmodel = MAGNeT.get_pretrained(\"${model.id}\")\n\ndescriptions = ['disco beat', 'energetic EDM', 'funky groove']\nwav = model.generate(descriptions)  # generates 3 samples.`,\n];\nconst audiogen = (model) => [\n    `from audiocraft.models import AudioGen\n\t\nmodel = AudioGen.get_pretrained(\"${model.id}\")\nmodel.set_generation_params(duration=5)  # generate 5 seconds.\ndescriptions = ['dog barking', 'sirene of an emergency vehicle', 'footsteps in a corridor']\nwav = model.generate(descriptions)  # generates 3 samples.`,\n];\nexport const anemoi = (model) => [\n    `from anemoi.inference.runners.default import DefaultRunner\nfrom anemoi.inference.config.run import RunConfiguration\n# Create Configuration\nconfig = RunConfiguration(checkpoint = {\"huggingface\":\"${model.id}\"})\n# Load Runner\nrunner = DefaultRunner(config)`,\n];\nexport const audiocraft = (model) => {\n    if (model.tags.includes(\"musicgen\")) {\n        return musicgen(model);\n    }\n    else if (model.tags.includes(\"audiogen\")) {\n        return audiogen(model);\n    }\n    else if (model.tags.includes(\"magnet\")) {\n        return magnet(model);\n    }\n    else {\n        return [`# Type of model unknown.`];\n    }\n};\nexport const whisperkit = () => [\n    `# Install CLI with Homebrew on macOS device\nbrew install whisperkit-cli\n\n# View all available inference options\nwhisperkit-cli transcribe --help\n\t\n# Download and run inference using whisper base model\nwhisperkit-cli transcribe --audio-path /path/to/audio.mp3\n\n# Or use your preferred model variant\nwhisperkit-cli transcribe --model \"large-v3\" --model-prefix \"distil\" --audio-path /path/to/audio.mp3 --verbose`,\n];\nexport const threedtopia_xl = (model) => [\n    `from threedtopia_xl.models import threedtopia_xl\n\nmodel = threedtopia_xl.from_pretrained(\"${model.id}\")\nmodel.generate(cond=\"path/to/image.png\")`,\n];\nexport const hezar = (model) => [\n    `from hezar import Model\n\nmodel = Model.load(\"${model.id}\")`,\n];\nexport const zonos = (model) => [\n    `# pip install git+https://github.com/Zyphra/Zonos.git\nimport torchaudio\nfrom zonos.model import Zonos\nfrom zonos.conditioning import make_cond_dict\n\nmodel = Zonos.from_pretrained(\"${model.id}\", device=\"cuda\")\n\nwav, sr = torchaudio.load(\"speaker.wav\")           # 5-10s reference clip\nspeaker = model.make_speaker_embedding(wav, sr)\n\ncond  = make_cond_dict(text=\"Hello, world!\", speaker=speaker, language=\"en-us\")\ncodes = model.generate(model.prepare_conditioning(cond))\n\naudio = model.autoencoder.decode(codes)[0].cpu()\ntorchaudio.save(\"sample.wav\", audio, model.autoencoder.sampling_rate)\n`,\n];\n//#endregion\n"],"mappings":"AAAA,SAASA,oBAAoB,QAAQ,uBAAuB;AAC5D,SAASC,oBAAoB,QAAQ,sBAAsB;AAC3D,SAASC,iBAAiB,QAAQ,sBAAsB;AACxD,MAAMC,eAAe,GAAG,aAAa;AACrC,SAASC,oBAAoBA,CAACC,OAAO,EAAE;EACnC,MAAMC,QAAQ,GAAGD,OAAO,CAACE,KAAK,CAAC,GAAG,CAAC;EACnC,OAAOD,QAAQ,CAACE,MAAM,KAAK,CAAC,GAAGF,QAAQ,CAAC,CAAC,CAAC,GAAGA,QAAQ,CAAC,CAAC,CAAC;AAC5D;AACA,MAAMG,mBAAmB,GAAIC,GAAG,IAAKC,IAAI,CAACC,SAAS,CAACF,GAAG,CAAC,CAACG,KAAK,CAAC,CAAC,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC;AACvE;AACA,OAAO,MAAMC,QAAQ,GAAIC,KAAK,IAAK,CAC/B;AACJ;AACA,4CAA4CA,KAAK,CAACC,MAAM,EAAEC,oBAAoB,EAAEC,UAAU;AAC1F,sBAAsBH,KAAK,CAACI,EAAE,qBAAqB,CAClD;AACD,MAAMC,eAAe,GAAIL,KAAK,IAAK,CAC/B;AACJ;AACA;AACA,wCAAwCA,KAAK,CAACI,EAAE,IAAI,CACnD;AACD,MAAME,yBAAyB,GAAIN,KAAK,IAAK,CACzC;AACJ;AACA;AACA,wCAAwCA,KAAK,CAACI,EAAE;AAChD;AACA,sDAAsD,CACrD;AACD,OAAO,MAAMG,QAAQ,GAAIP,KAAK,IAAK;EAC/B,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,oBAAoB,CAAC,EAAE;IAC3C,OAAOH,yBAAyB,CAACN,KAAK,CAAC;EAC3C;EACA,OAAOK,eAAe,CAACL,KAAK,CAAC;AACjC,CAAC;AACD,OAAO,MAAMU,OAAO,GAAIV,KAAK,IAAK,CAC9B;AACJ;AACA,mCAAmCA,KAAK,CAACI,EAAE,IAAI,CAC9C;AACD,OAAO,MAAMO,QAAQ,GAAIX,KAAK,IAAK,CAC/B;AACJ;AACA,qCAAqCA,KAAK,CAACI,EAAE,IAAI,CAChD;AACD,OAAO,MAAMQ,SAAS,GAAIZ,KAAK,IAAK;EAChC,MAAMa,gBAAgB,GAAG;AAC7B;AACA;AACA,oCAAoCb,KAAK,CAACI,EAAE;AAC5C;AACA;AACA;AACA;AACA,oCAAoC;EAChC,MAAMU,eAAe,GAAG;AAC5B;AACA;AACA,sCAAsCd,KAAK,CAACI,EAAE;AAC9C;AACA,mEAAmE;EAC/D,OAAO,CAACS,gBAAgB,EAAEC,eAAe,CAAC;AAC9C,CAAC;AACD,SAASC,wBAAwBA,CAACf,KAAK,EAAE;EACrC,OAAOA,KAAK,CAACgB,QAAQ,EAAEC,UAAU,EAAEC,QAAQ,CAAC,CAAC,IAAI,oBAAoB;AACzE;AACA,SAASC,+BAA+BA,CAACnB,KAAK,EAAE;EAC5C,MAAMoB,MAAM,GAAGpB,KAAK,CAACqB,UAAU,GAAG,CAAC,CAAC,EAAEC,IAAI,IAAItB,KAAK,CAACgB,QAAQ,EAAEO,eAAe;EAC7E,IAAIH,MAAM,EAAE;IACR,OAAO1B,mBAAmB,CAAC0B,MAAM,CAAC;EACtC;AACJ;AACA,OAAO,MAAMI,IAAI,GAAIxB,KAAK,IAAK,CAC3B;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,qCAAqCA,KAAK,CAACI,EAAE;AAC7C;AACA;AACA,CAAC,CACA;AACD,OAAO,MAAMqB,QAAQ,GAAIzB,KAAK,IAAK,CAC/B;AACJ;AACA,yBAAyBA,KAAK,CAACI,EAAE,IAAI,CACpC;AACD,OAAO,MAAMsB,KAAK,GAAI1B,KAAK,IAAK,CAC5B;AACJ;AACA,oCAAoCA,KAAK,CAACI,EAAE,IAAI,CAC/C;AACD,OAAO,MAAMuB,UAAU,GAAGA,CAAA,KAAM,CAC5B;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qCAAqC,CACpC;AACD,OAAO,MAAMC,UAAU,GAAGA,CAAA,KAAM;EAC5B,MAAMC,cAAc,GAAG,2DAA2D;EAClF,MAAMC,qBAAqB,GAAG;AAClC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uDAAuD;EACnD,MAAMC,kBAAkB,GAAG;AAC/B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,uBAAuB;EACnB,OAAO,CAACF,cAAc,EAAEC,qBAAqB,EAAEC,kBAAkB,CAAC;AACtE,CAAC;AACD,OAAO,MAAMC,cAAc,GAAGA,CAAA,KAAM,CAChC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0DAA0D,CACzD;AACD,OAAO,MAAMC,iBAAiB,GAAIjC,KAAK,IAAK;EACxC,IAAIkC,OAAO;EACX,IAAIC,QAAQ;EACZ,IAAIC,YAAY;EAChBF,OAAO,GAAG,WAAW;EACrBC,QAAQ,GAAG,sBAAsB;EACjCC,YAAY,GAAG,gBAAgB;EAC/B,IAAIpC,KAAK,CAACI,EAAE,KAAK,wCAAwC,EAAE;IACvD8B,OAAO,GAAG,MAAM;IAChBC,QAAQ,GAAG,IAAI;IACfC,YAAY,GAAG,oBAAoB;EACvC,CAAC,MACI,IAAIpC,KAAK,CAACI,EAAE,KAAK,uCAAuC,EAAE;IAC3D8B,OAAO,GAAG,MAAM;IAChBC,QAAQ,GAAG,KAAK;IAChBC,YAAY,GAAG,qBAAqB;EACxC,CAAC,MACI,IAAIpC,KAAK,CAACI,EAAE,KAAK,wCAAwC,EAAE;IAC5D8B,OAAO,GAAG,MAAM;IAChBC,QAAQ,GAAG,KAAK;IAChBC,YAAY,GAAG,uBAAuB;EAC1C;EACA,OAAO,CACH;AACR;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mCAAmCF,OAAO,eAAeC,QAAQ,kBAAkBC,YAAY;AAC/F;AACA;AACA,sCAAsCpC,KAAK,CAACI,EAAE,kCAAkC8B,OAAO;AACvF;AACA;AACA;AACA;AACA;AACA,KAAK,CACA;AACL,CAAC;AACD,OAAO,MAAMG,SAAS,GAAIrC,KAAK,IAAK;EAChC,MAAM6B,cAAc,GAAG;AAC3B;AACA,mDAAmD7B,KAAK,CAACI,EAAE,EAAE;EACzD,MAAMkC,gBAAgB,GAAG;AAC7B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8CAA8C;EAC1C,OAAO,CAACT,cAAc,EAAES,gBAAgB,CAAC;AAC7C,CAAC;AACD,OAAO,MAAMC,eAAe,GAAGA,CAAA,KAAM,CACjC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iDAAiD,CAChD;AACD,OAAO,MAAMC,GAAG,GAAIxC,KAAK,IAAK,CAC1B;AACJ;AACA;AACA,+BAA+BA,KAAK,CAACI,EAAE;AACvC;AACA;AACA;AACA,sCAAsC,CACrC;AACD,OAAO,MAAMqC,iBAAiB,GAAIzC,KAAK,IAAK,CACxC;AACJ;AACA;AACA;AACA,oBAAoBA,KAAK,CAACI,EAAE;AAC5B;AACA;AACA;AACA;AACA;AACA,EAAE,CACD;AACD,MAAMsC,iBAAiB,GAAG,uCAAuC;AACjE,MAAMC,sBAAsB,GAAG,uEAAuE;AACtG,MAAMC,6BAA6B,GAAG,0BAA0B;AAChE,MAAMC,2BAA2B,GAAG,yDAAyD;AAC7F,MAAMC,iBAAiB,GAAI9C,KAAK,IAAK,CACjC;AACJ;AACA,4CAA4CA,KAAK,CAACI,EAAE;AACpD;AACA,YAAYe,+BAA+B,CAACnB,KAAK,CAAC,IAAI2C,sBAAsB;AAC5E,+BAA+B,CAC9B;AACD,MAAMI,wBAAwB,GAAI/C,KAAK,IAAK,CACxC;AACJ;AACA;AACA,4CAA4CA,KAAK,CAACI,EAAE;AACpD;AACA,YAAYe,+BAA+B,CAACnB,KAAK,CAAC,IAAI4C,6BAA6B;AACnF;AACA;AACA,yDAAyD,CACxD;AACD,MAAMI,wBAAwB,GAAIhD,KAAK,IAAK,CACxC;AACJ;AACA;AACA;AACA,4CAA4CA,KAAK,CAACI,EAAE;AACpD;AACA;AACA,YAAYe,+BAA+B,CAACnB,KAAK,CAAC,IAAI6C,2BAA2B;AACjF;AACA;AACA;AACA;AACA;AACA,sCAAsC,CACrC;AACD,MAAMI,oBAAoB,GAAIjD,KAAK,IAAK,CACpC;AACJ;AACA,gDAAgDA,KAAK,CAACI,EAAE;AACxD;AACA,IAAIW,wBAAwB,CAACf,KAAK,CAAC;AACnC,EAAE,CACD;AACD,MAAMkD,cAAc,GAAIlD,KAAK,IAAK,CAC9B;AACJ;AACA,4CAA4Ce,wBAAwB,CAACf,KAAK,CAAC;AAC3E,0BAA0BA,KAAK,CAACI,EAAE;AAClC;AACA,YAAYe,+BAA+B,CAACnB,KAAK,CAAC,IAAI2C,sBAAsB;AAC5E,+BAA+B,CAC9B;AACD,MAAMQ,6BAA6B,GAAInD,KAAK,IAAK,CAC7C;AACJ;AACA;AACA,4CAA4Ce,wBAAwB,CAACf,KAAK,CAAC;AAC3E,0BAA0BA,KAAK,CAACI,EAAE;AAClC;AACA,YAAYe,+BAA+B,CAACnB,KAAK,CAAC,IAAI4C,6BAA6B;AACnF;AACA;AACA,yDAAyD,CACxD;AACD,MAAMQ,4BAA4B,GAAIpD,KAAK,IAAK,CAC5C;AACJ;AACA;AACA,4CAA4Ce,wBAAwB,CAACf,KAAK,CAAC;AAC3E,0BAA0BA,KAAK,CAACI,EAAE;AAClC;AACA,YAAYe,+BAA+B,CAACnB,KAAK,CAAC,IAAI6C,2BAA2B;AACjF;AACA;AACA,sCAAsC,CACrC;AACD,MAAMQ,6BAA6B,GAAIrD,KAAK,IAAK,CAC7C;AACJ;AACA;AACA,4CAA4Ce,wBAAwB,CAACf,KAAK,CAAC;AAC3E,0BAA0BA,KAAK,CAACI,EAAE;AAClC;AACA,YAAYe,+BAA+B,CAACnB,KAAK,CAAC,IAAI6C,2BAA2B;AACjF;AACA;AACA;AACA,sCAAsC,CACrC;AACD,MAAMS,2BAA2B,GAAItD,KAAK,IAAK,CAC3C;AACJ;AACA,4CAA4Ce,wBAAwB,CAACf,KAAK,CAAC;AAC3E,+BAA+BA,KAAK,CAACI,EAAE,IAAI,CAC1C;AACD,MAAMmD,mBAAmB,GAAIvD,KAAK,IAAK,CACnC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,2CAA2CA,KAAK,CAACI,EAAE;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iCAAiC,CAChC;AACD,MAAMoD,oBAAoB,GAAIxD,KAAK,IAAK,CACpC;AACJ;AACA;AACA;AACA,oDAAoDA,KAAK,CAACI,EAAE;AAC5D;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY,CACX;AACD,OAAO,MAAMqD,SAAS,GAAIzD,KAAK,IAAK;EAChC,IAAI0D,YAAY;EAChB,IAAI1D,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gCAAgC,CAAC,IACrDT,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,kCAAkC,CAAC,EAAE;IACzDiD,YAAY,GAAGF,oBAAoB,CAACxD,KAAK,CAAC;EAC9C,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,YAAY,CAAC,EAAE;IACxCiD,YAAY,GAAGT,oBAAoB,CAACjD,KAAK,CAAC;EAC9C,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,MAAM,CAAC,EAAE;IAClC,IAAIT,KAAK,CAAC2D,YAAY,KAAK,gBAAgB,EAAE;MACzCD,YAAY,GAAGP,6BAA6B,CAACnD,KAAK,CAAC;IACvD,CAAC,MACI,IAAIA,KAAK,CAAC2D,YAAY,KAAK,gBAAgB,EAAE;MAC9CD,YAAY,GAAGL,6BAA6B,CAACrD,KAAK,CAAC;IACvD,CAAC,MACI,IAAIA,KAAK,CAAC2D,YAAY,KAAK,eAAe,EAAE;MAC7CD,YAAY,GAAGN,4BAA4B,CAACpD,KAAK,CAAC;IACtD,CAAC,MACI;MACD0D,YAAY,GAAGR,cAAc,CAAClD,KAAK,CAAC;IACxC;EACJ,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,mBAAmB,CAAC,EAAE;IAC/CiD,YAAY,GAAGJ,2BAA2B,CAACtD,KAAK,CAAC;EACrD,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,kBAAkB,CAAC,EAAE;IAC9CiD,YAAY,GAAGH,mBAAmB,CAACvD,KAAK,CAAC;EAC7C,CAAC,MACI,IAAIA,KAAK,CAAC2D,YAAY,KAAK,gBAAgB,EAAE;IAC9CD,YAAY,GAAGV,wBAAwB,CAAChD,KAAK,CAAC;EAClD,CAAC,MACI,IAAIA,KAAK,CAAC2D,YAAY,KAAK,gBAAgB,EAAE;IAC9CD,YAAY,GAAGX,wBAAwB,CAAC/C,KAAK,CAAC;EAClD,CAAC,MACI;IACD0D,YAAY,GAAGZ,iBAAiB,CAAC9C,KAAK,CAAC;EAC3C;EACA,OAAO,CAAC0C,iBAAiB,EAAE,GAAGgB,YAAY,CAAC;AAC/C,CAAC;AACD,OAAO,MAAME,YAAY,GAAI5D,KAAK,IAAK;EACnC,MAAM6D,UAAU,GAAG;AACvB;AACA;AACA;AACA;AACA;AACA,iBAAiB7D,KAAK,CAACI,EAAE;AACzB;AACA;AACA;AACA,EAAE;EACE,MAAM0D,WAAW,GAAG;AACxB;AACA;AACA;AACA;AACA,kBAAkB9D,KAAK,CAACI,EAAE;AAC1B;AACA;AACA;AACA,EAAE;EACE,MAAM2D,eAAe,GAAG;AAC5B;AACA;AACA,cAAc/D,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,MAAM,CAAC,GAAG,CAAC,GAAG,EAAE;AAClD,eAAeT,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,MAAM,CAAC,GAAG,CAAC,GAAG,CAAC;AAClD;AACA;AACA;AACA;AACA;AACA;AACA,EAAE;EACE,MAAMuD,eAAe,GAAGhE,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,MAAM,CAAC,GAAGqD,WAAW,GAAGD,UAAU;EAC9E,OAAO,CAACG,eAAe,EAAED,eAAe,CAAC;AAC7C,CAAC;AACD,OAAO,MAAME,gBAAgB,GAAIjE,KAAK,IAAK,CACvC;AACJ;AACA;AACA;AACA,2CAA2CA,KAAK,CAACI,EAAE;AACnD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,EAAE,CACD;AACD,OAAO,MAAM8D,YAAY,GAAIlE,KAAK,IAAK,CACnC;AACJ;AACA;AACA,+BAA+BA,KAAK,CAACI,EAAE;AACvC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,CACA;AACD,OAAO,MAAM+D,MAAM,GAAInE,KAAK,IAAK;EAC7B,MAAMoE,WAAW,GAAG/E,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC,CAACiE,UAAU,CAAC,GAAG,EAAE,GAAG,CAAC;EACvE,OAAO,CACH;AACR;AACA,qBAAqBrE,KAAK,CAACI,EAAE;AAC7B,CAAC,EACO;AACR,0CAA0CJ,KAAK,CAACI,EAAE;AAClD;AACA;AACA,SAASgE,WAAW;AACpB;AACA,QAAQA,WAAW,8BAA8BA,WAAW;AAC5D,CAAC,CACI;AACL,CAAC;AACD,OAAO,MAAME,SAAS,GAAItE,KAAK,IAAK,CAChC;AACJ;AACA,uCAAuCA,KAAK,CAACI,EAAE;AAC/C;AACA,mDAAmD,CAClD;AACD,OAAO,MAAMmE,SAAS,GAAIvE,KAAK,IAAK,CAChC;AACJ;AACA;AACA,KAAKA,KAAK,CAACI,EAAE;AACb;AACA;AACA;AACA,4BAA4B,CAC3B;AACD,MAAMoE,aAAa,GAAGA,CAAA,KAAM,CAAC,6EAA6E,CAAC;AAC3G,OAAO,MAAMC,MAAM,GAAIzE,KAAK,IAAK;EAC7B,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,EAAE;IACvC,OAAO6D,SAAS,CAACtE,KAAK,CAAC;EAC3B,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,8BAA8B,CAAC,EAAE;IAC1D,OAAO8D,SAAS,CAACvE,KAAK,CAAC;EAC3B;EACA,OAAOwE,aAAa,CAAC,CAAC;AAC1B,CAAC;AACD,OAAO,MAAME,OAAO,GAAI1E,KAAK,IAAK,CAC9B;AACJ;AACA;AACA,OAAOA,KAAK,CAACI,EAAE;AACf,EAAE,CACD;AACD,OAAO,MAAMuE,KAAK,GAAI3E,KAAK,IAAK,CAC5B;AACJ;AACA,gCAAgCA,KAAK,CAACI,EAAE,IAAI,CAC3C;AACD,OAAO,MAAMwE,MAAM,GAAI5E,KAAK,IAAK,CAC7B;AACJ;AACA,kCAAkCA,KAAK,CAACI,EAAE,IAAI,CAC7C;AACD,OAAO,MAAMyE,QAAQ,GAAI7E,KAAK,IAAK,CAC/B;AACJ;AACA;AACA,oBAAoBA,KAAK,CAACI,EAAE;AAC5B;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oCAAoC,CACnC;AACD,OAAO,MAAM0E,OAAO,GAAI9E,KAAK,IAAK,CAC9B;AACJ;AACA,yDAAyD,EACrD;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,qCAAqCA,KAAK,CAACI,EAAE;AAC7C;AACA,OAAO,CACN;AACD,OAAO,MAAM2E,KAAK,GAAI/E,KAAK,IAAK,CAC5B;AACJ;AACA;AACA;AACA;AACA;AACA,wCAAwCA,KAAK,CAACI,EAAE;AAChD,CAAC,CACA;AACD,MAAM4E,oBAAoB,GAAI1F,OAAO,IAAK;AAC1C;AACA;AACA;AACA,0DAA0DA,OAAO;AACjE;AACA;AACA;AACA;AACA,CAAC;AACD,MAAM2F,wBAAwB,GAAI3F,OAAO,IAAK;AAC9C;AACA;AACA;AACA,iEAAiEA,OAAO;AACxE;AACA;AACA;AACA,CAAC;AACD,MAAM4F,0BAA0B,GAAI5F,OAAO,IAAK;AAChD;AACA;AACA;AACA;AACA,YAAYA,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD,MAAM6F,2BAA2B,GAAI7F,OAAO,IAAK;AACjD;AACA;AACA;AACA;AACA;AACA,YAAYA,OAAO;AACnB;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC;AACD,MAAM8F,6BAA6B,GAAG;EAClCC,QAAQ,EAAEL,oBAAoB;EAC9BM,WAAW,EAAEL,wBAAwB;EACrCM,cAAc,EAAEL,0BAA0B;EAC1CM,eAAe,EAAEL;AACrB,CAAC;AACD,MAAMM,+BAA+B,GAAGA,CAACC,IAAI,EAAEpG,OAAO,KAAK;AAC3D;AACA;AACA,aAAaoG,IAAI;AACjB,0BAA0BA,IAAI,sBAAsBpG,OAAO;AAC3D,CAAC;AACD,MAAMqG,2BAA2B,GAAIrG,OAAO,IAAK;AACjD;AACA;AACA;AACA,yDAAyDA,OAAO;AAChE,CAAC;AACD,OAAO,MAAMsG,SAAS,GAAI5F,KAAK,IAAK;EAChC,MAAMV,OAAO,GAAGU,KAAK,CAACI,EAAE;EACxB,MAAMyF,KAAK,GAAG7F,KAAK,CAACC,MAAM,EAAE2F,SAAS,EAAEC,KAAK,IAAI,EAAE;EAClD,MAAMC,QAAQ,GAAG,EAAE;EACnB;EACA,KAAK,MAAM,CAACJ,IAAI,EAAEK,OAAO,CAAC,IAAIC,MAAM,CAACC,OAAO,CAACb,6BAA6B,CAAC,EAAE;IACzE,IAAIS,KAAK,CAACpF,QAAQ,CAACiF,IAAI,CAAC,EAAE;MACtBI,QAAQ,CAACI,IAAI,CAACH,OAAO,CAACzG,OAAO,CAAC,CAAC;IACnC;EACJ;EACA;EACA,KAAK,MAAMoG,IAAI,IAAIG,KAAK,EAAE;IACtB,IAAI,CAACG,MAAM,CAACG,IAAI,CAACf,6BAA6B,CAAC,CAAC3E,QAAQ,CAACiF,IAAI,CAAC,EAAE;MAC5DI,QAAQ,CAACI,IAAI,CAACT,+BAA+B,CAACC,IAAI,EAAEpG,OAAO,CAAC,CAAC;IACjE;EACJ;EACA;EACAwG,QAAQ,CAACI,IAAI,CAACP,2BAA2B,CAACrG,OAAO,CAAC,CAAC;EACnD,OAAOwG,QAAQ;AACnB,CAAC;AACD,OAAO,MAAMM,UAAU,GAAIpG,KAAK,IAAK,CACjC;AACJ;AACA;AACA;AACA;AACA,gCAAgCA,KAAK,CAACI,EAAE;AACxC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,CACA;AACD,OAAO,MAAMiG,SAAS,GAAIrG,KAAK,IAAK,CAChC;AACJ,iBAAiBA,KAAK,CAACI,EAAE;AACzB;AACA;AACA;AACA;AACA;AACA,qCAAqC,CACpC;AACD,OAAO,MAAMkG,YAAY,GAAItG,KAAK,IAAK;EACnC,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,YAAY,CAAC,EAAE;IACnC,OAAO,CACH;AACZ;AACA;AACA,2BAA2BT,KAAK,CAACI,EAAE;AACnC;AACA,+CAA+C,CACtC;EACL,CAAC,MACI,IAAIJ,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,eAAe,CAAC,EAAE;IAC3C,OAAO,CACH;AACZ;AACA;AACA,8BAA8BT,KAAK,CAACI,EAAE;AACtC;AACA,+CAA+C,CACtC;EACL;EACA,OAAO,CACH;AACR;AACA;AACA;AACA;AACA,2BAA2BJ,KAAK,CAACI,EAAE;AACnC,gCAAgCJ,KAAK,CAACI,EAAE;AACxC;AACA,+CAA+C,CAC1C;AACL,CAAC;AACD,OAAO,MAAMmG,gBAAgB,GAAIvG,KAAK,IAAK;EACvC,MAAM8F,QAAQ,GAAG,CACb;AACR;AACA;AACA;AACA;AACA,YAAY9F,KAAK,CAACI,EAAE;AACpB;AACA;AACA,CAAC,CACI;EACD,IAAIJ,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,EAAE;IACvC,MAAM+F,QAAQ,GAAGtH,oBAAoB,CAACc,KAAK,CAAC;IAC5C8F,QAAQ,CAACI,IAAI,CAAC;AACtB,cAAc/G,iBAAiB,CAACqH,QAAQ,EAAE;MAAEC,kBAAkB,EAAE,IAAI;MAAEC,MAAM,EAAE;IAAK,CAAC,CAAC;AACrF,EAAE,CAAC;EACC,CAAC,MACI;IACDZ,QAAQ,CAACI,IAAI,CAAC;AACtB;AACA;AACA;AACA;AACA,cAAc,CAAC;EACX;EACA,OAAOJ,QAAQ;AACnB,CAAC;AACD,OAAO,MAAMa,OAAO,GAAI3G,KAAK,IAAK;EAC9B,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,SAAS,CAAC,EAAE;IAChC,MAAMmG,eAAe,GAAG;IACpB;IACA;AACZ;AACA;AACA,0BAA0B;IACd;IACA;AACZ;AACA,gBAAgB5G,KAAK,CAACI,EAAE;AACxB;AACA;AACA;AACA;AACA;AACA;AACA,oBAAoB,CACX;IACD,IAAIJ,KAAK,CAACI,EAAE,KAAK,sBAAsB,EAAE;MACrC;MACAwG,eAAe,CAACV,IAAI,CAAC;AACjC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kBAAkBlG,KAAK,CAACI,EAAE,EAAE,CAAC;IACrB;IACA,OAAOwG,eAAe;EAC1B;EACA,OAAO,EAAE;AACb,CAAC;AACD,OAAO,MAAMC,QAAQ,GAAI7G,KAAK,IAAK,CAC/B;AACJ;AACA;AACA;AACA,iCAAiCA,KAAK,CAACI,EAAE;AACzC,CAAC,CACA;AACD,OAAO,MAAM0G,SAAS,GAAI9G,KAAK,IAAK,CAChC;AACJ;AACA,4CAA4CA,KAAK,CAACI,EAAE,IAAI,CACvD;AACD,OAAO,MAAM2G,SAAS,GAAI/G,KAAK,IAAK,CAChC;AACJ;AACA;AACA,oCAAoCA,KAAK,CAACI,EAAE,IAAI,CAC/C;AACD,OAAO,MAAM4G,SAAS,GAAIhH,KAAK,IAAK,CAChC;AACJ;AACA;AACA,qCAAqCA,KAAK,CAACI,EAAE,IAAI,EAC7C;AACJ;AACA,6BAA6BJ,KAAK,CAACI,EAAE,IAAI,CACxC;AACD,OAAO,MAAM6G,aAAa,GAAGA,CAAA,KAAM,CAC/B;AACJ;AACA;AACA;AACA;AACA;AACA,2BAA2B,CAC1B;AACD,OAAO,MAAMC,SAAS,GAAIlH,KAAK,IAAK,CAChC;AACJ;AACA,0FAA0FA,KAAK,CAACI,EAAE;AAClG,8CAA8CJ,KAAK,CAACI,EAAE,IAAI,CACzD;AACD,OAAO,MAAM+G,SAAS,GAAInH,KAAK,IAAK;EAChC,IAAIA,KAAK,CAACC,MAAM,EAAEmH,aAAa,GAAG,CAAC,CAAC,EAAE;IAClC,MAAMC,YAAY,GAAGrH,KAAK,CAACC,MAAM,CAACmH,aAAa,CAAC,CAAC,CAAC;IAClD,OAAO,CACH,CACI,qDAAqDC,YAAY,EAAE,EACnE,EAAE,EACF,8CAA8CrH,KAAK,CAACI,EAAE,sBAAsB,EAC5E,WAAWiH,YAAY,qBAAqBrH,KAAK,CAACI,EAAE,sBAAsB,CAC7E,CAACkH,IAAI,CAAC,IAAI,CAAC,CACf;EACL,CAAC,MACI;IACD,OAAO,CACH,CACI,4BAA4B,EAC5B,6DAA6D,EAC7D,EAAE,EACF,8CAA8CtH,KAAK,CAACI,EAAE,sBAAsB,EAC5E,sCAAsCJ,KAAK,CAACI,EAAE,sBAAsB,CACvE,CAACkH,IAAI,CAAC,IAAI,CAAC,CACf;EACL;AACJ,CAAC;AACD,OAAO,MAAMC,SAAS,GAAIvH,KAAK,IAAK;EAChC,MAAMwH,OAAO,GAAG;IACZC,kBAAkB,EAAE;MAAEC,SAAS,EAAE;IAAgB,CAAC;IAClDC,oBAAoB,EAAE;MAAED,SAAS,EAAE;IAAkB,CAAC;IACtDE,mBAAmB,EAAE;MAAEF,SAAS,EAAE;IAAoB,CAAC;IACvDG,iBAAiB,EAAE;MAAEH,SAAS,EAAE;IAAqB,CAAC;IACtDI,kCAAkC,EAAE;MAAEJ,SAAS,EAAE;IAAkC,CAAC;IACpFK,mCAAmC,EAAE;MAAEL,SAAS,EAAE;IAAoC,CAAC;IACvFM,aAAa,EAAE;MAAEN,SAAS,EAAE;IAAe,CAAC;IAC5CO,mBAAmB,EAAE;MAAEP,SAAS,EAAE;IAAqB,CAAC;IACxDQ,gBAAgB,EAAE;MAAER,SAAS,EAAE;IAAkB,CAAC;IAClDS,qBAAqB,EAAE;MAAET,SAAS,EAAE;IAAsB,CAAC;IAC3DU,0BAA0B,EAAE;MAAEV,SAAS,EAAE;IAAsB,CAAC;IAChEW,2BAA2B,EAAE;MAAEX,SAAS,EAAE;IAA4B;EAC1E,CAAC;EACD,IAAI1H,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,SAAS,CAAC,EAAE;IAChC,OAAO,CACH;AACZ;AACA;AACA;AACA,6BAA6BpB,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC;AAC3D;AACA;AACA;AACA;AACA;AACA;AACA,oDAAoD,CAC3C;EACL;EACA,IAAIJ,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,EAAE;IACvC,OAAO,CACH;AACZ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,0CAA0C,CACjC;EACL;EACA,KAAK,MAAM6H,GAAG,IAAItI,KAAK,CAACQ,IAAI,EAAE;IAC1B,IAAI8H,GAAG,IAAId,OAAO,EAAE;MAChB,MAAM;QAAEE;MAAU,CAAC,GAAGF,OAAO,CAACc,GAAG,CAAC;MAClC,OAAO,CACH;AAChB;AACA;AACA,wBAAwBZ,SAAS;AACjC,UAAUA,SAAS,gBAAgBrI,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC;AACjE;AACA;AACA;AACA;AACA,oDAAoD,CACvC;IACL;EACJ;EACA,OAAO,CACH;AACR,kGAAkG,CAC7F;AACL,CAAC;AACD,OAAO,MAAMmI,kBAAkB,GAAIvI,KAAK,IAAK;EACzC,MAAMwI,UAAU,GAAG;AACvB;AACA;AACA,+BAA+BxI,KAAK,CAACI,EAAE,qBAAqB;EACxD,MAAMqI,cAAc,GAAG;AAC3B;AACA;AACA,4CAA4CzI,KAAK,CAACI,EAAE,qBAAqB;EACrE,IAAIJ,KAAK,CAACI,EAAE,CAACK,QAAQ,CAAC,MAAM,CAAC,EAAE;IAC3B,OAAO,CAAC+H,UAAU,EAAEC,cAAc,CAAC;EACvC,CAAC,MACI;IACD,OAAO,CAACA,cAAc,CAAC;EAC3B;AACJ,CAAC;AACD,OAAO,MAAMC,WAAW,GAAI1I,KAAK,IAAK,CAClC;AACJ;AACA;AACA,sCAAsCA,KAAK,CAACI,EAAE;AAC9C;AACA;AACA;AACA;AACA,qDAAqD,CACpD;AACD,OAAO,MAAMuI,uBAAuB,GAAI3I,KAAK,IAAK,CAC9C;AACJ;AACA,uCAAuCA,KAAK,CAACI,EAAE;AAC/C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6DAA6D,CAC5D;AACD,MAAMwI,oBAAoB,GAAI5I,KAAK,IAAK,CACpC;AACJ;AACA,iCAAiCA,KAAK,CAACI,EAAE;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,oCAAoC,CACnC;AACD,OAAO,MAAMyI,cAAc,GAAI7I,KAAK,IAAK;EACrC,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,yBAAyB,CAAC,EAAE;IAChD,OAAOkI,uBAAuB,CAAC3I,KAAK,CAAC;EACzC;EACA,OAAO4I,oBAAoB,CAAC5I,KAAK,CAAC;AACtC,CAAC;AACD,OAAO,MAAM8I,KAAK,GAAI9I,KAAK,IAAK,CAC5B;AACJ;AACA,iCAAiCA,KAAK,CAACI,EAAE,IAAI,CAC5C;AACD,OAAO,MAAM2I,YAAY,GAAI/I,KAAK,IAAK,CACnC;AACJ;AACA;AACA,4DAA4DA,KAAK,CAACI,EAAE,IAAI,CACvE;AACD,MAAM4I,sBAAsB,GAAIhJ,KAAK,IAAK,CACtC;AACJ;AACA,6CAA6CA,KAAK,CAACI,EAAE;AACrD,uCAAuCJ,KAAK,CAACI,EAAE;AAC/C,CAAC,CACA;AACD,MAAM6I,qBAAqB,GAAIjJ,KAAK,IAAK,CACrC;AACJ;AACA,uCAAuCA,KAAK,CAACI,EAAE;AAC/C;AACA,CAAC,CACA;AACD,MAAM8I,oBAAoB,GAAIlJ,KAAK,IAAK,CACpC;AACJ;AACA,uCAAuCA,KAAK,CAACI,EAAE;AAC/C,CAAC,CACA;AACD,OAAO,MAAM+I,aAAa,GAAInJ,KAAK,IAAK;EACpC,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,aAAa,CAAC,EAAE;IACpC,OAAOuI,sBAAsB,CAAChJ,KAAK,CAAC;EACxC,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,YAAY,CAAC,EAAE;IACxC,OAAOwI,qBAAqB,CAACjJ,KAAK,CAAC;EACvC;EACA,OAAOkJ,oBAAoB,CAAClJ,KAAK,CAAC;AACtC,CAAC;AACD,OAAO,MAAMoJ,IAAI,GAAIpJ,KAAK,IAAK,CAC3B;AACJ;AACA,oCAAoCA,KAAK,CAACI,EAAE,qBAAqB,CAChE;AACD,OAAO,MAAMiJ,OAAO,GAAGA,CAAE;AAAA,KAA2B,CAChD;AACJ;AACA;AACA;AACA;AACA;AACA,EAAE,CACD;AACD,OAAO,MAAMC,UAAU,GAAGA,CAAA,KAAM,CAC5B;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+DAA+D,CAC9D;AACD,MAAMC,WAAW,GAAGA,CAACvJ,KAAK,EAAEwJ,SAAS,KAAK;EACtC,OAAO,CACH;AACR;AACA,YAAYxJ,KAAK,CAACI,EAAE;AACpB;AACA,IAAIoJ,SAAS;AACb;AACA;AACA,kFAAkF,CAC7E;AACL,CAAC;AACD,MAAMC,WAAW,GAAGA,CAACzJ,KAAK,EAAEwJ,SAAS,KAAK;EACtC,OAAO,CACH;AACR;AACA,YAAYxJ,KAAK,CAACI,EAAE;AACpB;AACA;AACA,+BAA+BoJ,SAAS,IAAI,CACvC;AACL,CAAC;AACD,MAAME,WAAW,GAAI1J,KAAK,IAAK;EAC3B,OAAO,CACH;AACR;AACA;AACA,oBAAoBA,KAAK,CAACI,EAAE;AAC5B;AACA;AACA,kFAAkF,CAC7E;AACL,CAAC;AACD,OAAO,MAAMuJ,OAAO,GAAI3J,KAAK,IAAK;EAC9B,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,OAAO,CAAC,EAAE;IAC9B,MAAMmJ,cAAc,GAAG5J,KAAK,CAACC,MAAM,EAAE0J,OAAO,EAAE3J,KAAK,EAAE6J,IAAI;IACzD,MAAMC,eAAe,GAAG9J,KAAK,CAACC,MAAM,EAAE0J,OAAO,EAAEI,YAAY;IAC3D,IAAI,CAACH,cAAc,EAAE;MACjB,OAAO,CAAC,kDAAkD,CAAC;IAC/D;IACA,IAAIE,eAAe,KAAK,QAAQ,EAAE;MAC9B,OAAOP,WAAW,CAACvJ,KAAK,EAAE4J,cAAc,CAAC;IAC7C,CAAC,MACI;MACD,OAAOH,WAAW,CAACzJ,KAAK,EAAE4J,cAAc,CAAC;IAC7C;EACJ,CAAC,MACI;IACD,OAAOF,WAAW,CAAC1J,KAAK,CAAC;EAC7B;AACJ,CAAC;AACD,OAAO,MAAMgK,kBAAkB,GAAIhK,KAAK,IAAK,CACzC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,8CAA8CA,KAAK,CAACI,EAAE;AACtD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,mDAAmD,CAClD;AACD,OAAO,MAAM6J,MAAM,GAAIjK,KAAK,IAAK,CAC7B;AACJ;AACA,kCAAkCA,KAAK,CAACI,EAAE,IAAI,CAC7C;AACD,OAAO,MAAM8J,IAAI,GAAIlK,KAAK,IAAK;EAC3B,MAAMmK,eAAe,GAAG;AAC5B;AACA;AACA;AACA,iDAAiDnK,KAAK,CAACI,EAAE;AACzD;AACA;AACA;AACA,qDAAqD;EACjD,MAAMgK,eAAe,GAAG;AAC5B;AACA;AACA;AACA,iDAAiDpK,KAAK,CAACI,EAAE;AACzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,YAAY;EACR,OAAO,CAAC+J,eAAe,EAAEC,eAAe,CAAC;AAC7C,CAAC;AACD,OAAO,MAAMC,aAAa,GAAIrK,KAAK,IAAK,CACpC,yDAAyDA,KAAK,CAACI,EAAE,iBAAiB,CACrF;AACD,SAASkK,iCAAiCA,CAACtK,KAAK,EAAE;EAC9C,MAAMuK,aAAa,GAAGvK,KAAK,CAACqB,UAAU,GAAG,CAAC,CAAC;EAC3C,IAAIkJ,aAAa,EAAEC,eAAe,IAAID,aAAa,EAAEE,SAAS,EAAEhL,MAAM,EAAE;IACpE,OAAO,CAAC8K,aAAa,CAACC,eAAe,EAAE,GAAGD,aAAa,CAACE,SAAS,CAAC;EACtE;AACJ;AACA,OAAO,MAAMC,oBAAoB,GAAI1K,KAAK,IAAK;EAC3C,MAAM2K,mBAAmB,GAAG3K,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAACrB,eAAe,CAAC,GAAG,0BAA0B,GAAG,EAAE;EAClG,IAAIY,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,QAAQ,CAAC,EAAE;IAC/B,OAAO,CACH;AACZ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6CAA6CT,KAAK,CAACI,EAAE;AACrD;AACA;AACA,mDAAmD,CAC1C;EACL;EACA,IAAIJ,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,eAAe,CAAC,IAAIT,KAAK,CAAC2D,YAAY,IAAI,cAAc,EAAE;IAC9E,OAAO,CACH;AACZ;AACA,wBAAwB3D,KAAK,CAACI,EAAE,IAAIuK,mBAAmB;AACvD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc,CACL;EACL;EACA,MAAMC,gBAAgB,GAAGN,iCAAiC,CAACtK,KAAK,CAAC,IAAI,CACjE,8BAA8B,EAC9B,wBAAwB,EACxB,0BAA0B,CAC7B;EACD,OAAO,CACH;AACR;AACA,+BAA+BA,KAAK,CAACI,EAAE,IAAIuK,mBAAmB;AAC9D;AACA,cAAc/K,IAAI,CAACC,SAAS,CAAC+K,gBAAgB,EAAE,IAAI,EAAE,CAAC,CAAC;AACvD;AACA;AACA;AACA;AACA,KAAKA,gBAAgB,CAACnL,MAAM,KAAKmL,gBAAgB,CAACnL,MAAM,GAAG,CACtD;AACL,CAAC;AACD,OAAO,MAAMoL,MAAM,GAAI7K,KAAK,IAAK,CAC7B;AACJ;AACA,uCAAuCA,KAAK,CAACI,EAAE,IAAI,CAClD;AACD,OAAO,MAAM0K,KAAK,GAAI9K,KAAK,IAAK,CAC5B,uCAAuCA,KAAK,CAACI,EAAE,iBAAiBf,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC;AAClG;AACA;AACA;AACA,oBAAoBf,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC;AAClD;AACA;AACA,SAASf,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC;AACvC,QAAQf,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC,SAAS,CAC9C;AACD,OAAO,MAAM2K,WAAW,GAAI/K,KAAK,IAAK,CAClC;AACJ;AACA,2CAA2CA,KAAK,CAACI,EAAE,IAAI,CACtD;AACD,OAAO,MAAM4K,MAAM,GAAIhL,KAAK,IAAK,CAC7B;AACJ;AACA,mBAAmBX,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC,CAAC6K,OAAO,CAAC,SAAS,EAAE,EAAE,CAAC;AACxE,yBAAyB5L,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC,CAAC6K,OAAO,CAAC,SAAS,EAAE,EAAE,CAAC,IAAI,CACjF;AACD,MAAMC,iBAAiB,GAAIC,oBAAoB,IAAK;EAChD,QAAQA,oBAAoB;IACxB,KAAK,mBAAmB;MACpB,OAAO,eAAe;IAC1B,KAAK,mBAAmB;IACxB,KAAK,YAAY;MACb,OAAO,iBAAiB;IAC5B,KAAK,yBAAyB;MAC1B,OAAO,cAAc;IACzB,KAAK,qBAAqB;MACtB,OAAO,eAAe;IAC1B;MACI,OAAOC,SAAS;EACxB;AACJ,CAAC;AACD,OAAO,MAAMC,WAAW,GAAIrL,KAAK,IAAK;EAClC,MAAMmL,oBAAoB,GAAGnL,KAAK,CAACC,MAAM,EAAEoL,WAAW,EAAEC,qBAAqB;EAC7E,IAAIH,oBAAoB,KAAKC,SAAS,EAAE;IACpC,OAAO,CAAC,0CAA0C,CAAC;EACvD;EACA,MAAMG,iBAAiB,GAAGL,iBAAiB,CAACC,oBAAoB,CAAC;EACjE,IAAII,iBAAiB,KAAKH,SAAS,EAAE;IACjC,OAAO,CAAC,oCAAoC,CAAC;EACjD;EACA,OAAO,CACH,sCAAsCD,oBAAoB;AAClE,UAAUA,oBAAoB;AAC9B,KAAKnL,KAAK,CAACI,EAAE;AACb;AACA,QAAQmL,iBAAiB,cAAc,CAClC;AACL,CAAC;AACD,OAAO,MAAMC,UAAU,GAAIxL,KAAK,IAAK,CACjC;AACJ;AACA,mCAAmCA,KAAK,CAACI,EAAE,IAAI,CAC9C;AACD,MAAMqL,eAAe,GAAIzL,KAAK,IAAKA,KAAK,CAACC,MAAM,EAAEyL,gBAAgB,EAAEC,aAAa,KAAKP,SAAS,IAC1FpL,KAAK,CAACC,MAAM,EAAE2L,gBAAgB,EAAED,aAAa,KAAKP,SAAS,IAC3DpL,KAAK,CAACC,MAAM,EAAE4L,mBAAmB,KAAKT,SAAS;AACnD,OAAO,MAAMU,YAAY,GAAI9L,KAAK,IAAK;EACnC,MAAM+L,IAAI,GAAG/L,KAAK,CAACgM,gBAAgB;EACnC,IAAI,CAACD,IAAI,EAAE;IACP,OAAO,CAAC,4BAA4B,CAAC;EACzC;EACA,MAAMpB,mBAAmB,GAAG3K,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAACrB,eAAe,CAAC,GAAG,0BAA0B,GAAG,EAAE;EAClG,MAAM6M,WAAW,GAAG,EAAE;EACtB,IAAIF,IAAI,CAACG,SAAS,EAAE;IAChB,MAAMC,gBAAgB,GAAGJ,IAAI,CAACG,SAAS,KAAK,eAAe,GACrD,WAAW,GACXH,IAAI,CAACG,SAAS,KAAK,sBAAsB,GACrC,WAAW,GACX,WAAW;IACrBD,WAAW,CAAC/F,IAAI,CAAC,uBAAuB,EAAE,4BAA4B6F,IAAI,CAACG,SAAS,KAAKH,IAAI,CAACK,UAAU,EAAE,EAAE,EAAE,EAAE,GAAGD,gBAAgB,MAAMJ,IAAI,CAACG,SAAS,qBAAqBlM,KAAK,CAACI,EAAE,GAAG,GAAGuK,mBAAmB,GAAG,GAAG,EAAE,WAAWoB,IAAI,CAACK,UAAU,qBAAqBpM,KAAK,CAACI,EAAE,GAAG,GAAGuK,mBAAmB,GAAG,GAAG,CAAC;IAC5S,IAAI3K,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,IAAIgL,eAAe,CAACzL,KAAK,CAAC,EAAE;MACjE,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,oBAAoB,CAAC,EAAE;QAC3CwL,WAAW,CAAC/F,IAAI,CAAC,cAAc,EAAE,CAC7B,OAAO,EACP,yBAAyB,EACzB,sBAAsB,EACtB,yIAAyI,EACzI,sEAAsE,EACtE,WAAW,EACX,QAAQ,CACX,CAACoB,IAAI,CAAC,IAAI,CAAC,EAAE,GAAG,CAAC;MACtB,CAAC,MACI;QACD2E,WAAW,CAAC/F,IAAI,CAAC,cAAc,EAAE,kDAAkD,EAAE,GAAG,CAAC;MAC7F;MACA+F,WAAW,CAAC/F,IAAI,CAAC,YAAYiG,gBAAgB,uBAAuB,EAAE,YAAY,EAAE,8BAA8B,EAAE,iBAAiB,EAAE,oBAAoB,EAAE,uBAAuB,EAAE,oBAAoB,EAAE,EAAE,EAAE,uDAAuD,EAAE,SAASA,gBAAgB,sDAAsD,CAAC;IAC7V;EACJ,CAAC,MACI;IACDF,WAAW,CAAC/F,IAAI,CAAC,uBAAuB,EAAE,4BAA4B6F,IAAI,CAACK,UAAU,EAAE,EAAE,WAAWL,IAAI,CAACK,UAAU,qBAAqBpM,KAAK,CAACI,EAAE,GAAG,GAAGuK,mBAAmB,GAAG,uBAAuB,CAAC;EACxM;EACA,IAAI3K,KAAK,CAAC2D,YAAY,IAAI1E,oBAAoB,CAAC6M,YAAY,EAAErL,QAAQ,CAACT,KAAK,CAAC2D,YAAY,CAAC,EAAE;IACvF,MAAMK,eAAe,GAAG,CACpB,yCAAyC,EACzC,mCAAmC,EACnC,EAAE,EACF,oBAAoBhE,KAAK,CAAC2D,YAAY,aAAa3D,KAAK,CAACI,EAAE,GAAG,GAAGuK,mBAAmB,GAAG,GAAG,CAC7F;IACD,IAAI3K,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,EAAE;MACvC,IAAIT,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,oBAAoB,CAAC,EAAE;QAC3CuD,eAAe,CAACkC,IAAI,CAAC,cAAc,EAAE,CACjC,OAAO,EACP,yBAAyB,EACzB,sBAAsB,EACtB,yIAAyI,EACzI,sEAAsE,EACtE,WAAW,EACX,QAAQ,CACX,CAACoB,IAAI,CAAC,IAAI,CAAC,EAAE,GAAG,CAAC;QAClBtD,eAAe,CAACkC,IAAI,CAAC,qBAAqB,CAAC;MAC/C,CAAC,MACI;QACDlC,eAAe,CAACkC,IAAI,CAAC,cAAc,EAAE,kDAAkD,EAAE,GAAG,CAAC;QAC7FlC,eAAe,CAACkC,IAAI,CAAC,gBAAgB,CAAC;MAC1C;IACJ,CAAC,MACI,IAAIlG,KAAK,CAAC2D,YAAY,KAAK,gCAAgC,EAAE;MAC9DK,eAAe,CAACkC,IAAI,CAAC,OAAO,EAAE,sGAAsG,EAAE,0DAA0D,EAAE,GAAG,CAAC;IAC1M,CAAC,MACI,IAAIlG,KAAK,CAAC2D,YAAY,KAAK,sBAAsB,EAAE;MACpDK,eAAe,CAACkC,IAAI,CAAC,uGAAuG,CAAC;IACjI;IACA,OAAO,CAAClC,eAAe,CAACsD,IAAI,CAAC,IAAI,CAAC,EAAE2E,WAAW,CAAC3E,IAAI,CAAC,IAAI,CAAC,CAAC;EAC/D;EACA,OAAO,CAAC2E,WAAW,CAAC3E,IAAI,CAAC,IAAI,CAAC,CAAC;AACnC,CAAC;AACD,OAAO,MAAM+E,cAAc,GAAIrM,KAAK,IAAK;EACrC,IAAI,CAACA,KAAK,CAAC2D,YAAY,EAAE;IACrB,OAAO,CAAC,4BAA4B,CAAC;EACzC;EACA,MAAM2I,OAAO,GAAG,2BAA2B;EAC3C,OAAO,CACH,YAAYA,OAAO;AAC3B,4BAA4BA,OAAO;AACnC;AACA;AACA,+BAA+BtM,KAAK,CAAC2D,YAAY,OAAO3D,KAAK,CAACI,EAAE,KAAK,CAChE;AACL,CAAC;AACD,MAAMmM,QAAQ,GAAIC,YAAY,IAAK;EAC/B,QAAQA,YAAY;IAChB,KAAK,WAAW;MACZ,OAAO,UAAU;IACrB,KAAK,cAAc;MACf,OAAO,WAAW;IACtB,KAAK,WAAW;MACZ,OAAO,qBAAqB;IAChC,KAAK,SAAS;MACV,OAAO,wBAAwB;IACnC;MACI,OAAOpB,SAAS;EACxB;AACJ,CAAC;AACD,OAAO,MAAMqB,IAAI,GAAIzM,KAAK,IAAK;EAC3B,MAAM;IAAE0M,uBAAuB,EAAEC,aAAa;IAAEC,SAAS,EAAEJ;EAAa,CAAC,GAAGxM,KAAK,CAACC,MAAM,EAAEwM,IAAI,IAAI,CAAC,CAAC;EACpG,MAAMI,QAAQ,GAAGN,QAAQ,CAACC,YAAY,CAAC;EACvC,IAAI,CAACK,QAAQ,EAAE;IACX,OAAO,CAAC,uBAAuB,CAAC;EACpC;EACA,IAAI,CAACF,aAAa,EAAE;IAChB,OAAO,CAAC,0BAA0B,CAAC;EACvC;EACA,OAAO,CACH;AACR,uCAAuCE,QAAQ;AAC/C;AACA,2BAA2BA,QAAQ,qBAAqBF,aAAa;AACrE,iDAAiD3M,KAAK,CAACI,EAAE,IAAI,CACxD;AACL,CAAC;AACD,OAAO,MAAM0M,QAAQ,GAAI9M,KAAK,IAAK,CAC/B;AACJ;AACA;AACA,+CAA+CA,KAAK,CAACI,EAAE,kBAAkB,CACxE;AACD,OAAO,MAAM2M,gBAAgB,GAAI/M,KAAK,IAAK,CACvC;AACJ;AACA,YAAYA,KAAK,CAACI,EAAE;AACpB;AACA,EAAE,CACD;AACD,MAAM4M,kBAAkB,GAAGA,CAACC,MAAM,EAAEjN,KAAK,KAAK;EAC1C,QAAQiN,MAAM;IACV,KAAK,KAAK;MACN,OAAO,CACH;AAChB,wDAAwDjN,KAAK,CAACI,EAAE;AAChE;AACA,oDAAoD,CACvC;IACL;MACI,OAAOgL,SAAS;EACxB;AACJ,CAAC;AACD,OAAO,MAAM8B,QAAQ,GAAIlN,KAAK,IAAK,CAC/B,oCAAoCA,KAAK,CAACI,EAAE,uCAAuC,CACtF;AACD,OAAO,MAAM+M,MAAM,GAAGA,CAAE;AAAA,KAA2B,CAC/C;AACJ;AACA;AACA;AACA,CAAC,CACA;AACD,OAAO,MAAMC,IAAI,GAAIpN,KAAK,IAAK,CAC3B;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,6BAA6BA,KAAK,CAACI,EAAE;AACrC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,CACF;AACD,OAAO,MAAMiN,SAAS,GAAIrN,KAAK,IAAK,CAChC;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,kDAAkDA,KAAK,CAACI,EAAE;AAC1D;AACA,OAAOJ,KAAK,CAACI,EAAE;AACf;AACA;AACA;AACA;AACA;AACA;AACA;AACA,6DAA6D,CAC5D;AACD,OAAO,MAAMkN,UAAU,GAAItN,KAAK,IAAK,CACjC;AACJ;AACA;AACA;AACA,6BAA6BA,KAAK,CAACI,EAAE;AACrC,6CAA6CJ,KAAK,CAACI,EAAE;AACrD;AACA;AACA;AACA,6DAA6D,CAC5D;AACD,OAAO,MAAMmN,QAAQ,GAAIvN,KAAK,IAAK,CAC/B;AACJ;AACA,iCAAiCA,KAAK,CAACI,EAAE,IAAI,CAC5C;AACD,OAAO,MAAMoN,MAAM,GAAIxN,KAAK,IAAK,CAC7B;AACJ;AACA;AACA,gCAAgCA,KAAK,CAACI,EAAE;AACxC;AACA;AACA,kDAAkD,CACjD;AACD,OAAO,MAAMqN,UAAU,GAAIzN,KAAK,IAAK,CACjC;AACJ;AACA,sCAAsCA,KAAK,CAACI,EAAE,IAAI,CACjD;AACD,OAAO,MAAMsN,MAAM,GAAI1N,KAAK,IAAK,CAC7B;AACJ;AACA;AACA,kCAAkCA,KAAK,CAACI,EAAE;AAC1C;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2B,CAC1B;AACD,OAAO,MAAMuN,GAAG,GAAGA,CAAA,KAAM,CACrB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,CACA;AACD,OAAO,MAAMC,OAAO,GAAGA,CAAA,KAAM,CACzB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,iEAAiE,CAChE;AACD,OAAO,MAAMC,WAAW,GAAI7N,KAAK,IAAK;EAClC;EACA,MAAM8N,UAAU,GAAG9N,KAAK,CAACQ,IAAI,CAACuN,IAAI,CAAEzF,GAAG,IAAKA,GAAG,CAAC0F,KAAK,CAAC,YAAY,CAAC,CAAC;EACpE,MAAMtG,SAAS,GAAGoG,UAAU,GAAG,QAAQA,UAAU,CAAChO,KAAK,CAAC,CAAC,CAAC,EAAE,GAAG,SAAS;EACxE,MAAMmO,MAAM,GAAGH,UAAU,GACnB,EAAE,GACF,qFAAqF;EAC3F,OAAO,CACHG,MAAM,GACF,2BAA2BvG,SAAS;AAChD;AACA,UAAUA,SAAS,qBAAqB1H,KAAK,CAACI,EAAE;AAChD;AACA,wCAAwC,CACnC;AACL,CAAC;AACD,OAAO,MAAM8N,QAAQ,GAAIlO,KAAK,IAAK,CAC/B;AACJ;AACA;AACA,4DAA4DA,KAAK,CAACI,EAAE;AACpE,CAAC,EACG;AACJ;AACA;AACA;AACA;AACA,oCAAoCJ,KAAK,CAACI,EAAE,IAAI,CAC/C;AACD,OAAO,MAAM+N,WAAW,GAAInO,KAAK,IAAK,CAClC;AACJ;AACA,4CAA4CA,KAAK,CAACI,EAAE;AACpD,CAAC,CACA;AACD,OAAO,MAAMgO,KAAK,GAAIpO,KAAK,IAAK,CAC5B;AACJ;AACA;AACA;AACA,WAAWA,KAAK,CAACI,EAAE;AACnB;AACA;AACA;AACA,8CAA8CJ,KAAK,CAACI,EAAE;AACtD,CAAC,CACA;AACD,MAAMiO,WAAW,GAAIrO,KAAK,IAAK,CAC3B;AACJ;AACA;AACA,uCAAuCX,oBAAoB,CAACW,KAAK,CAACI,EAAE,CAAC,IAAIJ,KAAK,CAACI,EAAE,EAAE,CAClF;AACD,MAAMkO,KAAK,GAAItO,KAAK,IAAK,CACrB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2BA,KAAK,CAACI,EAAE;AACnC;AACA;AACA,+DAA+D,CAC9D;AACD,MAAMmO,OAAO,GAAIvO,KAAK,IAAK,CACvB;AACJ;AACA;AACA;AACA;AACA;AACA,2BAA2BA,KAAK,CAACI,EAAE;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+DAA+D,CAC9D;AACD,MAAMoO,MAAM,GAAIxO,KAAK,IAAK,CACtB;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA,2BAA2BA,KAAK,CAACI,EAAE;AACnC,wBAAwBJ,KAAK,CAACI,EAAE;AAChC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,cAAc,CACb;AACD,OAAO,MAAMqO,KAAK,GAAIzO,KAAK,IAAK,CAC5B;AACJ;AACA,uBAAuBA,KAAK,CAACI,EAAE,GAAG,CACjC;AACD,OAAO,MAAMsO,GAAG,GAAI1O,KAAK,IAAK;EAC1B,IAAIA,KAAK,CAAC2D,YAAY,KAAK,oBAAoB,EAAE;IAC7C,OAAO6K,MAAM,CAACxO,KAAK,CAAC;EACxB;EACA,IAAIA,KAAK,CAAC2D,YAAY,KAAK,iBAAiB,EAAE;IAC1C,IAAI3D,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,EAAE;MACvC,OAAO8N,OAAO,CAACvO,KAAK,CAAC;IACzB,CAAC,MACI;MACD,OAAOsO,KAAK,CAACtO,KAAK,CAAC;IACvB;EACJ;EACA,OAAOqO,WAAW,CAACrO,KAAK,CAAC;AAC7B,CAAC;AACD,OAAO,MAAM2O,SAAS,GAAI3O,KAAK,IAAK,CAChC;AACJ;AACA,uCAAuCA,KAAK,CAACI,EAAE,IAAI,CAClD;AACD,OAAO,MAAMwO,KAAK,GAAI5O,KAAK,IAAK;EAC5B,IAAI8F,QAAQ;EACZ,IAAI9F,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,WAAW,CAAC,EAAE;IAClCqF,QAAQ,GAAG+I,eAAe,CAAC7O,KAAK,CAAC;EACrC,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,cAAc,CAAC,EAAE;IAC1CqF,QAAQ,GAAGgJ,kBAAkB,CAAC9O,KAAK,CAAC;EACxC,CAAC,MACI;IACD8F,QAAQ,GAAGiJ,aAAa,CAAC/O,KAAK,CAAC;EACnC;EACA,MAAMgP,sBAAsB,GAAIjJ,OAAO,IAAK;IACxC,IAAI,CAAC,gCAAgC,CAACkJ,IAAI,CAAClJ,OAAO,CAAC,EAAE;MACjD,OAAO,iCAAiCA,OAAO,EAAE;IACrD;IACA,OAAOA,OAAO;EAClB,CAAC;EACDD,QAAQ,GAAGA,QAAQ,CAACoJ,GAAG,CAACF,sBAAsB,CAAC;EAC/C,IAAIhP,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,cAAc,CAAC,EAAE;IACrC,OAAOqF,QAAQ,CAACoJ,GAAG,CAAEnJ,OAAO,IAAKA,OAAO,CAACkF,OAAO,CAAC,YAAY,EAAE,WAAW,CAAC,CAACA,OAAO,CAAC,iBAAiB,EAAE,eAAe,CAAC,CAAC;EAC5H;EACA,OAAOnF,QAAQ;AACnB,CAAC;AACD,MAAM+I,eAAe,GAAI7O,KAAK,IAAK;EAC/B,MAAMmP,iBAAiB,GAAG1L,SAAS,CAACzD,KAAK,CAAC;EAC1C,OAAOmP,iBAAiB,CAACD,GAAG,CAAEnJ,OAAO,IAAKA;EACtC;EAAA,CACCkF,OAAO,CAAC,qBAAqB,EAAE,YAAY;EAC5C;EAAA,CACCA,OAAO,CAAC,mDAAmD,EAAE,EAAE,CAAC,CAChEA,OAAO,CAAC,iEAAiE,EAAE,0BAA0B,CAAC,CACtGA,OAAO,CAAC,iCAAiC,EAAE,EAAE;EAC9C;EAAA,CACCA,OAAO,CAAC,mCAAmC,EAAE,8BAA8B,CAAC,CAC5EA,OAAO,CAAC,8CAA8C,EAAE,0BAA0B,CAAC,CACnFA,OAAO,CAAC,8CAA8C,EAAE,0BAA0B;EACnF;EAAA,CACCA,OAAO,CAAC,QAAQ,EAAE,IAAI,CAAC,CACvBmE,IAAI,CAAC,CAAC,CAAC;AAChB,CAAC;AACD,MAAMN,kBAAkB,GAAI9O,KAAK,IAAK;EAClC,MAAM+L,IAAI,GAAG/L,KAAK,CAACgM,gBAAgB;EACnC,MAAMqD,oBAAoB,GAAGvD,YAAY,CAAC9L,KAAK,CAAC;EAChD;EACA,IAAIsP,iBAAiB,GAAGD,oBAAoB,CAACH,GAAG,CAAEnJ,OAAO,IAAKA,OAAO,CAChEkF,OAAO,CAAC,oCAAoC,EAAE,8BAA8B,CAAC,CAC7EA,OAAO,CAAC,oBAAoB,EAAE,+BAA+BjL,KAAK,CAACI,EAAE,IAAI,CAAC,CAAC;EAChF;EACA,IAAI2L,IAAI,EAAEK,UAAU,EAAE;IAClBkD,iBAAiB,GAAGA,iBAAiB,CAACJ,GAAG,CAAEnJ,OAAO,IAAKA,OAAO,CACzDkF,OAAO,CAAC,IAAIsE,MAAM,CAAC,4BAA4BxD,IAAI,CAACK,UAAU,KAAK,EAAE,GAAG,CAAC,EAAE,EAAE,CAAC,CAC9EnB,OAAO,CAAC,IAAIsE,MAAM,CAAC,GAAGxD,IAAI,CAACK,UAAU,kBAAkB,EAAE,GAAG,CAAC,EAAE,4BAA4B,CAAC,CAC5FnB,OAAO,CAAC,IAAIsE,MAAM,CAAC,wBAAwBxD,IAAI,CAACK,UAAU,OAAO,EAAE,IAAI,CAAC,EAAGoD,IAAI,IAAKA,IAAI,CAACvE,OAAO,CAAC,IAAIsE,MAAM,CAAC,MAAMxD,IAAI,CAACK,UAAU,EAAE,EAAE,GAAG,CAAC,EAAE,EAAE,CAAC,CAAC,CAAC;EACzJ;EACA,OAAOkD,iBAAiB;AAC5B,CAAC;AACD,MAAMP,aAAa,GAAI/O,KAAK,IAAK,CAC7B;AACJ,sCAAsCA,KAAK,CAACI,EAAE;AAC9C,CAAC,CACA;AACD,OAAO,MAAMqP,IAAI,GAAIzP,KAAK,IAAK;EAC3B,IAAI0P,OAAO,GAAGtE,SAAS;EACvB;EACA,IAAIpL,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,8BAA8B,CAAC,EAAE;IACrDiP,OAAO,GAAG1C,kBAAkB,CAAC,KAAK,EAAEhN,KAAK,CAAC;EAC9C;EACA,OAAO0P,OAAO,IAAI,CAAC,kDAAkD,CAAC;AAC1E,CAAC;AACD,OAAO,MAAMC,OAAO,GAAI3P,KAAK,IAAK;EAC9B;EACA,MAAM4P,CAAC,GAAG5P,KAAK,CAACQ,IAAI,IAAI,EAAE;EAC1B,IAAIoP,CAAC,CAACnP,QAAQ,CAAC,MAAM,CAAC,IAAImP,CAAC,CAACnP,QAAQ,CAAC,MAAM,CAAC,EACxC,OAAO,EAAE;EACb;EACA,OAAO,CACH;AACR;AACA;AACA,2BAA2BT,KAAK,CAACI,EAAE;AACnC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,GAAG,CACE;AACL,CAAC;AACD,OAAO,MAAMyP,IAAI,GAAI7P,KAAK,IAAK,CAC3B;AACJ;AACA,qCAAqCA,KAAK,CAACI,EAAE,IAAI,CAChD;AACD,OAAO,MAAM0P,MAAM,GAAI9P,KAAK,IAAK,CAC7B;AACJ;AACA,sCAAsCA,KAAK,CAACI,EAAE,IAAI,CACjD;AACD,MAAM2P,QAAQ,GAAI/P,KAAK,IAAK,CACxB;AACJ;AACA,mCAAmCA,KAAK,CAACI,EAAE;AAC3C;AACA;AACA,2DAA2D,CAC1D;AACD,MAAM4P,MAAM,GAAIhQ,KAAK,IAAK,CACtB;AACJ;AACA,iCAAiCA,KAAK,CAACI,EAAE;AACzC;AACA;AACA,2DAA2D,CAC1D;AACD,MAAM6P,QAAQ,GAAIjQ,KAAK,IAAK,CACxB;AACJ;AACA,mCAAmCA,KAAK,CAACI,EAAE;AAC3C;AACA;AACA,2DAA2D,CAC1D;AACD,OAAO,MAAM8P,MAAM,GAAIlQ,KAAK,IAAK,CAC7B;AACJ;AACA;AACA,yDAAyDA,KAAK,CAACI,EAAE;AACjE;AACA,+BAA+B,CAC9B;AACD,OAAO,MAAM+P,UAAU,GAAInQ,KAAK,IAAK;EACjC,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,UAAU,CAAC,EAAE;IACjC,OAAOsP,QAAQ,CAAC/P,KAAK,CAAC;EAC1B,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,UAAU,CAAC,EAAE;IACtC,OAAOwP,QAAQ,CAACjQ,KAAK,CAAC;EAC1B,CAAC,MACI,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,QAAQ,CAAC,EAAE;IACpC,OAAOuP,MAAM,CAAChQ,KAAK,CAAC;EACxB,CAAC,MACI;IACD,OAAO,CAAC,0BAA0B,CAAC;EACvC;AACJ,CAAC;AACD,OAAO,MAAMoQ,UAAU,GAAGA,CAAA,KAAM,CAC5B;AACJ;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,+GAA+G,CAC9G;AACD,OAAO,MAAMC,cAAc,GAAIrQ,KAAK,IAAK,CACrC;AACJ;AACA,0CAA0CA,KAAK,CAACI,EAAE;AAClD,yCAAyC,CACxC;AACD,OAAO,MAAMkQ,KAAK,GAAItQ,KAAK,IAAK,CAC5B;AACJ;AACA,sBAAsBA,KAAK,CAACI,EAAE,IAAI,CACjC;AACD,OAAO,MAAMmQ,KAAK,GAAIvQ,KAAK,IAAK,CAC5B;AACJ;AACA;AACA;AACA;AACA,iCAAiCA,KAAK,CAACI,EAAE;AACzC;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,CAAC,CACA;AACD","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}