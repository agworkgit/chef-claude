{"ast":null,"code":"import { makeRequestOptions } from \"../lib/makeRequestOptions.js\";\nimport { getLines, getMessages } from \"../vendor/fetch-event-source/parse.js\";\nimport { InferenceClientProviderApiError } from \"../errors.js\";\nfunction bodyToJson(body) {\n  let data = null;\n  if (body instanceof Blob || body instanceof ArrayBuffer) {\n    data = \"[Blob or ArrayBuffer]\";\n  } else if (typeof body === \"string\") {\n    try {\n      data = JSON.parse(body);\n    } catch {\n      data = body;\n    }\n  }\n  if (data.accessToken) {\n    data.accessToken = \"[REDACTED]\";\n  }\n  return data;\n}\n/**\n * Primitive to make custom calls to the inference provider\n */\nexport async function innerRequest(args, providerHelper, options) {\n  const {\n    url,\n    info\n  } = await makeRequestOptions(args, providerHelper, options);\n  const response = await (options?.fetch ?? fetch)(url, info);\n  const requestContext = {\n    url,\n    info\n  };\n  if (options?.retry_on_error !== false && response.status === 503) {\n    return innerRequest(args, providerHelper, options);\n  }\n  if (!response.ok) {\n    const contentType = response.headers.get(\"Content-Type\");\n    if ([\"application/json\", \"application/problem+json\"].some(ct => contentType?.startsWith(ct))) {\n      const output = await response.json();\n      if ([400, 422, 404, 500].includes(response.status) && options?.chatCompletion) {\n        throw new InferenceClientProviderApiError(`Provider ${args.provider} does not seem to support chat completion for model ${args.model} . Error: ${JSON.stringify(output.error)}`, {\n          url,\n          method: info.method ?? \"GET\",\n          headers: info.headers,\n          body: bodyToJson(info.body)\n        }, {\n          requestId: response.headers.get(\"x-request-id\") ?? \"\",\n          status: response.status,\n          body: output\n        });\n      }\n      if (typeof output.error === \"string\" || typeof output.detail === \"string\" || typeof output.message === \"string\") {\n        throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.error ?? output.detail ?? output.message}`, {\n          url,\n          method: info.method ?? \"GET\",\n          headers: info.headers,\n          body: bodyToJson(info.body)\n        }, {\n          requestId: response.headers.get(\"x-request-id\") ?? \"\",\n          status: response.status,\n          body: output\n        });\n      } else {\n        throw new InferenceClientProviderApiError(`Failed to perform inference: an HTTP error occurred when requesting the provider.`, {\n          url,\n          method: info.method ?? \"GET\",\n          headers: info.headers,\n          body: bodyToJson(info.body)\n        }, {\n          requestId: response.headers.get(\"x-request-id\") ?? \"\",\n          status: response.status,\n          body: output\n        });\n      }\n    }\n    const message = contentType?.startsWith(\"text/plain;\") ? await response.text() : undefined;\n    throw new InferenceClientProviderApiError(`Failed to perform inference: ${message ?? \"an HTTP error occurred when requesting the provider\"}`, {\n      url,\n      method: info.method ?? \"GET\",\n      headers: info.headers,\n      body: bodyToJson(info.body)\n    }, {\n      requestId: response.headers.get(\"x-request-id\") ?? \"\",\n      status: response.status,\n      body: message ?? \"\"\n    });\n  }\n  if (response.headers.get(\"Content-Type\")?.startsWith(\"application/json\")) {\n    const data = await response.json();\n    return {\n      data,\n      requestContext\n    };\n  }\n  const blob = await response.blob();\n  return {\n    data: blob,\n    requestContext\n  };\n}\n/**\n * Primitive to make custom inference calls that expect server-sent events, and returns the response through a generator\n */\nexport async function* innerStreamingRequest(args, providerHelper, options) {\n  const {\n    url,\n    info\n  } = await makeRequestOptions({\n    ...args,\n    stream: true\n  }, providerHelper, options);\n  const response = await (options?.fetch ?? fetch)(url, info);\n  if (options?.retry_on_error !== false && response.status === 503) {\n    return yield* innerStreamingRequest(args, providerHelper, options);\n  }\n  if (!response.ok) {\n    if (response.headers.get(\"Content-Type\")?.startsWith(\"application/json\")) {\n      const output = await response.json();\n      if ([400, 422, 404, 500].includes(response.status) && options?.chatCompletion) {\n        throw new InferenceClientProviderApiError(`Provider ${args.provider} does not seem to support chat completion for model ${args.model} . Error: ${JSON.stringify(output.error)}`, {\n          url,\n          method: info.method ?? \"GET\",\n          headers: info.headers,\n          body: bodyToJson(info.body)\n        }, {\n          requestId: response.headers.get(\"x-request-id\") ?? \"\",\n          status: response.status,\n          body: output\n        });\n      }\n      if (typeof output.error === \"string\") {\n        throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.error}`, {\n          url,\n          method: info.method ?? \"GET\",\n          headers: info.headers,\n          body: bodyToJson(info.body)\n        }, {\n          requestId: response.headers.get(\"x-request-id\") ?? \"\",\n          status: response.status,\n          body: output\n        });\n      }\n      if (output.error && \"message\" in output.error && typeof output.error.message === \"string\") {\n        /// OpenAI errors\n        throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.error.message}`, {\n          url,\n          method: info.method ?? \"GET\",\n          headers: info.headers,\n          body: bodyToJson(info.body)\n        }, {\n          requestId: response.headers.get(\"x-request-id\") ?? \"\",\n          status: response.status,\n          body: output\n        });\n      }\n      // Sambanova errors\n      if (typeof output.message === \"string\") {\n        throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.message}`, {\n          url,\n          method: info.method ?? \"GET\",\n          headers: info.headers,\n          body: bodyToJson(info.body)\n        }, {\n          requestId: response.headers.get(\"x-request-id\") ?? \"\",\n          status: response.status,\n          body: output\n        });\n      }\n    }\n    throw new InferenceClientProviderApiError(`Failed to perform inference: an HTTP error occurred when requesting the provider.`, {\n      url,\n      method: info.method ?? \"GET\",\n      headers: info.headers,\n      body: bodyToJson(info.body)\n    }, {\n      requestId: response.headers.get(\"x-request-id\") ?? \"\",\n      status: response.status,\n      body: \"\"\n    });\n  }\n  if (!response.headers.get(\"content-type\")?.startsWith(\"text/event-stream\")) {\n    throw new InferenceClientProviderApiError(`Failed to perform inference: server does not support event stream content type, it returned ` + response.headers.get(\"content-type\"), {\n      url,\n      method: info.method ?? \"GET\",\n      headers: info.headers,\n      body: bodyToJson(info.body)\n    }, {\n      requestId: response.headers.get(\"x-request-id\") ?? \"\",\n      status: response.status,\n      body: \"\"\n    });\n  }\n  if (!response.body) {\n    return;\n  }\n  const reader = response.body.getReader();\n  let events = [];\n  const onEvent = event => {\n    // accumulate events in array\n    events.push(event);\n  };\n  const onChunk = getLines(getMessages(() => {}, () => {}, onEvent));\n  try {\n    while (true) {\n      const {\n        done,\n        value\n      } = await reader.read();\n      if (done) {\n        return;\n      }\n      onChunk(value);\n      for (const event of events) {\n        if (event.data.length > 0) {\n          if (event.data === \"[DONE]\") {\n            return;\n          }\n          const data = JSON.parse(event.data);\n          if (typeof data === \"object\" && data !== null && \"error\" in data) {\n            const errorStr = typeof data.error === \"string\" ? data.error : typeof data.error === \"object\" && data.error && \"message\" in data.error && typeof data.error.message === \"string\" ? data.error.message : JSON.stringify(data.error);\n            throw new InferenceClientProviderApiError(`Failed to perform inference: an occurred while streaming the response: ${errorStr}`, {\n              url,\n              method: info.method ?? \"GET\",\n              headers: info.headers,\n              body: bodyToJson(info.body)\n            }, {\n              requestId: response.headers.get(\"x-request-id\") ?? \"\",\n              status: response.status,\n              body: data\n            });\n          }\n          yield data;\n        }\n      }\n      events = [];\n    }\n  } finally {\n    reader.releaseLock();\n  }\n}","map":{"version":3,"names":["makeRequestOptions","getLines","getMessages","InferenceClientProviderApiError","bodyToJson","body","data","Blob","ArrayBuffer","JSON","parse","accessToken","innerRequest","args","providerHelper","options","url","info","response","fetch","requestContext","retry_on_error","status","ok","contentType","headers","get","some","ct","startsWith","output","json","includes","chatCompletion","provider","model","stringify","error","method","requestId","detail","message","text","undefined","blob","innerStreamingRequest","stream","reader","getReader","events","onEvent","event","push","onChunk","done","value","read","length","errorStr","releaseLock"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/inference/dist/esm/utils/request.js"],"sourcesContent":["import { makeRequestOptions } from \"../lib/makeRequestOptions.js\";\nimport { getLines, getMessages } from \"../vendor/fetch-event-source/parse.js\";\nimport { InferenceClientProviderApiError } from \"../errors.js\";\nfunction bodyToJson(body) {\n    let data = null;\n    if (body instanceof Blob || body instanceof ArrayBuffer) {\n        data = \"[Blob or ArrayBuffer]\";\n    }\n    else if (typeof body === \"string\") {\n        try {\n            data = JSON.parse(body);\n        }\n        catch {\n            data = body;\n        }\n    }\n    if (data.accessToken) {\n        data.accessToken = \"[REDACTED]\";\n    }\n    return data;\n}\n/**\n * Primitive to make custom calls to the inference provider\n */\nexport async function innerRequest(args, providerHelper, options) {\n    const { url, info } = await makeRequestOptions(args, providerHelper, options);\n    const response = await (options?.fetch ?? fetch)(url, info);\n    const requestContext = { url, info };\n    if (options?.retry_on_error !== false && response.status === 503) {\n        return innerRequest(args, providerHelper, options);\n    }\n    if (!response.ok) {\n        const contentType = response.headers.get(\"Content-Type\");\n        if ([\"application/json\", \"application/problem+json\"].some((ct) => contentType?.startsWith(ct))) {\n            const output = await response.json();\n            if ([400, 422, 404, 500].includes(response.status) && options?.chatCompletion) {\n                throw new InferenceClientProviderApiError(`Provider ${args.provider} does not seem to support chat completion for model ${args.model} . Error: ${JSON.stringify(output.error)}`, {\n                    url,\n                    method: info.method ?? \"GET\",\n                    headers: info.headers,\n                    body: bodyToJson(info.body),\n                }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: output });\n            }\n            if (typeof output.error === \"string\" || typeof output.detail === \"string\" || typeof output.message === \"string\") {\n                throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.error ?? output.detail ?? output.message}`, {\n                    url,\n                    method: info.method ?? \"GET\",\n                    headers: info.headers,\n                    body: bodyToJson(info.body),\n                }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: output });\n            }\n            else {\n                throw new InferenceClientProviderApiError(`Failed to perform inference: an HTTP error occurred when requesting the provider.`, {\n                    url,\n                    method: info.method ?? \"GET\",\n                    headers: info.headers,\n                    body: bodyToJson(info.body),\n                }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: output });\n            }\n        }\n        const message = contentType?.startsWith(\"text/plain;\") ? await response.text() : undefined;\n        throw new InferenceClientProviderApiError(`Failed to perform inference: ${message ?? \"an HTTP error occurred when requesting the provider\"}`, {\n            url,\n            method: info.method ?? \"GET\",\n            headers: info.headers,\n            body: bodyToJson(info.body),\n        }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: message ?? \"\" });\n    }\n    if (response.headers.get(\"Content-Type\")?.startsWith(\"application/json\")) {\n        const data = (await response.json());\n        return { data, requestContext };\n    }\n    const blob = (await response.blob());\n    return { data: blob, requestContext };\n}\n/**\n * Primitive to make custom inference calls that expect server-sent events, and returns the response through a generator\n */\nexport async function* innerStreamingRequest(args, providerHelper, options) {\n    const { url, info } = await makeRequestOptions({ ...args, stream: true }, providerHelper, options);\n    const response = await (options?.fetch ?? fetch)(url, info);\n    if (options?.retry_on_error !== false && response.status === 503) {\n        return yield* innerStreamingRequest(args, providerHelper, options);\n    }\n    if (!response.ok) {\n        if (response.headers.get(\"Content-Type\")?.startsWith(\"application/json\")) {\n            const output = await response.json();\n            if ([400, 422, 404, 500].includes(response.status) && options?.chatCompletion) {\n                throw new InferenceClientProviderApiError(`Provider ${args.provider} does not seem to support chat completion for model ${args.model} . Error: ${JSON.stringify(output.error)}`, {\n                    url,\n                    method: info.method ?? \"GET\",\n                    headers: info.headers,\n                    body: bodyToJson(info.body),\n                }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: output });\n            }\n            if (typeof output.error === \"string\") {\n                throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.error}`, {\n                    url,\n                    method: info.method ?? \"GET\",\n                    headers: info.headers,\n                    body: bodyToJson(info.body),\n                }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: output });\n            }\n            if (output.error && \"message\" in output.error && typeof output.error.message === \"string\") {\n                /// OpenAI errors\n                throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.error.message}`, {\n                    url,\n                    method: info.method ?? \"GET\",\n                    headers: info.headers,\n                    body: bodyToJson(info.body),\n                }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: output });\n            }\n            // Sambanova errors\n            if (typeof output.message === \"string\") {\n                throw new InferenceClientProviderApiError(`Failed to perform inference: ${output.message}`, {\n                    url,\n                    method: info.method ?? \"GET\",\n                    headers: info.headers,\n                    body: bodyToJson(info.body),\n                }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: output });\n            }\n        }\n        throw new InferenceClientProviderApiError(`Failed to perform inference: an HTTP error occurred when requesting the provider.`, {\n            url,\n            method: info.method ?? \"GET\",\n            headers: info.headers,\n            body: bodyToJson(info.body),\n        }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: \"\" });\n    }\n    if (!response.headers.get(\"content-type\")?.startsWith(\"text/event-stream\")) {\n        throw new InferenceClientProviderApiError(`Failed to perform inference: server does not support event stream content type, it returned ` +\n            response.headers.get(\"content-type\"), {\n            url,\n            method: info.method ?? \"GET\",\n            headers: info.headers,\n            body: bodyToJson(info.body),\n        }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: \"\" });\n    }\n    if (!response.body) {\n        return;\n    }\n    const reader = response.body.getReader();\n    let events = [];\n    const onEvent = (event) => {\n        // accumulate events in array\n        events.push(event);\n    };\n    const onChunk = getLines(getMessages(() => { }, () => { }, onEvent));\n    try {\n        while (true) {\n            const { done, value } = await reader.read();\n            if (done) {\n                return;\n            }\n            onChunk(value);\n            for (const event of events) {\n                if (event.data.length > 0) {\n                    if (event.data === \"[DONE]\") {\n                        return;\n                    }\n                    const data = JSON.parse(event.data);\n                    if (typeof data === \"object\" && data !== null && \"error\" in data) {\n                        const errorStr = typeof data.error === \"string\"\n                            ? data.error\n                            : typeof data.error === \"object\" &&\n                                data.error &&\n                                \"message\" in data.error &&\n                                typeof data.error.message === \"string\"\n                                ? data.error.message\n                                : JSON.stringify(data.error);\n                        throw new InferenceClientProviderApiError(`Failed to perform inference: an occurred while streaming the response: ${errorStr}`, {\n                            url,\n                            method: info.method ?? \"GET\",\n                            headers: info.headers,\n                            body: bodyToJson(info.body),\n                        }, { requestId: response.headers.get(\"x-request-id\") ?? \"\", status: response.status, body: data });\n                    }\n                    yield data;\n                }\n            }\n            events = [];\n        }\n    }\n    finally {\n        reader.releaseLock();\n    }\n}\n"],"mappings":"AAAA,SAASA,kBAAkB,QAAQ,8BAA8B;AACjE,SAASC,QAAQ,EAAEC,WAAW,QAAQ,uCAAuC;AAC7E,SAASC,+BAA+B,QAAQ,cAAc;AAC9D,SAASC,UAAUA,CAACC,IAAI,EAAE;EACtB,IAAIC,IAAI,GAAG,IAAI;EACf,IAAID,IAAI,YAAYE,IAAI,IAAIF,IAAI,YAAYG,WAAW,EAAE;IACrDF,IAAI,GAAG,uBAAuB;EAClC,CAAC,MACI,IAAI,OAAOD,IAAI,KAAK,QAAQ,EAAE;IAC/B,IAAI;MACAC,IAAI,GAAGG,IAAI,CAACC,KAAK,CAACL,IAAI,CAAC;IAC3B,CAAC,CACD,MAAM;MACFC,IAAI,GAAGD,IAAI;IACf;EACJ;EACA,IAAIC,IAAI,CAACK,WAAW,EAAE;IAClBL,IAAI,CAACK,WAAW,GAAG,YAAY;EACnC;EACA,OAAOL,IAAI;AACf;AACA;AACA;AACA;AACA,OAAO,eAAeM,YAAYA,CAACC,IAAI,EAAEC,cAAc,EAAEC,OAAO,EAAE;EAC9D,MAAM;IAAEC,GAAG;IAAEC;EAAK,CAAC,GAAG,MAAMjB,kBAAkB,CAACa,IAAI,EAAEC,cAAc,EAAEC,OAAO,CAAC;EAC7E,MAAMG,QAAQ,GAAG,MAAM,CAACH,OAAO,EAAEI,KAAK,IAAIA,KAAK,EAAEH,GAAG,EAAEC,IAAI,CAAC;EAC3D,MAAMG,cAAc,GAAG;IAAEJ,GAAG;IAAEC;EAAK,CAAC;EACpC,IAAIF,OAAO,EAAEM,cAAc,KAAK,KAAK,IAAIH,QAAQ,CAACI,MAAM,KAAK,GAAG,EAAE;IAC9D,OAAOV,YAAY,CAACC,IAAI,EAAEC,cAAc,EAAEC,OAAO,CAAC;EACtD;EACA,IAAI,CAACG,QAAQ,CAACK,EAAE,EAAE;IACd,MAAMC,WAAW,GAAGN,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC;IACxD,IAAI,CAAC,kBAAkB,EAAE,0BAA0B,CAAC,CAACC,IAAI,CAAEC,EAAE,IAAKJ,WAAW,EAAEK,UAAU,CAACD,EAAE,CAAC,CAAC,EAAE;MAC5F,MAAME,MAAM,GAAG,MAAMZ,QAAQ,CAACa,IAAI,CAAC,CAAC;MACpC,IAAI,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,CAAC,CAACC,QAAQ,CAACd,QAAQ,CAACI,MAAM,CAAC,IAAIP,OAAO,EAAEkB,cAAc,EAAE;QAC3E,MAAM,IAAI9B,+BAA+B,CAAC,YAAYU,IAAI,CAACqB,QAAQ,uDAAuDrB,IAAI,CAACsB,KAAK,aAAa1B,IAAI,CAAC2B,SAAS,CAACN,MAAM,CAACO,KAAK,CAAC,EAAE,EAAE;UAC7KrB,GAAG;UACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;UAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;UACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;QAC9B,CAAC,EAAE;UAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;UAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;UAAEjB,IAAI,EAAEyB;QAAO,CAAC,CAAC;MACxG;MACA,IAAI,OAAOA,MAAM,CAACO,KAAK,KAAK,QAAQ,IAAI,OAAOP,MAAM,CAACU,MAAM,KAAK,QAAQ,IAAI,OAAOV,MAAM,CAACW,OAAO,KAAK,QAAQ,EAAE;QAC7G,MAAM,IAAItC,+BAA+B,CAAC,gCAAgC2B,MAAM,CAACO,KAAK,IAAIP,MAAM,CAACU,MAAM,IAAIV,MAAM,CAACW,OAAO,EAAE,EAAE;UACzHzB,GAAG;UACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;UAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;UACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;QAC9B,CAAC,EAAE;UAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;UAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;UAAEjB,IAAI,EAAEyB;QAAO,CAAC,CAAC;MACxG,CAAC,MACI;QACD,MAAM,IAAI3B,+BAA+B,CAAC,mFAAmF,EAAE;UAC3Ha,GAAG;UACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;UAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;UACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;QAC9B,CAAC,EAAE;UAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;UAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;UAAEjB,IAAI,EAAEyB;QAAO,CAAC,CAAC;MACxG;IACJ;IACA,MAAMW,OAAO,GAAGjB,WAAW,EAAEK,UAAU,CAAC,aAAa,CAAC,GAAG,MAAMX,QAAQ,CAACwB,IAAI,CAAC,CAAC,GAAGC,SAAS;IAC1F,MAAM,IAAIxC,+BAA+B,CAAC,gCAAgCsC,OAAO,IAAI,qDAAqD,EAAE,EAAE;MAC1IzB,GAAG;MACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;MAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;MACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;IAC9B,CAAC,EAAE;MAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;MAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;MAAEjB,IAAI,EAAEoC,OAAO,IAAI;IAAG,CAAC,CAAC;EAC/G;EACA,IAAIvB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,EAAEG,UAAU,CAAC,kBAAkB,CAAC,EAAE;IACtE,MAAMvB,IAAI,GAAI,MAAMY,QAAQ,CAACa,IAAI,CAAC,CAAE;IACpC,OAAO;MAAEzB,IAAI;MAAEc;IAAe,CAAC;EACnC;EACA,MAAMwB,IAAI,GAAI,MAAM1B,QAAQ,CAAC0B,IAAI,CAAC,CAAE;EACpC,OAAO;IAAEtC,IAAI,EAAEsC,IAAI;IAAExB;EAAe,CAAC;AACzC;AACA;AACA;AACA;AACA,OAAO,gBAAgByB,qBAAqBA,CAAChC,IAAI,EAAEC,cAAc,EAAEC,OAAO,EAAE;EACxE,MAAM;IAAEC,GAAG;IAAEC;EAAK,CAAC,GAAG,MAAMjB,kBAAkB,CAAC;IAAE,GAAGa,IAAI;IAAEiC,MAAM,EAAE;EAAK,CAAC,EAAEhC,cAAc,EAAEC,OAAO,CAAC;EAClG,MAAMG,QAAQ,GAAG,MAAM,CAACH,OAAO,EAAEI,KAAK,IAAIA,KAAK,EAAEH,GAAG,EAAEC,IAAI,CAAC;EAC3D,IAAIF,OAAO,EAAEM,cAAc,KAAK,KAAK,IAAIH,QAAQ,CAACI,MAAM,KAAK,GAAG,EAAE;IAC9D,OAAO,OAAOuB,qBAAqB,CAAChC,IAAI,EAAEC,cAAc,EAAEC,OAAO,CAAC;EACtE;EACA,IAAI,CAACG,QAAQ,CAACK,EAAE,EAAE;IACd,IAAIL,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,EAAEG,UAAU,CAAC,kBAAkB,CAAC,EAAE;MACtE,MAAMC,MAAM,GAAG,MAAMZ,QAAQ,CAACa,IAAI,CAAC,CAAC;MACpC,IAAI,CAAC,GAAG,EAAE,GAAG,EAAE,GAAG,EAAE,GAAG,CAAC,CAACC,QAAQ,CAACd,QAAQ,CAACI,MAAM,CAAC,IAAIP,OAAO,EAAEkB,cAAc,EAAE;QAC3E,MAAM,IAAI9B,+BAA+B,CAAC,YAAYU,IAAI,CAACqB,QAAQ,uDAAuDrB,IAAI,CAACsB,KAAK,aAAa1B,IAAI,CAAC2B,SAAS,CAACN,MAAM,CAACO,KAAK,CAAC,EAAE,EAAE;UAC7KrB,GAAG;UACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;UAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;UACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;QAC9B,CAAC,EAAE;UAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;UAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;UAAEjB,IAAI,EAAEyB;QAAO,CAAC,CAAC;MACxG;MACA,IAAI,OAAOA,MAAM,CAACO,KAAK,KAAK,QAAQ,EAAE;QAClC,MAAM,IAAIlC,+BAA+B,CAAC,gCAAgC2B,MAAM,CAACO,KAAK,EAAE,EAAE;UACtFrB,GAAG;UACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;UAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;UACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;QAC9B,CAAC,EAAE;UAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;UAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;UAAEjB,IAAI,EAAEyB;QAAO,CAAC,CAAC;MACxG;MACA,IAAIA,MAAM,CAACO,KAAK,IAAI,SAAS,IAAIP,MAAM,CAACO,KAAK,IAAI,OAAOP,MAAM,CAACO,KAAK,CAACI,OAAO,KAAK,QAAQ,EAAE;QACvF;QACA,MAAM,IAAItC,+BAA+B,CAAC,gCAAgC2B,MAAM,CAACO,KAAK,CAACI,OAAO,EAAE,EAAE;UAC9FzB,GAAG;UACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;UAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;UACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;QAC9B,CAAC,EAAE;UAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;UAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;UAAEjB,IAAI,EAAEyB;QAAO,CAAC,CAAC;MACxG;MACA;MACA,IAAI,OAAOA,MAAM,CAACW,OAAO,KAAK,QAAQ,EAAE;QACpC,MAAM,IAAItC,+BAA+B,CAAC,gCAAgC2B,MAAM,CAACW,OAAO,EAAE,EAAE;UACxFzB,GAAG;UACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;UAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;UACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;QAC9B,CAAC,EAAE;UAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;UAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;UAAEjB,IAAI,EAAEyB;QAAO,CAAC,CAAC;MACxG;IACJ;IACA,MAAM,IAAI3B,+BAA+B,CAAC,mFAAmF,EAAE;MAC3Ha,GAAG;MACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;MAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;MACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;IAC9B,CAAC,EAAE;MAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;MAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;MAAEjB,IAAI,EAAE;IAAG,CAAC,CAAC;EACpG;EACA,IAAI,CAACa,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,EAAEG,UAAU,CAAC,mBAAmB,CAAC,EAAE;IACxE,MAAM,IAAI1B,+BAA+B,CAAC,8FAA8F,GACpIe,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,EAAE;MACtCV,GAAG;MACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;MAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;MACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;IAC9B,CAAC,EAAE;MAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;MAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;MAAEjB,IAAI,EAAE;IAAG,CAAC,CAAC;EACpG;EACA,IAAI,CAACa,QAAQ,CAACb,IAAI,EAAE;IAChB;EACJ;EACA,MAAM0C,MAAM,GAAG7B,QAAQ,CAACb,IAAI,CAAC2C,SAAS,CAAC,CAAC;EACxC,IAAIC,MAAM,GAAG,EAAE;EACf,MAAMC,OAAO,GAAIC,KAAK,IAAK;IACvB;IACAF,MAAM,CAACG,IAAI,CAACD,KAAK,CAAC;EACtB,CAAC;EACD,MAAME,OAAO,GAAGpD,QAAQ,CAACC,WAAW,CAAC,MAAM,CAAE,CAAC,EAAE,MAAM,CAAE,CAAC,EAAEgD,OAAO,CAAC,CAAC;EACpE,IAAI;IACA,OAAO,IAAI,EAAE;MACT,MAAM;QAAEI,IAAI;QAAEC;MAAM,CAAC,GAAG,MAAMR,MAAM,CAACS,IAAI,CAAC,CAAC;MAC3C,IAAIF,IAAI,EAAE;QACN;MACJ;MACAD,OAAO,CAACE,KAAK,CAAC;MACd,KAAK,MAAMJ,KAAK,IAAIF,MAAM,EAAE;QACxB,IAAIE,KAAK,CAAC7C,IAAI,CAACmD,MAAM,GAAG,CAAC,EAAE;UACvB,IAAIN,KAAK,CAAC7C,IAAI,KAAK,QAAQ,EAAE;YACzB;UACJ;UACA,MAAMA,IAAI,GAAGG,IAAI,CAACC,KAAK,CAACyC,KAAK,CAAC7C,IAAI,CAAC;UACnC,IAAI,OAAOA,IAAI,KAAK,QAAQ,IAAIA,IAAI,KAAK,IAAI,IAAI,OAAO,IAAIA,IAAI,EAAE;YAC9D,MAAMoD,QAAQ,GAAG,OAAOpD,IAAI,CAAC+B,KAAK,KAAK,QAAQ,GACzC/B,IAAI,CAAC+B,KAAK,GACV,OAAO/B,IAAI,CAAC+B,KAAK,KAAK,QAAQ,IAC5B/B,IAAI,CAAC+B,KAAK,IACV,SAAS,IAAI/B,IAAI,CAAC+B,KAAK,IACvB,OAAO/B,IAAI,CAAC+B,KAAK,CAACI,OAAO,KAAK,QAAQ,GACpCnC,IAAI,CAAC+B,KAAK,CAACI,OAAO,GAClBhC,IAAI,CAAC2B,SAAS,CAAC9B,IAAI,CAAC+B,KAAK,CAAC;YACpC,MAAM,IAAIlC,+BAA+B,CAAC,0EAA0EuD,QAAQ,EAAE,EAAE;cAC5H1C,GAAG;cACHsB,MAAM,EAAErB,IAAI,CAACqB,MAAM,IAAI,KAAK;cAC5Bb,OAAO,EAAER,IAAI,CAACQ,OAAO;cACrBpB,IAAI,EAAED,UAAU,CAACa,IAAI,CAACZ,IAAI;YAC9B,CAAC,EAAE;cAAEkC,SAAS,EAAErB,QAAQ,CAACO,OAAO,CAACC,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;cAAEJ,MAAM,EAAEJ,QAAQ,CAACI,MAAM;cAAEjB,IAAI,EAAEC;YAAK,CAAC,CAAC;UACtG;UACA,MAAMA,IAAI;QACd;MACJ;MACA2C,MAAM,GAAG,EAAE;IACf;EACJ,CAAC,SACO;IACJF,MAAM,CAACY,WAAW,CAAC,CAAC;EACxB;AACJ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}