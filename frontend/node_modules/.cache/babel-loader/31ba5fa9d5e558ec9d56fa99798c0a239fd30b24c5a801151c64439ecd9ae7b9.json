{"ast":null,"code":"const taskData = {\n  datasets: [{\n    description: \"Bing queries with relevant passages from various web sources.\",\n    id: \"microsoft/ms_marco\"\n  }],\n  demo: {\n    inputs: [{\n      label: \"Source sentence\",\n      content: \"Machine learning is so easy.\",\n      type: \"text\"\n    }, {\n      label: \"Sentences to compare to\",\n      content: \"Deep learning is so straightforward.\",\n      type: \"text\"\n    }, {\n      label: \"\",\n      content: \"This is so difficult, like rocket science.\",\n      type: \"text\"\n    }, {\n      label: \"\",\n      content: \"I can't believe how much I struggled with this.\",\n      type: \"text\"\n    }],\n    outputs: [{\n      type: \"chart\",\n      data: [{\n        label: \"Deep learning is so straightforward.\",\n        score: 0.623\n      }, {\n        label: \"This is so difficult, like rocket science.\",\n        score: 0.413\n      }, {\n        label: \"I can't believe how much I struggled with this.\",\n        score: 0.256\n      }]\n    }]\n  },\n  metrics: [{\n    description: \"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1\",\n    id: \"Mean Reciprocal Rank\"\n  }, {\n    description: \"The similarity of the embeddings is evaluated mainly on cosine similarity. It is calculated as the cosine of the angle between two vectors. It is particularly useful when your texts are not the same length\",\n    id: \"Cosine Similarity\"\n  }],\n  models: [{\n    description: \"This model works well for sentences and paragraphs and can be used for clustering/grouping and semantic searches.\",\n    id: \"sentence-transformers/all-mpnet-base-v2\"\n  }, {\n    description: \"A multilingual robust sentence similarity model.\",\n    id: \"BAAI/bge-m3\"\n  }, {\n    description: \"A robust sentence similarity model.\",\n    id: \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\"\n  }],\n  spaces: [{\n    description: \"An application that leverages sentence similarity to answer questions from YouTube videos.\",\n    id: \"Gradio-Blocks/Ask_Questions_To_YouTube_Videos\"\n  }, {\n    description: \"An application that retrieves relevant PubMed abstracts for a given online article which can be used as further references.\",\n    id: \"Gradio-Blocks/pubmed-abstract-retriever\"\n  }, {\n    description: \"An application that leverages sentence similarity to summarize text.\",\n    id: \"nickmuchi/article-text-summarizer\"\n  }, {\n    description: \"A guide that explains how Sentence Transformers can be used for semantic search.\",\n    id: \"sentence-transformers/Sentence_Transformers_for_semantic_search\"\n  }],\n  summary: \"Sentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping.\",\n  widgetModels: [\"sentence-transformers/all-MiniLM-L6-v2\"],\n  youtubeId: \"VCZq5AkbNEU\"\n};\nexport default taskData;","map":{"version":3,"names":["taskData","datasets","description","id","demo","inputs","label","content","type","outputs","data","score","metrics","models","spaces","summary","widgetModels","youtubeId"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/tasks/sentence-similarity/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Bing queries with relevant passages from various web sources.\",\n            id: \"microsoft/ms_marco\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"Source sentence\",\n                content: \"Machine learning is so easy.\",\n                type: \"text\",\n            },\n            {\n                label: \"Sentences to compare to\",\n                content: \"Deep learning is so straightforward.\",\n                type: \"text\",\n            },\n            {\n                label: \"\",\n                content: \"This is so difficult, like rocket science.\",\n                type: \"text\",\n            },\n            {\n                label: \"\",\n                content: \"I can't believe how much I struggled with this.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                type: \"chart\",\n                data: [\n                    {\n                        label: \"Deep learning is so straightforward.\",\n                        score: 0.623,\n                    },\n                    {\n                        label: \"This is so difficult, like rocket science.\",\n                        score: 0.413,\n                    },\n                    {\n                        label: \"I can't believe how much I struggled with this.\",\n                        score: 0.256,\n                    },\n                ],\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Reciprocal Rank is a measure used to rank the relevancy of documents given a set of documents. Reciprocal Rank is the reciprocal of the rank of the document retrieved, meaning, if the rank is 3, the Reciprocal Rank is 0.33. If the rank is 1, the Reciprocal Rank is 1\",\n            id: \"Mean Reciprocal Rank\",\n        },\n        {\n            description: \"The similarity of the embeddings is evaluated mainly on cosine similarity. It is calculated as the cosine of the angle between two vectors. It is particularly useful when your texts are not the same length\",\n            id: \"Cosine Similarity\",\n        },\n    ],\n    models: [\n        {\n            description: \"This model works well for sentences and paragraphs and can be used for clustering/grouping and semantic searches.\",\n            id: \"sentence-transformers/all-mpnet-base-v2\",\n        },\n        {\n            description: \"A multilingual robust sentence similarity model.\",\n            id: \"BAAI/bge-m3\",\n        },\n        {\n            description: \"A robust sentence similarity model.\",\n            id: \"HIT-TMG/KaLM-embedding-multilingual-mini-instruct-v1.5\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application that leverages sentence similarity to answer questions from YouTube videos.\",\n            id: \"Gradio-Blocks/Ask_Questions_To_YouTube_Videos\",\n        },\n        {\n            description: \"An application that retrieves relevant PubMed abstracts for a given online article which can be used as further references.\",\n            id: \"Gradio-Blocks/pubmed-abstract-retriever\",\n        },\n        {\n            description: \"An application that leverages sentence similarity to summarize text.\",\n            id: \"nickmuchi/article-text-summarizer\",\n        },\n        {\n            description: \"A guide that explains how Sentence Transformers can be used for semantic search.\",\n            id: \"sentence-transformers/Sentence_Transformers_for_semantic_search\",\n        },\n    ],\n    summary: \"Sentence Similarity is the task of determining how similar two texts are. Sentence similarity models convert input texts into vectors (embeddings) that capture semantic information and calculate how close (similar) they are between them. This task is particularly useful for information retrieval and clustering/grouping.\",\n    widgetModels: [\"sentence-transformers/all-MiniLM-L6-v2\"],\n    youtubeId: \"VCZq5AkbNEU\",\n};\nexport default taskData;\n"],"mappings":"AAAA,MAAMA,QAAQ,GAAG;EACbC,QAAQ,EAAE,CACN;IACIC,WAAW,EAAE,+DAA+D;IAC5EC,EAAE,EAAE;EACR,CAAC,CACJ;EACDC,IAAI,EAAE;IACFC,MAAM,EAAE,CACJ;MACIC,KAAK,EAAE,iBAAiB;MACxBC,OAAO,EAAE,8BAA8B;MACvCC,IAAI,EAAE;IACV,CAAC,EACD;MACIF,KAAK,EAAE,yBAAyB;MAChCC,OAAO,EAAE,sCAAsC;MAC/CC,IAAI,EAAE;IACV,CAAC,EACD;MACIF,KAAK,EAAE,EAAE;MACTC,OAAO,EAAE,4CAA4C;MACrDC,IAAI,EAAE;IACV,CAAC,EACD;MACIF,KAAK,EAAE,EAAE;MACTC,OAAO,EAAE,iDAAiD;MAC1DC,IAAI,EAAE;IACV,CAAC,CACJ;IACDC,OAAO,EAAE,CACL;MACID,IAAI,EAAE,OAAO;MACbE,IAAI,EAAE,CACF;QACIJ,KAAK,EAAE,sCAAsC;QAC7CK,KAAK,EAAE;MACX,CAAC,EACD;QACIL,KAAK,EAAE,4CAA4C;QACnDK,KAAK,EAAE;MACX,CAAC,EACD;QACIL,KAAK,EAAE,iDAAiD;QACxDK,KAAK,EAAE;MACX,CAAC;IAET,CAAC;EAET,CAAC;EACDC,OAAO,EAAE,CACL;IACIV,WAAW,EAAE,4QAA4Q;IACzRC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,+MAA+M;IAC5NC,EAAE,EAAE;EACR,CAAC,CACJ;EACDU,MAAM,EAAE,CACJ;IACIX,WAAW,EAAE,mHAAmH;IAChIC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,kDAAkD;IAC/DC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,qCAAqC;IAClDC,EAAE,EAAE;EACR,CAAC,CACJ;EACDW,MAAM,EAAE,CACJ;IACIZ,WAAW,EAAE,4FAA4F;IACzGC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,6HAA6H;IAC1IC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,sEAAsE;IACnFC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,kFAAkF;IAC/FC,EAAE,EAAE;EACR,CAAC,CACJ;EACDY,OAAO,EAAE,mUAAmU;EAC5UC,YAAY,EAAE,CAAC,wCAAwC,CAAC;EACxDC,SAAS,EAAE;AACf,CAAC;AACD,eAAejB,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}