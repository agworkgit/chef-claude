{"ast":null,"code":"const taskData = {\n  datasets: [{\n    description: \"A curation of widely used datasets for Data Driven Deep Reinforcement Learning (D4RL)\",\n    id: \"edbeeching/decision_transformer_gym_replay\"\n  }],\n  demo: {\n    inputs: [{\n      label: \"State\",\n      content: \"Red traffic light, pedestrians are about to pass.\",\n      type: \"text\"\n    }],\n    outputs: [{\n      label: \"Action\",\n      content: \"Stop the car.\",\n      type: \"text\"\n    }, {\n      label: \"Next State\",\n      content: \"Yellow light, pedestrians have crossed.\",\n      type: \"text\"\n    }]\n  },\n  metrics: [{\n    description: \"Accumulated reward across all time steps discounted by a factor that ranges between 0 and 1 and determines how much the agent optimizes for future relative to immediate rewards. Measures how good is the policy ultimately found by a given algorithm considering uncertainty over the future.\",\n    id: \"Discounted Total Reward\"\n  }, {\n    description: \"Average return obtained after running the policy for a certain number of evaluation episodes. As opposed to total reward, mean reward considers how much reward a given algorithm receives while learning.\",\n    id: \"Mean Reward\"\n  }, {\n    description: \"Measures how good a given algorithm is after a predefined time. Some algorithms may be guaranteed to converge to optimal behavior across many time steps. However, an agent that reaches an acceptable level of optimality after a given time horizon may be preferable to one that ultimately reaches optimality but takes a long time.\",\n    id: \"Level of Performance After Some Time\"\n  }],\n  models: [{\n    description: \"A Reinforcement Learning model trained on expert data from the Gym Hopper environment\",\n    id: \"edbeeching/decision-transformer-gym-hopper-expert\"\n  }, {\n    description: \"A PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo.\",\n    id: \"HumanCompatibleAI/ppo-seals-CartPole-v0\"\n  }],\n  spaces: [{\n    description: \"An application for a cute puppy agent learning to catch a stick.\",\n    id: \"ThomasSimonini/Huggy\"\n  }, {\n    description: \"An application to play Snowball Fight with a reinforcement learning agent.\",\n    id: \"ThomasSimonini/SnowballFight\"\n  }],\n  summary: \"Reinforcement learning is the computational approach of learning from action by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback\",\n  widgetModels: [],\n  youtubeId: \"q0BiUn5LiBc\"\n};\nexport default taskData;","map":{"version":3,"names":["taskData","datasets","description","id","demo","inputs","label","content","type","outputs","metrics","models","spaces","summary","widgetModels","youtubeId"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/tasks/reinforcement-learning/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"A curation of widely used datasets for Data Driven Deep Reinforcement Learning (D4RL)\",\n            id: \"edbeeching/decision_transformer_gym_replay\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                label: \"State\",\n                content: \"Red traffic light, pedestrians are about to pass.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Action\",\n                content: \"Stop the car.\",\n                type: \"text\",\n            },\n            {\n                label: \"Next State\",\n                content: \"Yellow light, pedestrians have crossed.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [\n        {\n            description: \"Accumulated reward across all time steps discounted by a factor that ranges between 0 and 1 and determines how much the agent optimizes for future relative to immediate rewards. Measures how good is the policy ultimately found by a given algorithm considering uncertainty over the future.\",\n            id: \"Discounted Total Reward\",\n        },\n        {\n            description: \"Average return obtained after running the policy for a certain number of evaluation episodes. As opposed to total reward, mean reward considers how much reward a given algorithm receives while learning.\",\n            id: \"Mean Reward\",\n        },\n        {\n            description: \"Measures how good a given algorithm is after a predefined time. Some algorithms may be guaranteed to converge to optimal behavior across many time steps. However, an agent that reaches an acceptable level of optimality after a given time horizon may be preferable to one that ultimately reaches optimality but takes a long time.\",\n            id: \"Level of Performance After Some Time\",\n        },\n    ],\n    models: [\n        {\n            description: \"A Reinforcement Learning model trained on expert data from the Gym Hopper environment\",\n            id: \"edbeeching/decision-transformer-gym-hopper-expert\",\n        },\n        {\n            description: \"A PPO agent playing seals/CartPole-v0 using the stable-baselines3 library and the RL Zoo.\",\n            id: \"HumanCompatibleAI/ppo-seals-CartPole-v0\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application for a cute puppy agent learning to catch a stick.\",\n            id: \"ThomasSimonini/Huggy\",\n        },\n        {\n            description: \"An application to play Snowball Fight with a reinforcement learning agent.\",\n            id: \"ThomasSimonini/SnowballFight\",\n        },\n    ],\n    summary: \"Reinforcement learning is the computational approach of learning from action by interacting with an environment through trial and error and receiving rewards (negative or positive) as feedback\",\n    widgetModels: [],\n    youtubeId: \"q0BiUn5LiBc\",\n};\nexport default taskData;\n"],"mappings":"AAAA,MAAMA,QAAQ,GAAG;EACbC,QAAQ,EAAE,CACN;IACIC,WAAW,EAAE,uFAAuF;IACpGC,EAAE,EAAE;EACR,CAAC,CACJ;EACDC,IAAI,EAAE;IACFC,MAAM,EAAE,CACJ;MACIC,KAAK,EAAE,OAAO;MACdC,OAAO,EAAE,mDAAmD;MAC5DC,IAAI,EAAE;IACV,CAAC,CACJ;IACDC,OAAO,EAAE,CACL;MACIH,KAAK,EAAE,QAAQ;MACfC,OAAO,EAAE,eAAe;MACxBC,IAAI,EAAE;IACV,CAAC,EACD;MACIF,KAAK,EAAE,YAAY;MACnBC,OAAO,EAAE,yCAAyC;MAClDC,IAAI,EAAE;IACV,CAAC;EAET,CAAC;EACDE,OAAO,EAAE,CACL;IACIR,WAAW,EAAE,kSAAkS;IAC/SC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,4MAA4M;IACzNC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,0UAA0U;IACvVC,EAAE,EAAE;EACR,CAAC,CACJ;EACDQ,MAAM,EAAE,CACJ;IACIT,WAAW,EAAE,uFAAuF;IACpGC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,2FAA2F;IACxGC,EAAE,EAAE;EACR,CAAC,CACJ;EACDS,MAAM,EAAE,CACJ;IACIV,WAAW,EAAE,kEAAkE;IAC/EC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,4EAA4E;IACzFC,EAAE,EAAE;EACR,CAAC,CACJ;EACDU,OAAO,EAAE,kMAAkM;EAC3MC,YAAY,EAAE,EAAE;EAChBC,SAAS,EAAE;AACf,CAAC;AACD,eAAef,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}