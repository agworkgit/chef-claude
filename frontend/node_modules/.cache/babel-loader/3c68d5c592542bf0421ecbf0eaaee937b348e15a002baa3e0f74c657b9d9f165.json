{"ast":null,"code":"// This list is copied from gguf/types.ts, but will all types available (for backward compatibility)\n// NOT to be confused with GGMLQuantizationType, a FileQuantization can contain multiple GGMLQuantizationType\n// For example, Q4_K_M model can contains Q4_K and Q6_K tensors\nexport var GGMLFileQuantizationType;\n(function (GGMLFileQuantizationType) {\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"F32\"] = 0] = \"F32\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"F16\"] = 1] = \"F16\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0\"] = 2] = \"Q4_0\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_1\"] = 3] = \"Q4_1\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_1_SOME_F16\"] = 4] = \"Q4_1_SOME_F16\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_2\"] = 5] = \"Q4_2\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_3\"] = 6] = \"Q4_3\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q8_0\"] = 7] = \"Q8_0\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_0\"] = 8] = \"Q5_0\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_1\"] = 9] = \"Q5_1\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K\"] = 10] = \"Q2_K\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_S\"] = 11] = \"Q3_K_S\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_M\"] = 12] = \"Q3_K_M\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_L\"] = 13] = \"Q3_K_L\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_S\"] = 14] = \"Q4_K_S\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_M\"] = 15] = \"Q4_K_M\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_S\"] = 16] = \"Q5_K_S\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_M\"] = 17] = \"Q5_K_M\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q6_K\"] = 18] = \"Q6_K\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_XXS\"] = 19] = \"IQ2_XXS\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_XS\"] = 20] = \"IQ2_XS\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K_S\"] = 21] = \"Q2_K_S\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_XS\"] = 22] = \"IQ3_XS\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_XXS\"] = 23] = \"IQ3_XXS\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ1_S\"] = 24] = \"IQ1_S\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ4_NL\"] = 25] = \"IQ4_NL\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_S\"] = 26] = \"IQ3_S\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_M\"] = 27] = \"IQ3_M\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_S\"] = 28] = \"IQ2_S\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_M\"] = 29] = \"IQ2_M\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ4_XS\"] = 30] = \"IQ4_XS\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ1_M\"] = 31] = \"IQ1_M\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"BF16\"] = 32] = \"BF16\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_4_4\"] = 33] = \"Q4_0_4_4\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_4_8\"] = 34] = \"Q4_0_4_8\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_8_8\"] = 35] = \"Q4_0_8_8\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"TQ1_0\"] = 36] = \"TQ1_0\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"TQ2_0\"] = 37] = \"TQ2_0\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"MXFP4_MOE\"] = 38] = \"MXFP4_MOE\";\n  // custom quants used by unsloth\n  // they are not officially a scheme enum value in GGUF, but only here for naming\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K_XL\"] = 1000] = \"Q2_K_XL\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_XL\"] = 1001] = \"Q3_K_XL\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_XL\"] = 1002] = \"Q4_K_XL\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_XL\"] = 1003] = \"Q5_K_XL\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q6_K_XL\"] = 1004] = \"Q6_K_XL\";\n  GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q8_K_XL\"] = 1005] = \"Q8_K_XL\";\n})(GGMLFileQuantizationType || (GGMLFileQuantizationType = {}));\nconst ggufQuants = Object.values(GGMLFileQuantizationType).filter(v => typeof v === \"string\");\nexport const GGUF_QUANT_RE = new RegExp(`(?<quant>${ggufQuants.join(\"|\")})` + \"(_(?<sizeVariation>[A-Z]+))?\");\nexport const GGUF_QUANT_RE_GLOBAL = new RegExp(GGUF_QUANT_RE, \"g\");\nexport function parseGGUFQuantLabel(fname) {\n  const quantLabel = fname.toUpperCase().match(GGUF_QUANT_RE_GLOBAL)?.at(-1); // if there is multiple quant substrings in a name, we prefer the last one\n  return quantLabel;\n}\n// order of quantization, from biggest to smallest\n// this list must be in sync with the order in GGMLFileQuantizationType\n// the gguf.spec.ts tests are using verify if the order is correct\nexport const GGUF_QUANT_ORDER = [GGMLFileQuantizationType.F32, GGMLFileQuantizationType.BF16, GGMLFileQuantizationType.F16, GGMLFileQuantizationType.Q8_K_XL, GGMLFileQuantizationType.Q8_0,\n// 6-bit quantizations\nGGMLFileQuantizationType.Q6_K_XL, GGMLFileQuantizationType.Q6_K,\n// 5-bit quantizations\nGGMLFileQuantizationType.Q5_K_XL, GGMLFileQuantizationType.Q5_K_M, GGMLFileQuantizationType.Q5_K_S, GGMLFileQuantizationType.Q5_0, GGMLFileQuantizationType.Q5_1,\n// 4-bit quantizations\nGGMLFileQuantizationType.Q4_K_XL, GGMLFileQuantizationType.Q4_K_M, GGMLFileQuantizationType.Q4_K_S, GGMLFileQuantizationType.IQ4_NL, GGMLFileQuantizationType.IQ4_XS, GGMLFileQuantizationType.Q4_0_4_4, GGMLFileQuantizationType.Q4_0_4_8, GGMLFileQuantizationType.Q4_0_8_8, GGMLFileQuantizationType.Q4_1_SOME_F16, GGMLFileQuantizationType.Q4_0, GGMLFileQuantizationType.Q4_1, GGMLFileQuantizationType.Q4_2, GGMLFileQuantizationType.Q4_3, GGMLFileQuantizationType.MXFP4_MOE,\n// 3-bit quantizations\nGGMLFileQuantizationType.Q3_K_XL, GGMLFileQuantizationType.Q3_K_L, GGMLFileQuantizationType.Q3_K_M, GGMLFileQuantizationType.Q3_K_S, GGMLFileQuantizationType.IQ3_M, GGMLFileQuantizationType.IQ3_S, GGMLFileQuantizationType.IQ3_XS, GGMLFileQuantizationType.IQ3_XXS,\n// 2-bit quantizations\nGGMLFileQuantizationType.Q2_K_XL, GGMLFileQuantizationType.Q2_K, GGMLFileQuantizationType.Q2_K_S, GGMLFileQuantizationType.IQ2_M, GGMLFileQuantizationType.IQ2_S, GGMLFileQuantizationType.IQ2_XS, GGMLFileQuantizationType.IQ2_XXS,\n// 1-bit quantizations\nGGMLFileQuantizationType.IQ1_S, GGMLFileQuantizationType.IQ1_M, GGMLFileQuantizationType.TQ1_0, GGMLFileQuantizationType.TQ2_0];\n// This function finds the nearest quantization type that is less than or equal to the given quantization type.\n// It returns undefined if no such quantization type is found.\nexport function findNearestQuantType(quant, availableQuants) {\n  // Create a map for quick index lookup from the defined order\n  const orderMap = new Map();\n  GGUF_QUANT_ORDER.forEach((q, index) => {\n    orderMap.set(q, index);\n  });\n  const targetIndex = orderMap.get(quant) ?? 0; // the 0 case should never happen\n  // Filter the available quantizations to include only those defined in the order map,\n  // then sort them according to the GGUF_QUANT_ORDER (from largest/index 0 to smallest/highest index).\n  const sortedAvailable = availableQuants.filter(q => orderMap.has(q)).sort((a, b) => (orderMap.get(a) ?? Infinity) - (orderMap.get(b) ?? Infinity));\n  // If no valid quantizations are available after filtering\n  if (sortedAvailable.length === 0) {\n    return undefined;\n  }\n  // Iterate through the sorted available quantizations (largest to smallest).\n  // Find the first one whose order index is >= the target index.\n  // This means finding the largest quantization that is smaller than or equal to the target.\n  for (const availableQuant of sortedAvailable) {\n    // We know the key exists due to the filter above.\n    const availableIndex = orderMap.get(availableQuant) ?? 0;\n    if (availableIndex >= targetIndex) {\n      return availableQuant;\n    }\n  }\n  // If the loop completes, it means all available quantizations are larger (have a smaller index)\n  // than the target quantization. In this case, return the \"smallest\" available quantization,\n  // which is the last element in the sorted list (highest index among available).\n  return sortedAvailable[sortedAvailable.length - 1];\n}\n// This list is only used to calculate the size of the model, NOT to be confused with the quantization FILE type\nexport var GGMLQuantizationType;\n(function (GGMLQuantizationType) {\n  GGMLQuantizationType[GGMLQuantizationType[\"F32\"] = 0] = \"F32\";\n  GGMLQuantizationType[GGMLQuantizationType[\"F16\"] = 1] = \"F16\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q4_0\"] = 2] = \"Q4_0\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q4_1\"] = 3] = \"Q4_1\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q5_0\"] = 6] = \"Q5_0\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q5_1\"] = 7] = \"Q5_1\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q8_0\"] = 8] = \"Q8_0\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q8_1\"] = 9] = \"Q8_1\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q2_K\"] = 10] = \"Q2_K\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q3_K\"] = 11] = \"Q3_K\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q4_K\"] = 12] = \"Q4_K\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q5_K\"] = 13] = \"Q5_K\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q6_K\"] = 14] = \"Q6_K\";\n  GGMLQuantizationType[GGMLQuantizationType[\"Q8_K\"] = 15] = \"Q8_K\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ2_XXS\"] = 16] = \"IQ2_XXS\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ2_XS\"] = 17] = \"IQ2_XS\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ3_XXS\"] = 18] = \"IQ3_XXS\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ1_S\"] = 19] = \"IQ1_S\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ4_NL\"] = 20] = \"IQ4_NL\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ3_S\"] = 21] = \"IQ3_S\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ2_S\"] = 22] = \"IQ2_S\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ4_XS\"] = 23] = \"IQ4_XS\";\n  GGMLQuantizationType[GGMLQuantizationType[\"I8\"] = 24] = \"I8\";\n  GGMLQuantizationType[GGMLQuantizationType[\"I16\"] = 25] = \"I16\";\n  GGMLQuantizationType[GGMLQuantizationType[\"I32\"] = 26] = \"I32\";\n  GGMLQuantizationType[GGMLQuantizationType[\"I64\"] = 27] = \"I64\";\n  GGMLQuantizationType[GGMLQuantizationType[\"F64\"] = 28] = \"F64\";\n  GGMLQuantizationType[GGMLQuantizationType[\"IQ1_M\"] = 29] = \"IQ1_M\";\n  GGMLQuantizationType[GGMLQuantizationType[\"BF16\"] = 30] = \"BF16\";\n  GGMLQuantizationType[GGMLQuantizationType[\"TQ1_0\"] = 34] = \"TQ1_0\";\n  GGMLQuantizationType[GGMLQuantizationType[\"TQ2_0\"] = 35] = \"TQ2_0\";\n  GGMLQuantizationType[GGMLQuantizationType[\"MXFP4\"] = 39] = \"MXFP4\";\n})(GGMLQuantizationType || (GGMLQuantizationType = {}));","map":{"version":3,"names":["GGMLFileQuantizationType","ggufQuants","Object","values","filter","v","GGUF_QUANT_RE","RegExp","join","GGUF_QUANT_RE_GLOBAL","parseGGUFQuantLabel","fname","quantLabel","toUpperCase","match","at","GGUF_QUANT_ORDER","F32","BF16","F16","Q8_K_XL","Q8_0","Q6_K_XL","Q6_K","Q5_K_XL","Q5_K_M","Q5_K_S","Q5_0","Q5_1","Q4_K_XL","Q4_K_M","Q4_K_S","IQ4_NL","IQ4_XS","Q4_0_4_4","Q4_0_4_8","Q4_0_8_8","Q4_1_SOME_F16","Q4_0","Q4_1","Q4_2","Q4_3","MXFP4_MOE","Q3_K_XL","Q3_K_L","Q3_K_M","Q3_K_S","IQ3_M","IQ3_S","IQ3_XS","IQ3_XXS","Q2_K_XL","Q2_K","Q2_K_S","IQ2_M","IQ2_S","IQ2_XS","IQ2_XXS","IQ1_S","IQ1_M","TQ1_0","TQ2_0","findNearestQuantType","quant","availableQuants","orderMap","Map","forEach","q","index","set","targetIndex","get","sortedAvailable","has","sort","a","b","Infinity","length","undefined","availableQuant","availableIndex","GGMLQuantizationType"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/gguf.js"],"sourcesContent":["// This list is copied from gguf/types.ts, but will all types available (for backward compatibility)\n// NOT to be confused with GGMLQuantizationType, a FileQuantization can contain multiple GGMLQuantizationType\n// For example, Q4_K_M model can contains Q4_K and Q6_K tensors\nexport var GGMLFileQuantizationType;\n(function (GGMLFileQuantizationType) {\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"F32\"] = 0] = \"F32\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"F16\"] = 1] = \"F16\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0\"] = 2] = \"Q4_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_1\"] = 3] = \"Q4_1\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_1_SOME_F16\"] = 4] = \"Q4_1_SOME_F16\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_2\"] = 5] = \"Q4_2\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_3\"] = 6] = \"Q4_3\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q8_0\"] = 7] = \"Q8_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_0\"] = 8] = \"Q5_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_1\"] = 9] = \"Q5_1\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K\"] = 10] = \"Q2_K\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_S\"] = 11] = \"Q3_K_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_M\"] = 12] = \"Q3_K_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_L\"] = 13] = \"Q3_K_L\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_S\"] = 14] = \"Q4_K_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_M\"] = 15] = \"Q4_K_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_S\"] = 16] = \"Q5_K_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_M\"] = 17] = \"Q5_K_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q6_K\"] = 18] = \"Q6_K\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_XXS\"] = 19] = \"IQ2_XXS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_XS\"] = 20] = \"IQ2_XS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K_S\"] = 21] = \"Q2_K_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_XS\"] = 22] = \"IQ3_XS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_XXS\"] = 23] = \"IQ3_XXS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ1_S\"] = 24] = \"IQ1_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ4_NL\"] = 25] = \"IQ4_NL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_S\"] = 26] = \"IQ3_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ3_M\"] = 27] = \"IQ3_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_S\"] = 28] = \"IQ2_S\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ2_M\"] = 29] = \"IQ2_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ4_XS\"] = 30] = \"IQ4_XS\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"IQ1_M\"] = 31] = \"IQ1_M\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"BF16\"] = 32] = \"BF16\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_4_4\"] = 33] = \"Q4_0_4_4\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_4_8\"] = 34] = \"Q4_0_4_8\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_0_8_8\"] = 35] = \"Q4_0_8_8\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"TQ1_0\"] = 36] = \"TQ1_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"TQ2_0\"] = 37] = \"TQ2_0\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"MXFP4_MOE\"] = 38] = \"MXFP4_MOE\";\n    // custom quants used by unsloth\n    // they are not officially a scheme enum value in GGUF, but only here for naming\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q2_K_XL\"] = 1000] = \"Q2_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q3_K_XL\"] = 1001] = \"Q3_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q4_K_XL\"] = 1002] = \"Q4_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q5_K_XL\"] = 1003] = \"Q5_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q6_K_XL\"] = 1004] = \"Q6_K_XL\";\n    GGMLFileQuantizationType[GGMLFileQuantizationType[\"Q8_K_XL\"] = 1005] = \"Q8_K_XL\";\n})(GGMLFileQuantizationType || (GGMLFileQuantizationType = {}));\nconst ggufQuants = Object.values(GGMLFileQuantizationType).filter((v) => typeof v === \"string\");\nexport const GGUF_QUANT_RE = new RegExp(`(?<quant>${ggufQuants.join(\"|\")})` + \"(_(?<sizeVariation>[A-Z]+))?\");\nexport const GGUF_QUANT_RE_GLOBAL = new RegExp(GGUF_QUANT_RE, \"g\");\nexport function parseGGUFQuantLabel(fname) {\n    const quantLabel = fname.toUpperCase().match(GGUF_QUANT_RE_GLOBAL)?.at(-1); // if there is multiple quant substrings in a name, we prefer the last one\n    return quantLabel;\n}\n// order of quantization, from biggest to smallest\n// this list must be in sync with the order in GGMLFileQuantizationType\n// the gguf.spec.ts tests are using verify if the order is correct\nexport const GGUF_QUANT_ORDER = [\n    GGMLFileQuantizationType.F32,\n    GGMLFileQuantizationType.BF16,\n    GGMLFileQuantizationType.F16,\n    GGMLFileQuantizationType.Q8_K_XL,\n    GGMLFileQuantizationType.Q8_0,\n    // 6-bit quantizations\n    GGMLFileQuantizationType.Q6_K_XL,\n    GGMLFileQuantizationType.Q6_K,\n    // 5-bit quantizations\n    GGMLFileQuantizationType.Q5_K_XL,\n    GGMLFileQuantizationType.Q5_K_M,\n    GGMLFileQuantizationType.Q5_K_S,\n    GGMLFileQuantizationType.Q5_0,\n    GGMLFileQuantizationType.Q5_1,\n    // 4-bit quantizations\n    GGMLFileQuantizationType.Q4_K_XL,\n    GGMLFileQuantizationType.Q4_K_M,\n    GGMLFileQuantizationType.Q4_K_S,\n    GGMLFileQuantizationType.IQ4_NL,\n    GGMLFileQuantizationType.IQ4_XS,\n    GGMLFileQuantizationType.Q4_0_4_4,\n    GGMLFileQuantizationType.Q4_0_4_8,\n    GGMLFileQuantizationType.Q4_0_8_8,\n    GGMLFileQuantizationType.Q4_1_SOME_F16,\n    GGMLFileQuantizationType.Q4_0,\n    GGMLFileQuantizationType.Q4_1,\n    GGMLFileQuantizationType.Q4_2,\n    GGMLFileQuantizationType.Q4_3,\n    GGMLFileQuantizationType.MXFP4_MOE,\n    // 3-bit quantizations\n    GGMLFileQuantizationType.Q3_K_XL,\n    GGMLFileQuantizationType.Q3_K_L,\n    GGMLFileQuantizationType.Q3_K_M,\n    GGMLFileQuantizationType.Q3_K_S,\n    GGMLFileQuantizationType.IQ3_M,\n    GGMLFileQuantizationType.IQ3_S,\n    GGMLFileQuantizationType.IQ3_XS,\n    GGMLFileQuantizationType.IQ3_XXS,\n    // 2-bit quantizations\n    GGMLFileQuantizationType.Q2_K_XL,\n    GGMLFileQuantizationType.Q2_K,\n    GGMLFileQuantizationType.Q2_K_S,\n    GGMLFileQuantizationType.IQ2_M,\n    GGMLFileQuantizationType.IQ2_S,\n    GGMLFileQuantizationType.IQ2_XS,\n    GGMLFileQuantizationType.IQ2_XXS,\n    // 1-bit quantizations\n    GGMLFileQuantizationType.IQ1_S,\n    GGMLFileQuantizationType.IQ1_M,\n    GGMLFileQuantizationType.TQ1_0,\n    GGMLFileQuantizationType.TQ2_0,\n];\n// This function finds the nearest quantization type that is less than or equal to the given quantization type.\n// It returns undefined if no such quantization type is found.\nexport function findNearestQuantType(quant, availableQuants) {\n    // Create a map for quick index lookup from the defined order\n    const orderMap = new Map();\n    GGUF_QUANT_ORDER.forEach((q, index) => {\n        orderMap.set(q, index);\n    });\n    const targetIndex = orderMap.get(quant) ?? 0; // the 0 case should never happen\n    // Filter the available quantizations to include only those defined in the order map,\n    // then sort them according to the GGUF_QUANT_ORDER (from largest/index 0 to smallest/highest index).\n    const sortedAvailable = availableQuants\n        .filter((q) => orderMap.has(q))\n        .sort((a, b) => (orderMap.get(a) ?? Infinity) - (orderMap.get(b) ?? Infinity));\n    // If no valid quantizations are available after filtering\n    if (sortedAvailable.length === 0) {\n        return undefined;\n    }\n    // Iterate through the sorted available quantizations (largest to smallest).\n    // Find the first one whose order index is >= the target index.\n    // This means finding the largest quantization that is smaller than or equal to the target.\n    for (const availableQuant of sortedAvailable) {\n        // We know the key exists due to the filter above.\n        const availableIndex = orderMap.get(availableQuant) ?? 0;\n        if (availableIndex >= targetIndex) {\n            return availableQuant;\n        }\n    }\n    // If the loop completes, it means all available quantizations are larger (have a smaller index)\n    // than the target quantization. In this case, return the \"smallest\" available quantization,\n    // which is the last element in the sorted list (highest index among available).\n    return sortedAvailable[sortedAvailable.length - 1];\n}\n// This list is only used to calculate the size of the model, NOT to be confused with the quantization FILE type\nexport var GGMLQuantizationType;\n(function (GGMLQuantizationType) {\n    GGMLQuantizationType[GGMLQuantizationType[\"F32\"] = 0] = \"F32\";\n    GGMLQuantizationType[GGMLQuantizationType[\"F16\"] = 1] = \"F16\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q4_0\"] = 2] = \"Q4_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q4_1\"] = 3] = \"Q4_1\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q5_0\"] = 6] = \"Q5_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q5_1\"] = 7] = \"Q5_1\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q8_0\"] = 8] = \"Q8_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q8_1\"] = 9] = \"Q8_1\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q2_K\"] = 10] = \"Q2_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q3_K\"] = 11] = \"Q3_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q4_K\"] = 12] = \"Q4_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q5_K\"] = 13] = \"Q5_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q6_K\"] = 14] = \"Q6_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"Q8_K\"] = 15] = \"Q8_K\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ2_XXS\"] = 16] = \"IQ2_XXS\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ2_XS\"] = 17] = \"IQ2_XS\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ3_XXS\"] = 18] = \"IQ3_XXS\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ1_S\"] = 19] = \"IQ1_S\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ4_NL\"] = 20] = \"IQ4_NL\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ3_S\"] = 21] = \"IQ3_S\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ2_S\"] = 22] = \"IQ2_S\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ4_XS\"] = 23] = \"IQ4_XS\";\n    GGMLQuantizationType[GGMLQuantizationType[\"I8\"] = 24] = \"I8\";\n    GGMLQuantizationType[GGMLQuantizationType[\"I16\"] = 25] = \"I16\";\n    GGMLQuantizationType[GGMLQuantizationType[\"I32\"] = 26] = \"I32\";\n    GGMLQuantizationType[GGMLQuantizationType[\"I64\"] = 27] = \"I64\";\n    GGMLQuantizationType[GGMLQuantizationType[\"F64\"] = 28] = \"F64\";\n    GGMLQuantizationType[GGMLQuantizationType[\"IQ1_M\"] = 29] = \"IQ1_M\";\n    GGMLQuantizationType[GGMLQuantizationType[\"BF16\"] = 30] = \"BF16\";\n    GGMLQuantizationType[GGMLQuantizationType[\"TQ1_0\"] = 34] = \"TQ1_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"TQ2_0\"] = 35] = \"TQ2_0\";\n    GGMLQuantizationType[GGMLQuantizationType[\"MXFP4\"] = 39] = \"MXFP4\";\n})(GGMLQuantizationType || (GGMLQuantizationType = {}));\n"],"mappings":"AAAA;AACA;AACA;AACA,OAAO,IAAIA,wBAAwB;AACnC,CAAC,UAAUA,wBAAwB,EAAE;EACjCA,wBAAwB,CAACA,wBAAwB,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,GAAG,KAAK;EACrEA,wBAAwB,CAACA,wBAAwB,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,GAAG,KAAK;EACrEA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EACvEA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EACvEA,wBAAwB,CAACA,wBAAwB,CAAC,eAAe,CAAC,GAAG,CAAC,CAAC,GAAG,eAAe;EACzFA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EACvEA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EACvEA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EACvEA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EACvEA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EACvEA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EACxEA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EACxEA,wBAAwB,CAACA,wBAAwB,CAAC,SAAS,CAAC,GAAG,EAAE,CAAC,GAAG,SAAS;EAC9EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,SAAS,CAAC,GAAG,EAAE,CAAC,GAAG,SAAS;EAC9EA,wBAAwB,CAACA,wBAAwB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAC1EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAC1EA,wBAAwB,CAACA,wBAAwB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAC1EA,wBAAwB,CAACA,wBAAwB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAC1EA,wBAAwB,CAACA,wBAAwB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAC1EA,wBAAwB,CAACA,wBAAwB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EAC5EA,wBAAwB,CAACA,wBAAwB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAC1EA,wBAAwB,CAACA,wBAAwB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EACxEA,wBAAwB,CAACA,wBAAwB,CAAC,UAAU,CAAC,GAAG,EAAE,CAAC,GAAG,UAAU;EAChFA,wBAAwB,CAACA,wBAAwB,CAAC,UAAU,CAAC,GAAG,EAAE,CAAC,GAAG,UAAU;EAChFA,wBAAwB,CAACA,wBAAwB,CAAC,UAAU,CAAC,GAAG,EAAE,CAAC,GAAG,UAAU;EAChFA,wBAAwB,CAACA,wBAAwB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAC1EA,wBAAwB,CAACA,wBAAwB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAC1EA,wBAAwB,CAACA,wBAAwB,CAAC,WAAW,CAAC,GAAG,EAAE,CAAC,GAAG,WAAW;EAClF;EACA;EACAA,wBAAwB,CAACA,wBAAwB,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,GAAG,SAAS;EAChFA,wBAAwB,CAACA,wBAAwB,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,GAAG,SAAS;EAChFA,wBAAwB,CAACA,wBAAwB,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,GAAG,SAAS;EAChFA,wBAAwB,CAACA,wBAAwB,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,GAAG,SAAS;EAChFA,wBAAwB,CAACA,wBAAwB,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,GAAG,SAAS;EAChFA,wBAAwB,CAACA,wBAAwB,CAAC,SAAS,CAAC,GAAG,IAAI,CAAC,GAAG,SAAS;AACpF,CAAC,EAAEA,wBAAwB,KAAKA,wBAAwB,GAAG,CAAC,CAAC,CAAC,CAAC;AAC/D,MAAMC,UAAU,GAAGC,MAAM,CAACC,MAAM,CAACH,wBAAwB,CAAC,CAACI,MAAM,CAAEC,CAAC,IAAK,OAAOA,CAAC,KAAK,QAAQ,CAAC;AAC/F,OAAO,MAAMC,aAAa,GAAG,IAAIC,MAAM,CAAC,YAAYN,UAAU,CAACO,IAAI,CAAC,GAAG,CAAC,GAAG,GAAG,8BAA8B,CAAC;AAC7G,OAAO,MAAMC,oBAAoB,GAAG,IAAIF,MAAM,CAACD,aAAa,EAAE,GAAG,CAAC;AAClE,OAAO,SAASI,mBAAmBA,CAACC,KAAK,EAAE;EACvC,MAAMC,UAAU,GAAGD,KAAK,CAACE,WAAW,CAAC,CAAC,CAACC,KAAK,CAACL,oBAAoB,CAAC,EAAEM,EAAE,CAAC,CAAC,CAAC,CAAC,CAAC,CAAC;EAC5E,OAAOH,UAAU;AACrB;AACA;AACA;AACA;AACA,OAAO,MAAMI,gBAAgB,GAAG,CAC5BhB,wBAAwB,CAACiB,GAAG,EAC5BjB,wBAAwB,CAACkB,IAAI,EAC7BlB,wBAAwB,CAACmB,GAAG,EAC5BnB,wBAAwB,CAACoB,OAAO,EAChCpB,wBAAwB,CAACqB,IAAI;AAC7B;AACArB,wBAAwB,CAACsB,OAAO,EAChCtB,wBAAwB,CAACuB,IAAI;AAC7B;AACAvB,wBAAwB,CAACwB,OAAO,EAChCxB,wBAAwB,CAACyB,MAAM,EAC/BzB,wBAAwB,CAAC0B,MAAM,EAC/B1B,wBAAwB,CAAC2B,IAAI,EAC7B3B,wBAAwB,CAAC4B,IAAI;AAC7B;AACA5B,wBAAwB,CAAC6B,OAAO,EAChC7B,wBAAwB,CAAC8B,MAAM,EAC/B9B,wBAAwB,CAAC+B,MAAM,EAC/B/B,wBAAwB,CAACgC,MAAM,EAC/BhC,wBAAwB,CAACiC,MAAM,EAC/BjC,wBAAwB,CAACkC,QAAQ,EACjClC,wBAAwB,CAACmC,QAAQ,EACjCnC,wBAAwB,CAACoC,QAAQ,EACjCpC,wBAAwB,CAACqC,aAAa,EACtCrC,wBAAwB,CAACsC,IAAI,EAC7BtC,wBAAwB,CAACuC,IAAI,EAC7BvC,wBAAwB,CAACwC,IAAI,EAC7BxC,wBAAwB,CAACyC,IAAI,EAC7BzC,wBAAwB,CAAC0C,SAAS;AAClC;AACA1C,wBAAwB,CAAC2C,OAAO,EAChC3C,wBAAwB,CAAC4C,MAAM,EAC/B5C,wBAAwB,CAAC6C,MAAM,EAC/B7C,wBAAwB,CAAC8C,MAAM,EAC/B9C,wBAAwB,CAAC+C,KAAK,EAC9B/C,wBAAwB,CAACgD,KAAK,EAC9BhD,wBAAwB,CAACiD,MAAM,EAC/BjD,wBAAwB,CAACkD,OAAO;AAChC;AACAlD,wBAAwB,CAACmD,OAAO,EAChCnD,wBAAwB,CAACoD,IAAI,EAC7BpD,wBAAwB,CAACqD,MAAM,EAC/BrD,wBAAwB,CAACsD,KAAK,EAC9BtD,wBAAwB,CAACuD,KAAK,EAC9BvD,wBAAwB,CAACwD,MAAM,EAC/BxD,wBAAwB,CAACyD,OAAO;AAChC;AACAzD,wBAAwB,CAAC0D,KAAK,EAC9B1D,wBAAwB,CAAC2D,KAAK,EAC9B3D,wBAAwB,CAAC4D,KAAK,EAC9B5D,wBAAwB,CAAC6D,KAAK,CACjC;AACD;AACA;AACA,OAAO,SAASC,oBAAoBA,CAACC,KAAK,EAAEC,eAAe,EAAE;EACzD;EACA,MAAMC,QAAQ,GAAG,IAAIC,GAAG,CAAC,CAAC;EAC1BlD,gBAAgB,CAACmD,OAAO,CAAC,CAACC,CAAC,EAAEC,KAAK,KAAK;IACnCJ,QAAQ,CAACK,GAAG,CAACF,CAAC,EAAEC,KAAK,CAAC;EAC1B,CAAC,CAAC;EACF,MAAME,WAAW,GAAGN,QAAQ,CAACO,GAAG,CAACT,KAAK,CAAC,IAAI,CAAC,CAAC,CAAC;EAC9C;EACA;EACA,MAAMU,eAAe,GAAGT,eAAe,CAClC5D,MAAM,CAAEgE,CAAC,IAAKH,QAAQ,CAACS,GAAG,CAACN,CAAC,CAAC,CAAC,CAC9BO,IAAI,CAAC,CAACC,CAAC,EAAEC,CAAC,KAAK,CAACZ,QAAQ,CAACO,GAAG,CAACI,CAAC,CAAC,IAAIE,QAAQ,KAAKb,QAAQ,CAACO,GAAG,CAACK,CAAC,CAAC,IAAIC,QAAQ,CAAC,CAAC;EAClF;EACA,IAAIL,eAAe,CAACM,MAAM,KAAK,CAAC,EAAE;IAC9B,OAAOC,SAAS;EACpB;EACA;EACA;EACA;EACA,KAAK,MAAMC,cAAc,IAAIR,eAAe,EAAE;IAC1C;IACA,MAAMS,cAAc,GAAGjB,QAAQ,CAACO,GAAG,CAACS,cAAc,CAAC,IAAI,CAAC;IACxD,IAAIC,cAAc,IAAIX,WAAW,EAAE;MAC/B,OAAOU,cAAc;IACzB;EACJ;EACA;EACA;EACA;EACA,OAAOR,eAAe,CAACA,eAAe,CAACM,MAAM,GAAG,CAAC,CAAC;AACtD;AACA;AACA,OAAO,IAAII,oBAAoB;AAC/B,CAAC,UAAUA,oBAAoB,EAAE;EAC7BA,oBAAoB,CAACA,oBAAoB,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,GAAG,KAAK;EAC7DA,oBAAoB,CAACA,oBAAoB,CAAC,KAAK,CAAC,GAAG,CAAC,CAAC,GAAG,KAAK;EAC7DA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EAC/DA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EAC/DA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EAC/DA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EAC/DA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EAC/DA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,CAAC,CAAC,GAAG,MAAM;EAC/DA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EAChEA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EAChEA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EAChEA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EAChEA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EAChEA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EAChEA,oBAAoB,CAACA,oBAAoB,CAAC,SAAS,CAAC,GAAG,EAAE,CAAC,GAAG,SAAS;EACtEA,oBAAoB,CAACA,oBAAoB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EACpEA,oBAAoB,CAACA,oBAAoB,CAAC,SAAS,CAAC,GAAG,EAAE,CAAC,GAAG,SAAS;EACtEA,oBAAoB,CAACA,oBAAoB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAClEA,oBAAoB,CAACA,oBAAoB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EACpEA,oBAAoB,CAACA,oBAAoB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAClEA,oBAAoB,CAACA,oBAAoB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAClEA,oBAAoB,CAACA,oBAAoB,CAAC,QAAQ,CAAC,GAAG,EAAE,CAAC,GAAG,QAAQ;EACpEA,oBAAoB,CAACA,oBAAoB,CAAC,IAAI,CAAC,GAAG,EAAE,CAAC,GAAG,IAAI;EAC5DA,oBAAoB,CAACA,oBAAoB,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC,GAAG,KAAK;EAC9DA,oBAAoB,CAACA,oBAAoB,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC,GAAG,KAAK;EAC9DA,oBAAoB,CAACA,oBAAoB,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC,GAAG,KAAK;EAC9DA,oBAAoB,CAACA,oBAAoB,CAAC,KAAK,CAAC,GAAG,EAAE,CAAC,GAAG,KAAK;EAC9DA,oBAAoB,CAACA,oBAAoB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAClEA,oBAAoB,CAACA,oBAAoB,CAAC,MAAM,CAAC,GAAG,EAAE,CAAC,GAAG,MAAM;EAChEA,oBAAoB,CAACA,oBAAoB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAClEA,oBAAoB,CAACA,oBAAoB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;EAClEA,oBAAoB,CAACA,oBAAoB,CAAC,OAAO,CAAC,GAAG,EAAE,CAAC,GAAG,OAAO;AACtE,CAAC,EAAEA,oBAAoB,KAAKA,oBAAoB,GAAG,CAAC,CAAC,CAAC,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}