{"ast":null,"code":"import { HF_HUB_URL } from \"../config.js\";\nimport { HARDCODED_MODEL_INFERENCE_MAPPING } from \"../providers/consts.js\";\nimport { EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS } from \"../providers/hf-inference.js\";\nimport { typedInclude } from \"../utils/typedInclude.js\";\nimport { InferenceClientHubApiError, InferenceClientInputError } from \"../errors.js\";\nimport { getLogger } from \"./logger.js\";\nexport const inferenceProviderMappingCache = new Map();\n/**\n * Normalize inferenceProviderMapping to always return an array format.\n * This provides backward and forward compatibility for the API changes.\n *\n * Vendored from @huggingface/hub to avoid extra dependency.\n */\nfunction normalizeInferenceProviderMapping(modelId, inferenceProviderMapping) {\n  if (!inferenceProviderMapping) {\n    return [];\n  }\n  // If it's already an array, return it as is\n  if (Array.isArray(inferenceProviderMapping)) {\n    return inferenceProviderMapping;\n  }\n  // Convert mapping to array format\n  return Object.entries(inferenceProviderMapping).map(([provider, mapping]) => ({\n    provider,\n    hfModelId: modelId,\n    providerId: mapping.providerId,\n    status: mapping.status,\n    task: mapping.task,\n    adapter: mapping.adapter,\n    adapterWeightsPath: mapping.adapterWeightsPath\n  }));\n}\nexport async function fetchInferenceProviderMappingForModel(modelId, accessToken, options) {\n  let inferenceProviderMapping;\n  if (inferenceProviderMappingCache.has(modelId)) {\n    // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n    inferenceProviderMapping = inferenceProviderMappingCache.get(modelId);\n  } else {\n    const url = `${HF_HUB_URL}/api/models/${modelId}?expand[]=inferenceProviderMapping`;\n    const resp = await (options?.fetch ?? fetch)(url, {\n      headers: accessToken?.startsWith(\"hf_\") ? {\n        Authorization: `Bearer ${accessToken}`\n      } : {}\n    });\n    if (!resp.ok) {\n      if (resp.headers.get(\"Content-Type\")?.startsWith(\"application/json\")) {\n        const error = await resp.json();\n        if (\"error\" in error && typeof error.error === \"string\") {\n          throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}: ${error.error}`, {\n            url,\n            method: \"GET\"\n          }, {\n            requestId: resp.headers.get(\"x-request-id\") ?? \"\",\n            status: resp.status,\n            body: error\n          });\n        }\n      } else {\n        throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}`, {\n          url,\n          method: \"GET\"\n        }, {\n          requestId: resp.headers.get(\"x-request-id\") ?? \"\",\n          status: resp.status,\n          body: await resp.text()\n        });\n      }\n    }\n    let payload = null;\n    try {\n      payload = await resp.json();\n    } catch {\n      throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}: malformed API response, invalid JSON`, {\n        url,\n        method: \"GET\"\n      }, {\n        requestId: resp.headers.get(\"x-request-id\") ?? \"\",\n        status: resp.status,\n        body: await resp.text()\n      });\n    }\n    if (!payload?.inferenceProviderMapping) {\n      throw new InferenceClientHubApiError(`We have not been able to find inference provider information for model ${modelId}.`, {\n        url,\n        method: \"GET\"\n      }, {\n        requestId: resp.headers.get(\"x-request-id\") ?? \"\",\n        status: resp.status,\n        body: await resp.text()\n      });\n    }\n    inferenceProviderMapping = normalizeInferenceProviderMapping(modelId, payload.inferenceProviderMapping);\n    inferenceProviderMappingCache.set(modelId, inferenceProviderMapping);\n  }\n  return inferenceProviderMapping;\n}\nexport async function getInferenceProviderMapping(params, options) {\n  const logger = getLogger();\n  if (params.provider === \"auto\" && params.task === \"conversational\") {\n    // Special case for auto + conversational to avoid extra API calls\n    // Call directly the server-side auto router\n    return {\n      hfModelId: params.modelId,\n      provider: \"auto\",\n      providerId: params.modelId,\n      status: \"live\",\n      task: \"conversational\"\n    };\n  }\n  if (HARDCODED_MODEL_INFERENCE_MAPPING[params.provider][params.modelId]) {\n    return HARDCODED_MODEL_INFERENCE_MAPPING[params.provider][params.modelId];\n  }\n  const mappings = await fetchInferenceProviderMappingForModel(params.modelId, params.accessToken, options);\n  const providerMapping = mappings.find(mapping => mapping.provider === params.provider);\n  if (providerMapping) {\n    const equivalentTasks = params.provider === \"hf-inference\" && typedInclude(EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS, params.task) ? EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS : [params.task];\n    if (!typedInclude(equivalentTasks, providerMapping.task)) {\n      throw new InferenceClientInputError(`Model ${params.modelId} is not supported for task ${params.task} and provider ${params.provider}. Supported task: ${providerMapping.task}.`);\n    }\n    if (providerMapping.status === \"staging\") {\n      logger.warn(`Model ${params.modelId} is in staging mode for provider ${params.provider}. Meant for test purposes only.`);\n    }\n    return providerMapping;\n  }\n  return null;\n}\nexport async function resolveProvider(provider, modelId, endpointUrl) {\n  const logger = getLogger();\n  if (endpointUrl) {\n    if (provider) {\n      throw new InferenceClientInputError(\"Specifying both endpointUrl and provider is not supported.\");\n    }\n    /// Defaulting to hf-inference helpers / API\n    return \"hf-inference\";\n  }\n  if (!provider) {\n    logger.log(\"Defaulting to 'auto' which will select the first provider available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.\");\n    provider = \"auto\";\n  }\n  if (provider === \"auto\") {\n    if (!modelId) {\n      throw new InferenceClientInputError(\"Specifying a model is required when provider is 'auto'\");\n    }\n    const mappings = await fetchInferenceProviderMappingForModel(modelId);\n    provider = mappings[0]?.provider;\n    logger.log(\"Auto selected provider:\", provider);\n  }\n  if (!provider) {\n    throw new InferenceClientInputError(`No Inference Provider available for model ${modelId}.`);\n  }\n  return provider;\n}","map":{"version":3,"names":["HF_HUB_URL","HARDCODED_MODEL_INFERENCE_MAPPING","EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS","typedInclude","InferenceClientHubApiError","InferenceClientInputError","getLogger","inferenceProviderMappingCache","Map","normalizeInferenceProviderMapping","modelId","inferenceProviderMapping","Array","isArray","Object","entries","map","provider","mapping","hfModelId","providerId","status","task","adapter","adapterWeightsPath","fetchInferenceProviderMappingForModel","accessToken","options","has","get","url","resp","fetch","headers","startsWith","Authorization","ok","error","json","method","requestId","body","text","payload","set","getInferenceProviderMapping","params","logger","mappings","providerMapping","find","equivalentTasks","warn","resolveProvider","endpointUrl","log"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/inference/dist/esm/lib/getInferenceProviderMapping.js"],"sourcesContent":["import { HF_HUB_URL } from \"../config.js\";\nimport { HARDCODED_MODEL_INFERENCE_MAPPING } from \"../providers/consts.js\";\nimport { EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS } from \"../providers/hf-inference.js\";\nimport { typedInclude } from \"../utils/typedInclude.js\";\nimport { InferenceClientHubApiError, InferenceClientInputError } from \"../errors.js\";\nimport { getLogger } from \"./logger.js\";\nexport const inferenceProviderMappingCache = new Map();\n/**\n * Normalize inferenceProviderMapping to always return an array format.\n * This provides backward and forward compatibility for the API changes.\n *\n * Vendored from @huggingface/hub to avoid extra dependency.\n */\nfunction normalizeInferenceProviderMapping(modelId, inferenceProviderMapping) {\n    if (!inferenceProviderMapping) {\n        return [];\n    }\n    // If it's already an array, return it as is\n    if (Array.isArray(inferenceProviderMapping)) {\n        return inferenceProviderMapping;\n    }\n    // Convert mapping to array format\n    return Object.entries(inferenceProviderMapping).map(([provider, mapping]) => ({\n        provider,\n        hfModelId: modelId,\n        providerId: mapping.providerId,\n        status: mapping.status,\n        task: mapping.task,\n        adapter: mapping.adapter,\n        adapterWeightsPath: mapping.adapterWeightsPath,\n    }));\n}\nexport async function fetchInferenceProviderMappingForModel(modelId, accessToken, options) {\n    let inferenceProviderMapping;\n    if (inferenceProviderMappingCache.has(modelId)) {\n        // eslint-disable-next-line @typescript-eslint/no-non-null-assertion\n        inferenceProviderMapping = inferenceProviderMappingCache.get(modelId);\n    }\n    else {\n        const url = `${HF_HUB_URL}/api/models/${modelId}?expand[]=inferenceProviderMapping`;\n        const resp = await (options?.fetch ?? fetch)(url, {\n            headers: accessToken?.startsWith(\"hf_\") ? { Authorization: `Bearer ${accessToken}` } : {},\n        });\n        if (!resp.ok) {\n            if (resp.headers.get(\"Content-Type\")?.startsWith(\"application/json\")) {\n                const error = await resp.json();\n                if (\"error\" in error && typeof error.error === \"string\") {\n                    throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}: ${error.error}`, { url, method: \"GET\" }, { requestId: resp.headers.get(\"x-request-id\") ?? \"\", status: resp.status, body: error });\n                }\n            }\n            else {\n                throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}`, { url, method: \"GET\" }, { requestId: resp.headers.get(\"x-request-id\") ?? \"\", status: resp.status, body: await resp.text() });\n            }\n        }\n        let payload = null;\n        try {\n            payload = await resp.json();\n        }\n        catch {\n            throw new InferenceClientHubApiError(`Failed to fetch inference provider mapping for model ${modelId}: malformed API response, invalid JSON`, { url, method: \"GET\" }, { requestId: resp.headers.get(\"x-request-id\") ?? \"\", status: resp.status, body: await resp.text() });\n        }\n        if (!payload?.inferenceProviderMapping) {\n            throw new InferenceClientHubApiError(`We have not been able to find inference provider information for model ${modelId}.`, { url, method: \"GET\" }, { requestId: resp.headers.get(\"x-request-id\") ?? \"\", status: resp.status, body: await resp.text() });\n        }\n        inferenceProviderMapping = normalizeInferenceProviderMapping(modelId, payload.inferenceProviderMapping);\n        inferenceProviderMappingCache.set(modelId, inferenceProviderMapping);\n    }\n    return inferenceProviderMapping;\n}\nexport async function getInferenceProviderMapping(params, options) {\n    const logger = getLogger();\n    if (params.provider === \"auto\" && params.task === \"conversational\") {\n        // Special case for auto + conversational to avoid extra API calls\n        // Call directly the server-side auto router\n        return {\n            hfModelId: params.modelId,\n            provider: \"auto\",\n            providerId: params.modelId,\n            status: \"live\",\n            task: \"conversational\",\n        };\n    }\n    if (HARDCODED_MODEL_INFERENCE_MAPPING[params.provider][params.modelId]) {\n        return HARDCODED_MODEL_INFERENCE_MAPPING[params.provider][params.modelId];\n    }\n    const mappings = await fetchInferenceProviderMappingForModel(params.modelId, params.accessToken, options);\n    const providerMapping = mappings.find((mapping) => mapping.provider === params.provider);\n    if (providerMapping) {\n        const equivalentTasks = params.provider === \"hf-inference\" && typedInclude(EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS, params.task)\n            ? EQUIVALENT_SENTENCE_TRANSFORMERS_TASKS\n            : [params.task];\n        if (!typedInclude(equivalentTasks, providerMapping.task)) {\n            throw new InferenceClientInputError(`Model ${params.modelId} is not supported for task ${params.task} and provider ${params.provider}. Supported task: ${providerMapping.task}.`);\n        }\n        if (providerMapping.status === \"staging\") {\n            logger.warn(`Model ${params.modelId} is in staging mode for provider ${params.provider}. Meant for test purposes only.`);\n        }\n        return providerMapping;\n    }\n    return null;\n}\nexport async function resolveProvider(provider, modelId, endpointUrl) {\n    const logger = getLogger();\n    if (endpointUrl) {\n        if (provider) {\n            throw new InferenceClientInputError(\"Specifying both endpointUrl and provider is not supported.\");\n        }\n        /// Defaulting to hf-inference helpers / API\n        return \"hf-inference\";\n    }\n    if (!provider) {\n        logger.log(\"Defaulting to 'auto' which will select the first provider available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.\");\n        provider = \"auto\";\n    }\n    if (provider === \"auto\") {\n        if (!modelId) {\n            throw new InferenceClientInputError(\"Specifying a model is required when provider is 'auto'\");\n        }\n        const mappings = await fetchInferenceProviderMappingForModel(modelId);\n        provider = mappings[0]?.provider;\n        logger.log(\"Auto selected provider:\", provider);\n    }\n    if (!provider) {\n        throw new InferenceClientInputError(`No Inference Provider available for model ${modelId}.`);\n    }\n    return provider;\n}\n"],"mappings":"AAAA,SAASA,UAAU,QAAQ,cAAc;AACzC,SAASC,iCAAiC,QAAQ,wBAAwB;AAC1E,SAASC,sCAAsC,QAAQ,8BAA8B;AACrF,SAASC,YAAY,QAAQ,0BAA0B;AACvD,SAASC,0BAA0B,EAAEC,yBAAyB,QAAQ,cAAc;AACpF,SAASC,SAAS,QAAQ,aAAa;AACvC,OAAO,MAAMC,6BAA6B,GAAG,IAAIC,GAAG,CAAC,CAAC;AACtD;AACA;AACA;AACA;AACA;AACA;AACA,SAASC,iCAAiCA,CAACC,OAAO,EAAEC,wBAAwB,EAAE;EAC1E,IAAI,CAACA,wBAAwB,EAAE;IAC3B,OAAO,EAAE;EACb;EACA;EACA,IAAIC,KAAK,CAACC,OAAO,CAACF,wBAAwB,CAAC,EAAE;IACzC,OAAOA,wBAAwB;EACnC;EACA;EACA,OAAOG,MAAM,CAACC,OAAO,CAACJ,wBAAwB,CAAC,CAACK,GAAG,CAAC,CAAC,CAACC,QAAQ,EAAEC,OAAO,CAAC,MAAM;IAC1ED,QAAQ;IACRE,SAAS,EAAET,OAAO;IAClBU,UAAU,EAAEF,OAAO,CAACE,UAAU;IAC9BC,MAAM,EAAEH,OAAO,CAACG,MAAM;IACtBC,IAAI,EAAEJ,OAAO,CAACI,IAAI;IAClBC,OAAO,EAAEL,OAAO,CAACK,OAAO;IACxBC,kBAAkB,EAAEN,OAAO,CAACM;EAChC,CAAC,CAAC,CAAC;AACP;AACA,OAAO,eAAeC,qCAAqCA,CAACf,OAAO,EAAEgB,WAAW,EAAEC,OAAO,EAAE;EACvF,IAAIhB,wBAAwB;EAC5B,IAAIJ,6BAA6B,CAACqB,GAAG,CAAClB,OAAO,CAAC,EAAE;IAC5C;IACAC,wBAAwB,GAAGJ,6BAA6B,CAACsB,GAAG,CAACnB,OAAO,CAAC;EACzE,CAAC,MACI;IACD,MAAMoB,GAAG,GAAG,GAAG9B,UAAU,eAAeU,OAAO,oCAAoC;IACnF,MAAMqB,IAAI,GAAG,MAAM,CAACJ,OAAO,EAAEK,KAAK,IAAIA,KAAK,EAAEF,GAAG,EAAE;MAC9CG,OAAO,EAAEP,WAAW,EAAEQ,UAAU,CAAC,KAAK,CAAC,GAAG;QAAEC,aAAa,EAAE,UAAUT,WAAW;MAAG,CAAC,GAAG,CAAC;IAC5F,CAAC,CAAC;IACF,IAAI,CAACK,IAAI,CAACK,EAAE,EAAE;MACV,IAAIL,IAAI,CAACE,OAAO,CAACJ,GAAG,CAAC,cAAc,CAAC,EAAEK,UAAU,CAAC,kBAAkB,CAAC,EAAE;QAClE,MAAMG,KAAK,GAAG,MAAMN,IAAI,CAACO,IAAI,CAAC,CAAC;QAC/B,IAAI,OAAO,IAAID,KAAK,IAAI,OAAOA,KAAK,CAACA,KAAK,KAAK,QAAQ,EAAE;UACrD,MAAM,IAAIjC,0BAA0B,CAAC,wDAAwDM,OAAO,KAAK2B,KAAK,CAACA,KAAK,EAAE,EAAE;YAAEP,GAAG;YAAES,MAAM,EAAE;UAAM,CAAC,EAAE;YAAEC,SAAS,EAAET,IAAI,CAACE,OAAO,CAACJ,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;YAAER,MAAM,EAAEU,IAAI,CAACV,MAAM;YAAEoB,IAAI,EAAEJ;UAAM,CAAC,CAAC;QAC5O;MACJ,CAAC,MACI;QACD,MAAM,IAAIjC,0BAA0B,CAAC,wDAAwDM,OAAO,EAAE,EAAE;UAAEoB,GAAG;UAAES,MAAM,EAAE;QAAM,CAAC,EAAE;UAAEC,SAAS,EAAET,IAAI,CAACE,OAAO,CAACJ,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;UAAER,MAAM,EAAEU,IAAI,CAACV,MAAM;UAAEoB,IAAI,EAAE,MAAMV,IAAI,CAACW,IAAI,CAAC;QAAE,CAAC,CAAC;MACxO;IACJ;IACA,IAAIC,OAAO,GAAG,IAAI;IAClB,IAAI;MACAA,OAAO,GAAG,MAAMZ,IAAI,CAACO,IAAI,CAAC,CAAC;IAC/B,CAAC,CACD,MAAM;MACF,MAAM,IAAIlC,0BAA0B,CAAC,wDAAwDM,OAAO,wCAAwC,EAAE;QAAEoB,GAAG;QAAES,MAAM,EAAE;MAAM,CAAC,EAAE;QAAEC,SAAS,EAAET,IAAI,CAACE,OAAO,CAACJ,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;QAAER,MAAM,EAAEU,IAAI,CAACV,MAAM;QAAEoB,IAAI,EAAE,MAAMV,IAAI,CAACW,IAAI,CAAC;MAAE,CAAC,CAAC;IAC9Q;IACA,IAAI,CAACC,OAAO,EAAEhC,wBAAwB,EAAE;MACpC,MAAM,IAAIP,0BAA0B,CAAC,0EAA0EM,OAAO,GAAG,EAAE;QAAEoB,GAAG;QAAES,MAAM,EAAE;MAAM,CAAC,EAAE;QAAEC,SAAS,EAAET,IAAI,CAACE,OAAO,CAACJ,GAAG,CAAC,cAAc,CAAC,IAAI,EAAE;QAAER,MAAM,EAAEU,IAAI,CAACV,MAAM;QAAEoB,IAAI,EAAE,MAAMV,IAAI,CAACW,IAAI,CAAC;MAAE,CAAC,CAAC;IAC3P;IACA/B,wBAAwB,GAAGF,iCAAiC,CAACC,OAAO,EAAEiC,OAAO,CAAChC,wBAAwB,CAAC;IACvGJ,6BAA6B,CAACqC,GAAG,CAAClC,OAAO,EAAEC,wBAAwB,CAAC;EACxE;EACA,OAAOA,wBAAwB;AACnC;AACA,OAAO,eAAekC,2BAA2BA,CAACC,MAAM,EAAEnB,OAAO,EAAE;EAC/D,MAAMoB,MAAM,GAAGzC,SAAS,CAAC,CAAC;EAC1B,IAAIwC,MAAM,CAAC7B,QAAQ,KAAK,MAAM,IAAI6B,MAAM,CAACxB,IAAI,KAAK,gBAAgB,EAAE;IAChE;IACA;IACA,OAAO;MACHH,SAAS,EAAE2B,MAAM,CAACpC,OAAO;MACzBO,QAAQ,EAAE,MAAM;MAChBG,UAAU,EAAE0B,MAAM,CAACpC,OAAO;MAC1BW,MAAM,EAAE,MAAM;MACdC,IAAI,EAAE;IACV,CAAC;EACL;EACA,IAAIrB,iCAAiC,CAAC6C,MAAM,CAAC7B,QAAQ,CAAC,CAAC6B,MAAM,CAACpC,OAAO,CAAC,EAAE;IACpE,OAAOT,iCAAiC,CAAC6C,MAAM,CAAC7B,QAAQ,CAAC,CAAC6B,MAAM,CAACpC,OAAO,CAAC;EAC7E;EACA,MAAMsC,QAAQ,GAAG,MAAMvB,qCAAqC,CAACqB,MAAM,CAACpC,OAAO,EAAEoC,MAAM,CAACpB,WAAW,EAAEC,OAAO,CAAC;EACzG,MAAMsB,eAAe,GAAGD,QAAQ,CAACE,IAAI,CAAEhC,OAAO,IAAKA,OAAO,CAACD,QAAQ,KAAK6B,MAAM,CAAC7B,QAAQ,CAAC;EACxF,IAAIgC,eAAe,EAAE;IACjB,MAAME,eAAe,GAAGL,MAAM,CAAC7B,QAAQ,KAAK,cAAc,IAAId,YAAY,CAACD,sCAAsC,EAAE4C,MAAM,CAACxB,IAAI,CAAC,GACzHpB,sCAAsC,GACtC,CAAC4C,MAAM,CAACxB,IAAI,CAAC;IACnB,IAAI,CAACnB,YAAY,CAACgD,eAAe,EAAEF,eAAe,CAAC3B,IAAI,CAAC,EAAE;MACtD,MAAM,IAAIjB,yBAAyB,CAAC,SAASyC,MAAM,CAACpC,OAAO,8BAA8BoC,MAAM,CAACxB,IAAI,iBAAiBwB,MAAM,CAAC7B,QAAQ,qBAAqBgC,eAAe,CAAC3B,IAAI,GAAG,CAAC;IACrL;IACA,IAAI2B,eAAe,CAAC5B,MAAM,KAAK,SAAS,EAAE;MACtC0B,MAAM,CAACK,IAAI,CAAC,SAASN,MAAM,CAACpC,OAAO,oCAAoCoC,MAAM,CAAC7B,QAAQ,iCAAiC,CAAC;IAC5H;IACA,OAAOgC,eAAe;EAC1B;EACA,OAAO,IAAI;AACf;AACA,OAAO,eAAeI,eAAeA,CAACpC,QAAQ,EAAEP,OAAO,EAAE4C,WAAW,EAAE;EAClE,MAAMP,MAAM,GAAGzC,SAAS,CAAC,CAAC;EAC1B,IAAIgD,WAAW,EAAE;IACb,IAAIrC,QAAQ,EAAE;MACV,MAAM,IAAIZ,yBAAyB,CAAC,4DAA4D,CAAC;IACrG;IACA;IACA,OAAO,cAAc;EACzB;EACA,IAAI,CAACY,QAAQ,EAAE;IACX8B,MAAM,CAACQ,GAAG,CAAC,8JAA8J,CAAC;IAC1KtC,QAAQ,GAAG,MAAM;EACrB;EACA,IAAIA,QAAQ,KAAK,MAAM,EAAE;IACrB,IAAI,CAACP,OAAO,EAAE;MACV,MAAM,IAAIL,yBAAyB,CAAC,wDAAwD,CAAC;IACjG;IACA,MAAM2C,QAAQ,GAAG,MAAMvB,qCAAqC,CAACf,OAAO,CAAC;IACrEO,QAAQ,GAAG+B,QAAQ,CAAC,CAAC,CAAC,EAAE/B,QAAQ;IAChC8B,MAAM,CAACQ,GAAG,CAAC,yBAAyB,EAAEtC,QAAQ,CAAC;EACnD;EACA,IAAI,CAACA,QAAQ,EAAE;IACX,MAAM,IAAIZ,yBAAyB,CAAC,6CAA6CK,OAAO,GAAG,CAAC;EAChG;EACA,OAAOO,QAAQ;AACnB","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}