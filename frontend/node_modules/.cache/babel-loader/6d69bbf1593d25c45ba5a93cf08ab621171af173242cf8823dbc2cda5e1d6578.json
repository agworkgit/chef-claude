{"ast":null,"code":"const taskData = {\n  datasets: [{\n    description: \"Instructions composed of image and text.\",\n    id: \"liuhaotian/LLaVA-Instruct-150K\"\n  }, {\n    description: \"Collection of image-text pairs on scientific topics.\",\n    id: \"DAMO-NLP-SG/multimodal_textbook\"\n  }, {\n    description: \"A collection of datasets made for model fine-tuning.\",\n    id: \"HuggingFaceM4/the_cauldron\"\n  }, {\n    description: \"Screenshots of websites with their HTML/CSS codes.\",\n    id: \"HuggingFaceM4/WebSight\"\n  }],\n  demo: {\n    inputs: [{\n      filename: \"image-text-to-text-input.png\",\n      type: \"img\"\n    }, {\n      label: \"Text Prompt\",\n      content: \"Describe the position of the bee in detail.\",\n      type: \"text\"\n    }],\n    outputs: [{\n      label: \"Answer\",\n      content: \"The bee is sitting on a pink flower, surrounded by other flowers. The bee is positioned in the center of the flower, with its head and front legs sticking out.\",\n      type: \"text\"\n    }]\n  },\n  metrics: [],\n  models: [{\n    description: \"Small and efficient yet powerful vision language model.\",\n    id: \"HuggingFaceTB/SmolVLM-Instruct\"\n  }, {\n    description: \"Cutting-edge reasoning vision language model.\",\n    id: \"zai-org/GLM-4.5V\"\n  }, {\n    description: \"Cutting-edge small vision language model to convert documents to text.\",\n    id: \"rednote-hilab/dots.ocr\"\n  }, {\n    description: \"Small yet powerful model.\",\n    id: \"Qwen/Qwen2.5-VL-3B-Instruct\"\n  }, {\n    description: \"Image-text-to-text model with agentic capabilities.\",\n    id: \"microsoft/Magma-8B\"\n  }],\n  spaces: [{\n    description: \"Leaderboard to evaluate vision language models.\",\n    id: \"opencompass/open_vlm_leaderboard\"\n  }, {\n    description: \"An application that compares object detection capabilities of different vision language models.\",\n    id: \"sergiopaniego/vlm_object_understanding\"\n  }, {\n    description: \"An application to compare different OCR models.\",\n    id: \"prithivMLmods/Multimodal-OCR\"\n  }],\n  summary: \"Image-text-to-text models take in an image and text prompt and output text. These models are also called vision-language models, or VLMs. The difference from image-to-text models is that these models take an additional text input, not restricting the model to certain use cases like image captioning, and may also be trained to accept a conversation as input.\",\n  widgetModels: [\"zai-org/GLM-4.5V\"],\n  youtubeId: \"IoGaGfU1CIg\"\n};\nexport default taskData;","map":{"version":3,"names":["taskData","datasets","description","id","demo","inputs","filename","type","label","content","outputs","metrics","models","spaces","summary","widgetModels","youtubeId"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/tasks/image-text-to-text/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Instructions composed of image and text.\",\n            id: \"liuhaotian/LLaVA-Instruct-150K\",\n        },\n        {\n            description: \"Collection of image-text pairs on scientific topics.\",\n            id: \"DAMO-NLP-SG/multimodal_textbook\",\n        },\n        {\n            description: \"A collection of datasets made for model fine-tuning.\",\n            id: \"HuggingFaceM4/the_cauldron\",\n        },\n        {\n            description: \"Screenshots of websites with their HTML/CSS codes.\",\n            id: \"HuggingFaceM4/WebSight\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"image-text-to-text-input.png\",\n                type: \"img\",\n            },\n            {\n                label: \"Text Prompt\",\n                content: \"Describe the position of the bee in detail.\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Answer\",\n                content: \"The bee is sitting on a pink flower, surrounded by other flowers. The bee is positioned in the center of the flower, with its head and front legs sticking out.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"Small and efficient yet powerful vision language model.\",\n            id: \"HuggingFaceTB/SmolVLM-Instruct\",\n        },\n        {\n            description: \"Cutting-edge reasoning vision language model.\",\n            id: \"zai-org/GLM-4.5V\",\n        },\n        {\n            description: \"Cutting-edge small vision language model to convert documents to text.\",\n            id: \"rednote-hilab/dots.ocr\",\n        },\n        {\n            description: \"Small yet powerful model.\",\n            id: \"Qwen/Qwen2.5-VL-3B-Instruct\",\n        },\n        {\n            description: \"Image-text-to-text model with agentic capabilities.\",\n            id: \"microsoft/Magma-8B\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"Leaderboard to evaluate vision language models.\",\n            id: \"opencompass/open_vlm_leaderboard\",\n        },\n        {\n            description: \"An application that compares object detection capabilities of different vision language models.\",\n            id: \"sergiopaniego/vlm_object_understanding\",\n        },\n        {\n            description: \"An application to compare different OCR models.\",\n            id: \"prithivMLmods/Multimodal-OCR\",\n        },\n    ],\n    summary: \"Image-text-to-text models take in an image and text prompt and output text. These models are also called vision-language models, or VLMs. The difference from image-to-text models is that these models take an additional text input, not restricting the model to certain use cases like image captioning, and may also be trained to accept a conversation as input.\",\n    widgetModels: [\"zai-org/GLM-4.5V\"],\n    youtubeId: \"IoGaGfU1CIg\",\n};\nexport default taskData;\n"],"mappings":"AAAA,MAAMA,QAAQ,GAAG;EACbC,QAAQ,EAAE,CACN;IACIC,WAAW,EAAE,0CAA0C;IACvDC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,sDAAsD;IACnEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,sDAAsD;IACnEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,oDAAoD;IACjEC,EAAE,EAAE;EACR,CAAC,CACJ;EACDC,IAAI,EAAE;IACFC,MAAM,EAAE,CACJ;MACIC,QAAQ,EAAE,8BAA8B;MACxCC,IAAI,EAAE;IACV,CAAC,EACD;MACIC,KAAK,EAAE,aAAa;MACpBC,OAAO,EAAE,6CAA6C;MACtDF,IAAI,EAAE;IACV,CAAC,CACJ;IACDG,OAAO,EAAE,CACL;MACIF,KAAK,EAAE,QAAQ;MACfC,OAAO,EAAE,iKAAiK;MAC1KF,IAAI,EAAE;IACV,CAAC;EAET,CAAC;EACDI,OAAO,EAAE,EAAE;EACXC,MAAM,EAAE,CACJ;IACIV,WAAW,EAAE,yDAAyD;IACtEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,+CAA+C;IAC5DC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,wEAAwE;IACrFC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,2BAA2B;IACxCC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,qDAAqD;IAClEC,EAAE,EAAE;EACR,CAAC,CACJ;EACDU,MAAM,EAAE,CACJ;IACIX,WAAW,EAAE,iDAAiD;IAC9DC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,iGAAiG;IAC9GC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,iDAAiD;IAC9DC,EAAE,EAAE;EACR,CAAC,CACJ;EACDW,OAAO,EAAE,yWAAyW;EAClXC,YAAY,EAAE,CAAC,kBAAkB,CAAC;EAClCC,SAAS,EAAE;AACf,CAAC;AACD,eAAehB,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}