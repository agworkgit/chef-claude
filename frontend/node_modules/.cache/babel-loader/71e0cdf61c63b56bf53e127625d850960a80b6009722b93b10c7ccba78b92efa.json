{"ast":null,"code":"import { parseGGUFQuantLabel } from \"./gguf.js\";\nimport { stringifyMessages } from \"./snippets/common.js\";\nimport { getModelInputSnippet } from \"./snippets/inputs.js\";\nfunction isAwqModel(model) {\n  return model.config?.quantization_config?.quant_method === \"awq\";\n}\nfunction isGptqModel(model) {\n  return model.config?.quantization_config?.quant_method === \"gptq\";\n}\nfunction isAqlmModel(model) {\n  return model.config?.quantization_config?.quant_method === \"aqlm\";\n}\nfunction isMarlinModel(model) {\n  return model.config?.quantization_config?.quant_method === \"marlin\";\n}\nfunction isTransformersModel(model) {\n  return model.tags.includes(\"transformers\");\n}\nfunction isTgiModel(model) {\n  return model.tags.includes(\"text-generation-inference\");\n}\nfunction isLlamaCppGgufModel(model) {\n  return !!model.gguf?.context_length;\n}\nfunction isAmdRyzenModel(model) {\n  return model.tags.includes(\"ryzenai-hybrid\") || model.tags.includes(\"ryzenai-npu\");\n}\nfunction isMlxModel(model) {\n  return model.tags.includes(\"mlx\");\n}\nfunction getQuantTag(filepath) {\n  const defaultTag = \":{{QUANT_TAG}}\";\n  if (!filepath) {\n    return defaultTag;\n  }\n  const quantLabel = parseGGUFQuantLabel(filepath);\n  return quantLabel ? `:${quantLabel}` : defaultTag;\n}\nconst snippetLlamacpp = (model, filepath) => {\n  const command = binary => {\n    const snippet = [\"# Load and run the model:\", `${binary} -hf ${model.id}${getQuantTag(filepath)}`];\n    return snippet.join(\"\\n\");\n  };\n  return [{\n    title: \"Install from brew\",\n    setup: \"brew install llama.cpp\",\n    content: command(\"llama-server\")\n  }, {\n    title: \"Install from WinGet (Windows)\",\n    setup: \"winget install llama.cpp\",\n    content: command(\"llama-server\")\n  }, {\n    title: \"Use pre-built binary\",\n    setup: [\n    // prettier-ignore\n    \"# Download pre-built binary from:\", \"# https://github.com/ggerganov/llama.cpp/releases\"].join(\"\\n\"),\n    content: command(\"./llama-server\")\n  }, {\n    title: \"Build from source code\",\n    setup: [\"git clone https://github.com/ggerganov/llama.cpp.git\", \"cd llama.cpp\", \"cmake -B build\", \"cmake --build build -j --target llama-server\"].join(\"\\n\"),\n    content: command(\"./build/bin/llama-server\")\n  }];\n};\nconst snippetNodeLlamaCppCli = (model, filepath) => {\n  const tagName = getQuantTag(filepath);\n  return [{\n    title: \"Chat with the model\",\n    content: `npx -y node-llama-cpp chat hf:${model.id}${tagName}`\n  }, {\n    title: \"Estimate the model compatibility with your hardware\",\n    content: `npx -y node-llama-cpp inspect estimate hf:${model.id}${tagName}`\n  }];\n};\nconst snippetOllama = (model, filepath) => {\n  return `ollama run hf.co/${model.id}${getQuantTag(filepath)}`;\n};\nconst snippetLocalAI = (model, filepath) => {\n  const command = binary => [\"# Load and run the model:\", `${binary} huggingface://${model.id}/${filepath ?? \"{{GGUF_FILE}}\"}`].join(\"\\n\");\n  return [{\n    title: \"Install from binary\",\n    setup: \"curl https://localai.io/install.sh | sh\",\n    content: command(\"local-ai run\")\n  }, {\n    title: \"Use Docker images\",\n    setup: [\n    // prettier-ignore\n    \"# Pull the image:\", \"docker pull localai/localai:latest-cpu\"].join(\"\\n\"),\n    content: command(\"docker run -p 8080:8080 --name localai -v $PWD/models:/build/models localai/localai:latest-cpu\")\n  }];\n};\nconst snippetVllm = model => {\n  const messages = getModelInputSnippet(model);\n  const runCommandInstruct = `# Call the server using curl:\ncurl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\\n\t-H \"Content-Type: application/json\" \\\\\n\t--data '{\n\t\t\"model\": \"${model.id}\",\n\t\t\"messages\": ${stringifyMessages(messages, {\n    indent: \"\\t\\t\",\n    attributeKeyQuotes: true,\n    customContentEscaper: str => str.replace(/'/g, \"'\\\\''\")\n  })}\n\t}'`;\n  const runCommandNonInstruct = `# Call the server using curl:\ncurl -X POST \"http://localhost:8000/v1/completions\" \\\\\n\t-H \"Content-Type: application/json\" \\\\\n\t--data '{\n\t\t\"model\": \"${model.id}\",\n\t\t\"prompt\": \"Once upon a time,\",\n\t\t\"max_tokens\": 512,\n\t\t\"temperature\": 0.5\n\t}'`;\n  const runCommand = model.tags.includes(\"conversational\") ? runCommandInstruct : runCommandNonInstruct;\n  let setup;\n  let dockerCommand;\n  if (model.tags.includes(\"mistral-common\")) {\n    setup = [\"# Install vLLM from pip:\", \"pip install vllm\", \"# Make sure you have the latest version of mistral-common installed:\", \"pip install --upgrade mistral-common\"].join(\"\\n\");\n    dockerCommand = `# Load and run the model:\\ndocker exec -it my_vllm_container bash -c \"vllm serve ${model.id} --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\"`;\n  } else {\n    setup = [\"# Install vLLM from pip:\", \"pip install vllm\"].join(\"\\n\");\n    dockerCommand = `# Load and run the model:\\ndocker exec -it my_vllm_container bash -c \"vllm serve ${model.id}\"`;\n  }\n  return [{\n    title: \"Install from pip\",\n    setup: setup,\n    content: [`# Load and run the model:\\nvllm serve \"${model.id}\"`, runCommand]\n  }, {\n    title: \"Use Docker images\",\n    setup: [\"# Deploy with docker on Linux:\", `docker run --runtime nvidia --gpus all \\\\`, `\t--name my_vllm_container \\\\`, `\t-v ~/.cache/huggingface:/root/.cache/huggingface \\\\`, ` \t--env \"HUGGING_FACE_HUB_TOKEN=<secret>\" \\\\`, `\t-p 8000:8000 \\\\`, `\t--ipc=host \\\\`, `\tvllm/vllm-openai:latest \\\\`, `\t--model ${model.id}`].join(\"\\n\"),\n    content: [dockerCommand, runCommand]\n  }];\n};\nconst snippetTgi = model => {\n  const runCommand = [\"# Call the server using curl:\", `curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\`, `\t-H \"Content-Type: application/json\" \\\\`, `\t--data '{`, `\t\t\"model\": \"${model.id}\",`, `\t\t\"messages\": [`, `\t\t\t{\"role\": \"user\", \"content\": \"What is the capital of France?\"}`, `\t\t]`, `\t}'`];\n  return [{\n    title: \"Use Docker images\",\n    setup: [\"# Deploy with docker on Linux:\", `docker run --gpus all \\\\`, `\t-v ~/.cache/huggingface:/root/.cache/huggingface \\\\`, ` \t-e HF_TOKEN=\"<secret>\" \\\\`, `\t-p 8000:80 \\\\`, `\tghcr.io/huggingface/text-generation-inference:latest \\\\`, `\t--model-id ${model.id}`].join(\"\\n\"),\n    content: [runCommand.join(\"\\n\")]\n  }];\n};\nconst snippetMlxLm = model => {\n  const openaiCurl = [\"# Calling the OpenAI-compatible server with curl\", `curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\`, `   -H \"Content-Type: application/json\" \\\\`, `   --data '{`, `     \"model\": \"${model.id}\",`, `     \"messages\": [`, `       {\"role\": \"user\", \"content\": \"Hello\"}`, `     ]`, `   }'`];\n  return [{\n    title: \"Generate or start a chat session\",\n    setup: [\"# Install MLX LM\", \"uv tool install mlx-lm\"].join(\"\\n\"),\n    content: [...(model.tags.includes(\"conversational\") ? [\"# Interactive chat REPL\", `mlx_lm.chat --model \"${model.id}\"`] : [\"# Generate some text\", `mlx_lm.generate --model \"${model.id}\" --prompt \"Once upon a time\"`])].join(\"\\n\")\n  }, ...(model.tags.includes(\"conversational\") ? [{\n    title: \"Run an OpenAI-compatible server\",\n    setup: [\"# Install MLX LM\", \"uv tool install mlx-lm\"].join(\"\\n\"),\n    content: [\"# Start the server\", `mlx_lm.server --model \"${model.id}\"`, ...openaiCurl].join(\"\\n\")\n  }] : [])];\n};\nconst snippetDockerModelRunner = (model, filepath) => {\n  return `docker model run hf.co/${model.id}${getQuantTag(filepath)}`;\n};\nconst snippetLemonade = (model, filepath) => {\n  const tagName = getQuantTag(filepath);\n  const modelName = model.id.includes(\"/\") ? model.id.split(\"/\")[1] : model.id;\n  // Get recipe according to model type\n  let simplifiedModelName;\n  let recipe;\n  let checkpoint;\n  let requirements;\n  if (model.tags.some(tag => [\"ryzenai-npu\", \"ryzenai-hybrid\"].includes(tag))) {\n    recipe = model.tags.includes(\"ryzenai-npu\") ? \"oga-npu\" : \"oga-hybrid\";\n    checkpoint = model.id;\n    requirements = \" (requires RyzenAI 300 series)\";\n    simplifiedModelName = modelName.split(\"-awq-\")[0];\n    simplifiedModelName += recipe === \"oga-npu\" ? \"-NPU\" : \"-Hybrid\";\n  } else {\n    recipe = \"llamacpp\";\n    checkpoint = `${model.id}${tagName}`;\n    requirements = \"\";\n    simplifiedModelName = modelName;\n  }\n  return [{\n    title: \"Pull the model\",\n    setup: \"# Download Lemonade from https://lemonade-server.ai/\",\n    content: [`lemonade-server pull user.${simplifiedModelName} --checkpoint ${checkpoint} --recipe ${recipe}`, \"# Note: If you installed from source, use the lemonade-server-dev command instead.\"].join(\"\\n\")\n  }, {\n    title: `Run and chat with the model${requirements}`,\n    content: `lemonade-server run user.${simplifiedModelName}`\n  }, {\n    title: \"List all available models\",\n    content: \"lemonade-server list\"\n  }];\n};\n/**\n * Add your new local app here.\n *\n * This is open to new suggestions and awesome upcoming apps.\n *\n * /!\\ IMPORTANT\n *\n * If possible, you need to support deeplinks and be as cross-platform as possible.\n *\n * Ping the HF team if we can help with anything!\n */\nexport const LOCAL_APPS = {\n  \"llama.cpp\": {\n    prettyLabel: \"llama.cpp\",\n    docsUrl: \"https://github.com/ggerganov/llama.cpp\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    snippet: snippetLlamacpp\n  },\n  \"node-llama-cpp\": {\n    prettyLabel: \"node-llama-cpp\",\n    docsUrl: \"https://node-llama-cpp.withcat.ai\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    snippet: snippetNodeLlamaCppCli\n  },\n  vllm: {\n    prettyLabel: \"vLLM\",\n    docsUrl: \"https://docs.vllm.ai\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: model => (isAwqModel(model) || isGptqModel(model) || isAqlmModel(model) || isMarlinModel(model) || isLlamaCppGgufModel(model) || isTransformersModel(model)) && (model.pipeline_tag === \"text-generation\" || model.pipeline_tag === \"image-text-to-text\"),\n    snippet: snippetVllm\n  },\n  \"mlx-lm\": {\n    prettyLabel: \"MLX LM\",\n    docsUrl: \"https://github.com/ml-explore/mlx-lm\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: model => model.pipeline_tag === \"text-generation\" && isMlxModel(model),\n    snippet: snippetMlxLm\n  },\n  tgi: {\n    prettyLabel: \"TGI\",\n    docsUrl: \"https://huggingface.co/docs/text-generation-inference/\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isTgiModel,\n    snippet: snippetTgi\n  },\n  lmstudio: {\n    prettyLabel: \"LM Studio\",\n    docsUrl: \"https://lmstudio.ai\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: model => isLlamaCppGgufModel(model) || isMlxModel(model),\n    deeplink: (model, filepath) => new URL(`lmstudio://open_from_hf?model=${model.id}${filepath ? `&file=${filepath}` : \"\"}`)\n  },\n  localai: {\n    prettyLabel: \"LocalAI\",\n    docsUrl: \"https://github.com/mudler/LocalAI\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    snippet: snippetLocalAI\n  },\n  jan: {\n    prettyLabel: \"Jan\",\n    docsUrl: \"https://jan.ai\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    deeplink: model => new URL(`jan://models/huggingface/${model.id}`)\n  },\n  backyard: {\n    prettyLabel: \"Backyard AI\",\n    docsUrl: \"https://backyard.ai\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    deeplink: model => new URL(`https://backyard.ai/hf/model/${model.id}`)\n  },\n  sanctum: {\n    prettyLabel: \"Sanctum\",\n    docsUrl: \"https://sanctum.ai\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    deeplink: model => new URL(`sanctum://open_from_hf?model=${model.id}`)\n  },\n  jellybox: {\n    prettyLabel: \"Jellybox\",\n    docsUrl: \"https://jellybox.com\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: model => isLlamaCppGgufModel(model) || model.library_name === \"diffusers\" && model.tags.includes(\"safetensors\") && (model.pipeline_tag === \"text-to-image\" || model.tags.includes(\"lora\")),\n    deeplink: model => {\n      if (isLlamaCppGgufModel(model)) {\n        return new URL(`jellybox://llm/models/huggingface/LLM/${model.id}`);\n      } else if (model.tags.includes(\"lora\")) {\n        return new URL(`jellybox://image/models/huggingface/ImageLora/${model.id}`);\n      } else {\n        return new URL(`jellybox://image/models/huggingface/Image/${model.id}`);\n      }\n    }\n  },\n  msty: {\n    prettyLabel: \"Msty\",\n    docsUrl: \"https://msty.app\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    deeplink: model => new URL(`msty://models/search/hf/${model.id}`)\n  },\n  recursechat: {\n    prettyLabel: \"RecurseChat\",\n    docsUrl: \"https://recurse.chat\",\n    mainTask: \"text-generation\",\n    macOSOnly: true,\n    displayOnModelPage: isLlamaCppGgufModel,\n    deeplink: model => new URL(`recursechat://new-hf-gguf-model?hf-model-id=${model.id}`)\n  },\n  drawthings: {\n    prettyLabel: \"Draw Things\",\n    docsUrl: \"https://drawthings.ai\",\n    mainTask: \"text-to-image\",\n    macOSOnly: true,\n    displayOnModelPage: model => model.library_name === \"diffusers\" && (model.pipeline_tag === \"text-to-image\" || model.tags.includes(\"lora\")),\n    deeplink: model => {\n      if (model.tags.includes(\"lora\")) {\n        return new URL(`https://drawthings.ai/import/diffusers/pipeline.load_lora_weights?repo_id=${model.id}`);\n      } else {\n        return new URL(`https://drawthings.ai/import/diffusers/pipeline.from_pretrained?repo_id=${model.id}`);\n      }\n    }\n  },\n  diffusionbee: {\n    prettyLabel: \"DiffusionBee\",\n    docsUrl: \"https://diffusionbee.com\",\n    mainTask: \"text-to-image\",\n    macOSOnly: true,\n    displayOnModelPage: model => model.library_name === \"diffusers\" && model.pipeline_tag === \"text-to-image\",\n    deeplink: model => new URL(`https://diffusionbee.com/huggingface_import?model_id=${model.id}`)\n  },\n  joyfusion: {\n    prettyLabel: \"JoyFusion\",\n    docsUrl: \"https://joyfusion.app\",\n    mainTask: \"text-to-image\",\n    macOSOnly: true,\n    displayOnModelPage: model => model.tags.includes(\"coreml\") && model.tags.includes(\"joyfusion\") && model.pipeline_tag === \"text-to-image\",\n    deeplink: model => new URL(`https://joyfusion.app/import_from_hf?repo_id=${model.id}`)\n  },\n  invoke: {\n    prettyLabel: \"Invoke\",\n    docsUrl: \"https://github.com/invoke-ai/InvokeAI\",\n    mainTask: \"text-to-image\",\n    displayOnModelPage: model => model.library_name === \"diffusers\" && model.pipeline_tag === \"text-to-image\",\n    deeplink: model => new URL(`https://models.invoke.ai/huggingface/${model.id}`)\n  },\n  ollama: {\n    prettyLabel: \"Ollama\",\n    docsUrl: \"https://ollama.com\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    snippet: snippetOllama\n  },\n  \"docker-model-runner\": {\n    prettyLabel: \"Docker Model Runner\",\n    docsUrl: \"https://docs.docker.com/ai/model-runner/\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: isLlamaCppGgufModel,\n    snippet: snippetDockerModelRunner\n  },\n  lemonade: {\n    prettyLabel: \"Lemonade\",\n    docsUrl: \"https://lemonade-server.ai\",\n    mainTask: \"text-generation\",\n    displayOnModelPage: model => isLlamaCppGgufModel(model) || isAmdRyzenModel(model),\n    snippet: snippetLemonade\n  }\n};","map":{"version":3,"names":["parseGGUFQuantLabel","stringifyMessages","getModelInputSnippet","isAwqModel","model","config","quantization_config","quant_method","isGptqModel","isAqlmModel","isMarlinModel","isTransformersModel","tags","includes","isTgiModel","isLlamaCppGgufModel","gguf","context_length","isAmdRyzenModel","isMlxModel","getQuantTag","filepath","defaultTag","quantLabel","snippetLlamacpp","command","binary","snippet","id","join","title","setup","content","snippetNodeLlamaCppCli","tagName","snippetOllama","snippetLocalAI","snippetVllm","messages","runCommandInstruct","indent","attributeKeyQuotes","customContentEscaper","str","replace","runCommandNonInstruct","runCommand","dockerCommand","snippetTgi","snippetMlxLm","openaiCurl","snippetDockerModelRunner","snippetLemonade","modelName","split","simplifiedModelName","recipe","checkpoint","requirements","some","tag","LOCAL_APPS","prettyLabel","docsUrl","mainTask","displayOnModelPage","vllm","pipeline_tag","tgi","lmstudio","deeplink","URL","localai","jan","backyard","sanctum","jellybox","library_name","msty","recursechat","macOSOnly","drawthings","diffusionbee","joyfusion","invoke","ollama","lemonade"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/local-apps.js"],"sourcesContent":["import { parseGGUFQuantLabel } from \"./gguf.js\";\nimport { stringifyMessages } from \"./snippets/common.js\";\nimport { getModelInputSnippet } from \"./snippets/inputs.js\";\nfunction isAwqModel(model) {\n    return model.config?.quantization_config?.quant_method === \"awq\";\n}\nfunction isGptqModel(model) {\n    return model.config?.quantization_config?.quant_method === \"gptq\";\n}\nfunction isAqlmModel(model) {\n    return model.config?.quantization_config?.quant_method === \"aqlm\";\n}\nfunction isMarlinModel(model) {\n    return model.config?.quantization_config?.quant_method === \"marlin\";\n}\nfunction isTransformersModel(model) {\n    return model.tags.includes(\"transformers\");\n}\nfunction isTgiModel(model) {\n    return model.tags.includes(\"text-generation-inference\");\n}\nfunction isLlamaCppGgufModel(model) {\n    return !!model.gguf?.context_length;\n}\nfunction isAmdRyzenModel(model) {\n    return model.tags.includes(\"ryzenai-hybrid\") || model.tags.includes(\"ryzenai-npu\");\n}\nfunction isMlxModel(model) {\n    return model.tags.includes(\"mlx\");\n}\nfunction getQuantTag(filepath) {\n    const defaultTag = \":{{QUANT_TAG}}\";\n    if (!filepath) {\n        return defaultTag;\n    }\n    const quantLabel = parseGGUFQuantLabel(filepath);\n    return quantLabel ? `:${quantLabel}` : defaultTag;\n}\nconst snippetLlamacpp = (model, filepath) => {\n    const command = (binary) => {\n        const snippet = [\"# Load and run the model:\", `${binary} -hf ${model.id}${getQuantTag(filepath)}`];\n        return snippet.join(\"\\n\");\n    };\n    return [\n        {\n            title: \"Install from brew\",\n            setup: \"brew install llama.cpp\",\n            content: command(\"llama-server\"),\n        },\n        {\n            title: \"Install from WinGet (Windows)\",\n            setup: \"winget install llama.cpp\",\n            content: command(\"llama-server\"),\n        },\n        {\n            title: \"Use pre-built binary\",\n            setup: [\n                // prettier-ignore\n                \"# Download pre-built binary from:\",\n                \"# https://github.com/ggerganov/llama.cpp/releases\",\n            ].join(\"\\n\"),\n            content: command(\"./llama-server\"),\n        },\n        {\n            title: \"Build from source code\",\n            setup: [\n                \"git clone https://github.com/ggerganov/llama.cpp.git\",\n                \"cd llama.cpp\",\n                \"cmake -B build\",\n                \"cmake --build build -j --target llama-server\",\n            ].join(\"\\n\"),\n            content: command(\"./build/bin/llama-server\"),\n        },\n    ];\n};\nconst snippetNodeLlamaCppCli = (model, filepath) => {\n    const tagName = getQuantTag(filepath);\n    return [\n        {\n            title: \"Chat with the model\",\n            content: `npx -y node-llama-cpp chat hf:${model.id}${tagName}`,\n        },\n        {\n            title: \"Estimate the model compatibility with your hardware\",\n            content: `npx -y node-llama-cpp inspect estimate hf:${model.id}${tagName}`,\n        },\n    ];\n};\nconst snippetOllama = (model, filepath) => {\n    return `ollama run hf.co/${model.id}${getQuantTag(filepath)}`;\n};\nconst snippetLocalAI = (model, filepath) => {\n    const command = (binary) => [\"# Load and run the model:\", `${binary} huggingface://${model.id}/${filepath ?? \"{{GGUF_FILE}}\"}`].join(\"\\n\");\n    return [\n        {\n            title: \"Install from binary\",\n            setup: \"curl https://localai.io/install.sh | sh\",\n            content: command(\"local-ai run\"),\n        },\n        {\n            title: \"Use Docker images\",\n            setup: [\n                // prettier-ignore\n                \"# Pull the image:\",\n                \"docker pull localai/localai:latest-cpu\",\n            ].join(\"\\n\"),\n            content: command(\"docker run -p 8080:8080 --name localai -v $PWD/models:/build/models localai/localai:latest-cpu\"),\n        },\n    ];\n};\nconst snippetVllm = (model) => {\n    const messages = getModelInputSnippet(model);\n    const runCommandInstruct = `# Call the server using curl:\ncurl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\\n\t-H \"Content-Type: application/json\" \\\\\n\t--data '{\n\t\t\"model\": \"${model.id}\",\n\t\t\"messages\": ${stringifyMessages(messages, {\n        indent: \"\\t\\t\",\n        attributeKeyQuotes: true,\n        customContentEscaper: (str) => str.replace(/'/g, \"'\\\\''\"),\n    })}\n\t}'`;\n    const runCommandNonInstruct = `# Call the server using curl:\ncurl -X POST \"http://localhost:8000/v1/completions\" \\\\\n\t-H \"Content-Type: application/json\" \\\\\n\t--data '{\n\t\t\"model\": \"${model.id}\",\n\t\t\"prompt\": \"Once upon a time,\",\n\t\t\"max_tokens\": 512,\n\t\t\"temperature\": 0.5\n\t}'`;\n    const runCommand = model.tags.includes(\"conversational\") ? runCommandInstruct : runCommandNonInstruct;\n    let setup;\n    let dockerCommand;\n    if (model.tags.includes(\"mistral-common\")) {\n        setup = [\n            \"# Install vLLM from pip:\",\n            \"pip install vllm\",\n            \"# Make sure you have the latest version of mistral-common installed:\",\n            \"pip install --upgrade mistral-common\",\n        ].join(\"\\n\");\n        dockerCommand = `# Load and run the model:\\ndocker exec -it my_vllm_container bash -c \"vllm serve ${model.id} --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice\"`;\n    }\n    else {\n        setup = [\"# Install vLLM from pip:\", \"pip install vllm\"].join(\"\\n\");\n        dockerCommand = `# Load and run the model:\\ndocker exec -it my_vllm_container bash -c \"vllm serve ${model.id}\"`;\n    }\n    return [\n        {\n            title: \"Install from pip\",\n            setup: setup,\n            content: [`# Load and run the model:\\nvllm serve \"${model.id}\"`, runCommand],\n        },\n        {\n            title: \"Use Docker images\",\n            setup: [\n                \"# Deploy with docker on Linux:\",\n                `docker run --runtime nvidia --gpus all \\\\`,\n                `\t--name my_vllm_container \\\\`,\n                `\t-v ~/.cache/huggingface:/root/.cache/huggingface \\\\`,\n                ` \t--env \"HUGGING_FACE_HUB_TOKEN=<secret>\" \\\\`,\n                `\t-p 8000:8000 \\\\`,\n                `\t--ipc=host \\\\`,\n                `\tvllm/vllm-openai:latest \\\\`,\n                `\t--model ${model.id}`,\n            ].join(\"\\n\"),\n            content: [dockerCommand, runCommand],\n        },\n    ];\n};\nconst snippetTgi = (model) => {\n    const runCommand = [\n        \"# Call the server using curl:\",\n        `curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\`,\n        `\t-H \"Content-Type: application/json\" \\\\`,\n        `\t--data '{`,\n        `\t\t\"model\": \"${model.id}\",`,\n        `\t\t\"messages\": [`,\n        `\t\t\t{\"role\": \"user\", \"content\": \"What is the capital of France?\"}`,\n        `\t\t]`,\n        `\t}'`,\n    ];\n    return [\n        {\n            title: \"Use Docker images\",\n            setup: [\n                \"# Deploy with docker on Linux:\",\n                `docker run --gpus all \\\\`,\n                `\t-v ~/.cache/huggingface:/root/.cache/huggingface \\\\`,\n                ` \t-e HF_TOKEN=\"<secret>\" \\\\`,\n                `\t-p 8000:80 \\\\`,\n                `\tghcr.io/huggingface/text-generation-inference:latest \\\\`,\n                `\t--model-id ${model.id}`,\n            ].join(\"\\n\"),\n            content: [runCommand.join(\"\\n\")],\n        },\n    ];\n};\nconst snippetMlxLm = (model) => {\n    const openaiCurl = [\n        \"# Calling the OpenAI-compatible server with curl\",\n        `curl -X POST \"http://localhost:8000/v1/chat/completions\" \\\\`,\n        `   -H \"Content-Type: application/json\" \\\\`,\n        `   --data '{`,\n        `     \"model\": \"${model.id}\",`,\n        `     \"messages\": [`,\n        `       {\"role\": \"user\", \"content\": \"Hello\"}`,\n        `     ]`,\n        `   }'`,\n    ];\n    return [\n        {\n            title: \"Generate or start a chat session\",\n            setup: [\"# Install MLX LM\", \"uv tool install mlx-lm\"].join(\"\\n\"),\n            content: [\n                ...(model.tags.includes(\"conversational\")\n                    ? [\"# Interactive chat REPL\", `mlx_lm.chat --model \"${model.id}\"`]\n                    : [\"# Generate some text\", `mlx_lm.generate --model \"${model.id}\" --prompt \"Once upon a time\"`]),\n            ].join(\"\\n\"),\n        },\n        ...(model.tags.includes(\"conversational\")\n            ? [\n                {\n                    title: \"Run an OpenAI-compatible server\",\n                    setup: [\"# Install MLX LM\", \"uv tool install mlx-lm\"].join(\"\\n\"),\n                    content: [\"# Start the server\", `mlx_lm.server --model \"${model.id}\"`, ...openaiCurl].join(\"\\n\"),\n                },\n            ]\n            : []),\n    ];\n};\nconst snippetDockerModelRunner = (model, filepath) => {\n    return `docker model run hf.co/${model.id}${getQuantTag(filepath)}`;\n};\nconst snippetLemonade = (model, filepath) => {\n    const tagName = getQuantTag(filepath);\n    const modelName = model.id.includes(\"/\") ? model.id.split(\"/\")[1] : model.id;\n    // Get recipe according to model type\n    let simplifiedModelName;\n    let recipe;\n    let checkpoint;\n    let requirements;\n    if (model.tags.some((tag) => [\"ryzenai-npu\", \"ryzenai-hybrid\"].includes(tag))) {\n        recipe = model.tags.includes(\"ryzenai-npu\") ? \"oga-npu\" : \"oga-hybrid\";\n        checkpoint = model.id;\n        requirements = \" (requires RyzenAI 300 series)\";\n        simplifiedModelName = modelName.split(\"-awq-\")[0];\n        simplifiedModelName += recipe === \"oga-npu\" ? \"-NPU\" : \"-Hybrid\";\n    }\n    else {\n        recipe = \"llamacpp\";\n        checkpoint = `${model.id}${tagName}`;\n        requirements = \"\";\n        simplifiedModelName = modelName;\n    }\n    return [\n        {\n            title: \"Pull the model\",\n            setup: \"# Download Lemonade from https://lemonade-server.ai/\",\n            content: [\n                `lemonade-server pull user.${simplifiedModelName} --checkpoint ${checkpoint} --recipe ${recipe}`,\n                \"# Note: If you installed from source, use the lemonade-server-dev command instead.\",\n            ].join(\"\\n\"),\n        },\n        {\n            title: `Run and chat with the model${requirements}`,\n            content: `lemonade-server run user.${simplifiedModelName}`,\n        },\n        {\n            title: \"List all available models\",\n            content: \"lemonade-server list\",\n        },\n    ];\n};\n/**\n * Add your new local app here.\n *\n * This is open to new suggestions and awesome upcoming apps.\n *\n * /!\\ IMPORTANT\n *\n * If possible, you need to support deeplinks and be as cross-platform as possible.\n *\n * Ping the HF team if we can help with anything!\n */\nexport const LOCAL_APPS = {\n    \"llama.cpp\": {\n        prettyLabel: \"llama.cpp\",\n        docsUrl: \"https://github.com/ggerganov/llama.cpp\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetLlamacpp,\n    },\n    \"node-llama-cpp\": {\n        prettyLabel: \"node-llama-cpp\",\n        docsUrl: \"https://node-llama-cpp.withcat.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetNodeLlamaCppCli,\n    },\n    vllm: {\n        prettyLabel: \"vLLM\",\n        docsUrl: \"https://docs.vllm.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => (isAwqModel(model) ||\n            isGptqModel(model) ||\n            isAqlmModel(model) ||\n            isMarlinModel(model) ||\n            isLlamaCppGgufModel(model) ||\n            isTransformersModel(model)) &&\n            (model.pipeline_tag === \"text-generation\" || model.pipeline_tag === \"image-text-to-text\"),\n        snippet: snippetVllm,\n    },\n    \"mlx-lm\": {\n        prettyLabel: \"MLX LM\",\n        docsUrl: \"https://github.com/ml-explore/mlx-lm\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => model.pipeline_tag === \"text-generation\" && isMlxModel(model),\n        snippet: snippetMlxLm,\n    },\n    tgi: {\n        prettyLabel: \"TGI\",\n        docsUrl: \"https://huggingface.co/docs/text-generation-inference/\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isTgiModel,\n        snippet: snippetTgi,\n    },\n    lmstudio: {\n        prettyLabel: \"LM Studio\",\n        docsUrl: \"https://lmstudio.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => isLlamaCppGgufModel(model) || isMlxModel(model),\n        deeplink: (model, filepath) => new URL(`lmstudio://open_from_hf?model=${model.id}${filepath ? `&file=${filepath}` : \"\"}`),\n    },\n    localai: {\n        prettyLabel: \"LocalAI\",\n        docsUrl: \"https://github.com/mudler/LocalAI\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetLocalAI,\n    },\n    jan: {\n        prettyLabel: \"Jan\",\n        docsUrl: \"https://jan.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`jan://models/huggingface/${model.id}`),\n    },\n    backyard: {\n        prettyLabel: \"Backyard AI\",\n        docsUrl: \"https://backyard.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`https://backyard.ai/hf/model/${model.id}`),\n    },\n    sanctum: {\n        prettyLabel: \"Sanctum\",\n        docsUrl: \"https://sanctum.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`sanctum://open_from_hf?model=${model.id}`),\n    },\n    jellybox: {\n        prettyLabel: \"Jellybox\",\n        docsUrl: \"https://jellybox.com\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => isLlamaCppGgufModel(model) ||\n            (model.library_name === \"diffusers\" &&\n                model.tags.includes(\"safetensors\") &&\n                (model.pipeline_tag === \"text-to-image\" || model.tags.includes(\"lora\"))),\n        deeplink: (model) => {\n            if (isLlamaCppGgufModel(model)) {\n                return new URL(`jellybox://llm/models/huggingface/LLM/${model.id}`);\n            }\n            else if (model.tags.includes(\"lora\")) {\n                return new URL(`jellybox://image/models/huggingface/ImageLora/${model.id}`);\n            }\n            else {\n                return new URL(`jellybox://image/models/huggingface/Image/${model.id}`);\n            }\n        },\n    },\n    msty: {\n        prettyLabel: \"Msty\",\n        docsUrl: \"https://msty.app\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`msty://models/search/hf/${model.id}`),\n    },\n    recursechat: {\n        prettyLabel: \"RecurseChat\",\n        docsUrl: \"https://recurse.chat\",\n        mainTask: \"text-generation\",\n        macOSOnly: true,\n        displayOnModelPage: isLlamaCppGgufModel,\n        deeplink: (model) => new URL(`recursechat://new-hf-gguf-model?hf-model-id=${model.id}`),\n    },\n    drawthings: {\n        prettyLabel: \"Draw Things\",\n        docsUrl: \"https://drawthings.ai\",\n        mainTask: \"text-to-image\",\n        macOSOnly: true,\n        displayOnModelPage: (model) => model.library_name === \"diffusers\" && (model.pipeline_tag === \"text-to-image\" || model.tags.includes(\"lora\")),\n        deeplink: (model) => {\n            if (model.tags.includes(\"lora\")) {\n                return new URL(`https://drawthings.ai/import/diffusers/pipeline.load_lora_weights?repo_id=${model.id}`);\n            }\n            else {\n                return new URL(`https://drawthings.ai/import/diffusers/pipeline.from_pretrained?repo_id=${model.id}`);\n            }\n        },\n    },\n    diffusionbee: {\n        prettyLabel: \"DiffusionBee\",\n        docsUrl: \"https://diffusionbee.com\",\n        mainTask: \"text-to-image\",\n        macOSOnly: true,\n        displayOnModelPage: (model) => model.library_name === \"diffusers\" && model.pipeline_tag === \"text-to-image\",\n        deeplink: (model) => new URL(`https://diffusionbee.com/huggingface_import?model_id=${model.id}`),\n    },\n    joyfusion: {\n        prettyLabel: \"JoyFusion\",\n        docsUrl: \"https://joyfusion.app\",\n        mainTask: \"text-to-image\",\n        macOSOnly: true,\n        displayOnModelPage: (model) => model.tags.includes(\"coreml\") && model.tags.includes(\"joyfusion\") && model.pipeline_tag === \"text-to-image\",\n        deeplink: (model) => new URL(`https://joyfusion.app/import_from_hf?repo_id=${model.id}`),\n    },\n    invoke: {\n        prettyLabel: \"Invoke\",\n        docsUrl: \"https://github.com/invoke-ai/InvokeAI\",\n        mainTask: \"text-to-image\",\n        displayOnModelPage: (model) => model.library_name === \"diffusers\" && model.pipeline_tag === \"text-to-image\",\n        deeplink: (model) => new URL(`https://models.invoke.ai/huggingface/${model.id}`),\n    },\n    ollama: {\n        prettyLabel: \"Ollama\",\n        docsUrl: \"https://ollama.com\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetOllama,\n    },\n    \"docker-model-runner\": {\n        prettyLabel: \"Docker Model Runner\",\n        docsUrl: \"https://docs.docker.com/ai/model-runner/\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: isLlamaCppGgufModel,\n        snippet: snippetDockerModelRunner,\n    },\n    lemonade: {\n        prettyLabel: \"Lemonade\",\n        docsUrl: \"https://lemonade-server.ai\",\n        mainTask: \"text-generation\",\n        displayOnModelPage: (model) => isLlamaCppGgufModel(model) || isAmdRyzenModel(model),\n        snippet: snippetLemonade,\n    },\n};\n"],"mappings":"AAAA,SAASA,mBAAmB,QAAQ,WAAW;AAC/C,SAASC,iBAAiB,QAAQ,sBAAsB;AACxD,SAASC,oBAAoB,QAAQ,sBAAsB;AAC3D,SAASC,UAAUA,CAACC,KAAK,EAAE;EACvB,OAAOA,KAAK,CAACC,MAAM,EAAEC,mBAAmB,EAAEC,YAAY,KAAK,KAAK;AACpE;AACA,SAASC,WAAWA,CAACJ,KAAK,EAAE;EACxB,OAAOA,KAAK,CAACC,MAAM,EAAEC,mBAAmB,EAAEC,YAAY,KAAK,MAAM;AACrE;AACA,SAASE,WAAWA,CAACL,KAAK,EAAE;EACxB,OAAOA,KAAK,CAACC,MAAM,EAAEC,mBAAmB,EAAEC,YAAY,KAAK,MAAM;AACrE;AACA,SAASG,aAAaA,CAACN,KAAK,EAAE;EAC1B,OAAOA,KAAK,CAACC,MAAM,EAAEC,mBAAmB,EAAEC,YAAY,KAAK,QAAQ;AACvE;AACA,SAASI,mBAAmBA,CAACP,KAAK,EAAE;EAChC,OAAOA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,cAAc,CAAC;AAC9C;AACA,SAASC,UAAUA,CAACV,KAAK,EAAE;EACvB,OAAOA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,2BAA2B,CAAC;AAC3D;AACA,SAASE,mBAAmBA,CAACX,KAAK,EAAE;EAChC,OAAO,CAAC,CAACA,KAAK,CAACY,IAAI,EAAEC,cAAc;AACvC;AACA,SAASC,eAAeA,CAACd,KAAK,EAAE;EAC5B,OAAOA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,IAAIT,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,aAAa,CAAC;AACtF;AACA,SAASM,UAAUA,CAACf,KAAK,EAAE;EACvB,OAAOA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,KAAK,CAAC;AACrC;AACA,SAASO,WAAWA,CAACC,QAAQ,EAAE;EAC3B,MAAMC,UAAU,GAAG,gBAAgB;EACnC,IAAI,CAACD,QAAQ,EAAE;IACX,OAAOC,UAAU;EACrB;EACA,MAAMC,UAAU,GAAGvB,mBAAmB,CAACqB,QAAQ,CAAC;EAChD,OAAOE,UAAU,GAAG,IAAIA,UAAU,EAAE,GAAGD,UAAU;AACrD;AACA,MAAME,eAAe,GAAGA,CAACpB,KAAK,EAAEiB,QAAQ,KAAK;EACzC,MAAMI,OAAO,GAAIC,MAAM,IAAK;IACxB,MAAMC,OAAO,GAAG,CAAC,2BAA2B,EAAE,GAAGD,MAAM,QAAQtB,KAAK,CAACwB,EAAE,GAAGR,WAAW,CAACC,QAAQ,CAAC,EAAE,CAAC;IAClG,OAAOM,OAAO,CAACE,IAAI,CAAC,IAAI,CAAC;EAC7B,CAAC;EACD,OAAO,CACH;IACIC,KAAK,EAAE,mBAAmB;IAC1BC,KAAK,EAAE,wBAAwB;IAC/BC,OAAO,EAAEP,OAAO,CAAC,cAAc;EACnC,CAAC,EACD;IACIK,KAAK,EAAE,+BAA+B;IACtCC,KAAK,EAAE,0BAA0B;IACjCC,OAAO,EAAEP,OAAO,CAAC,cAAc;EACnC,CAAC,EACD;IACIK,KAAK,EAAE,sBAAsB;IAC7BC,KAAK,EAAE;IACH;IACA,mCAAmC,EACnC,mDAAmD,CACtD,CAACF,IAAI,CAAC,IAAI,CAAC;IACZG,OAAO,EAAEP,OAAO,CAAC,gBAAgB;EACrC,CAAC,EACD;IACIK,KAAK,EAAE,wBAAwB;IAC/BC,KAAK,EAAE,CACH,sDAAsD,EACtD,cAAc,EACd,gBAAgB,EAChB,8CAA8C,CACjD,CAACF,IAAI,CAAC,IAAI,CAAC;IACZG,OAAO,EAAEP,OAAO,CAAC,0BAA0B;EAC/C,CAAC,CACJ;AACL,CAAC;AACD,MAAMQ,sBAAsB,GAAGA,CAAC7B,KAAK,EAAEiB,QAAQ,KAAK;EAChD,MAAMa,OAAO,GAAGd,WAAW,CAACC,QAAQ,CAAC;EACrC,OAAO,CACH;IACIS,KAAK,EAAE,qBAAqB;IAC5BE,OAAO,EAAE,iCAAiC5B,KAAK,CAACwB,EAAE,GAAGM,OAAO;EAChE,CAAC,EACD;IACIJ,KAAK,EAAE,qDAAqD;IAC5DE,OAAO,EAAE,6CAA6C5B,KAAK,CAACwB,EAAE,GAAGM,OAAO;EAC5E,CAAC,CACJ;AACL,CAAC;AACD,MAAMC,aAAa,GAAGA,CAAC/B,KAAK,EAAEiB,QAAQ,KAAK;EACvC,OAAO,oBAAoBjB,KAAK,CAACwB,EAAE,GAAGR,WAAW,CAACC,QAAQ,CAAC,EAAE;AACjE,CAAC;AACD,MAAMe,cAAc,GAAGA,CAAChC,KAAK,EAAEiB,QAAQ,KAAK;EACxC,MAAMI,OAAO,GAAIC,MAAM,IAAK,CAAC,2BAA2B,EAAE,GAAGA,MAAM,kBAAkBtB,KAAK,CAACwB,EAAE,IAAIP,QAAQ,IAAI,eAAe,EAAE,CAAC,CAACQ,IAAI,CAAC,IAAI,CAAC;EAC1I,OAAO,CACH;IACIC,KAAK,EAAE,qBAAqB;IAC5BC,KAAK,EAAE,yCAAyC;IAChDC,OAAO,EAAEP,OAAO,CAAC,cAAc;EACnC,CAAC,EACD;IACIK,KAAK,EAAE,mBAAmB;IAC1BC,KAAK,EAAE;IACH;IACA,mBAAmB,EACnB,wCAAwC,CAC3C,CAACF,IAAI,CAAC,IAAI,CAAC;IACZG,OAAO,EAAEP,OAAO,CAAC,gGAAgG;EACrH,CAAC,CACJ;AACL,CAAC;AACD,MAAMY,WAAW,GAAIjC,KAAK,IAAK;EAC3B,MAAMkC,QAAQ,GAAGpC,oBAAoB,CAACE,KAAK,CAAC;EAC5C,MAAMmC,kBAAkB,GAAG;AAC/B;AACA;AACA;AACA,cAAcnC,KAAK,CAACwB,EAAE;AACtB,gBAAgB3B,iBAAiB,CAACqC,QAAQ,EAAE;IACpCE,MAAM,EAAE,MAAM;IACdC,kBAAkB,EAAE,IAAI;IACxBC,oBAAoB,EAAGC,GAAG,IAAKA,GAAG,CAACC,OAAO,CAAC,IAAI,EAAE,OAAO;EAC5D,CAAC,CAAC;AACN,IAAI;EACA,MAAMC,qBAAqB,GAAG;AAClC;AACA;AACA;AACA,cAAczC,KAAK,CAACwB,EAAE;AACtB;AACA;AACA;AACA,IAAI;EACA,MAAMkB,UAAU,GAAG1C,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,GAAG0B,kBAAkB,GAAGM,qBAAqB;EACrG,IAAId,KAAK;EACT,IAAIgB,aAAa;EACjB,IAAI3C,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,EAAE;IACvCkB,KAAK,GAAG,CACJ,0BAA0B,EAC1B,kBAAkB,EAClB,sEAAsE,EACtE,sCAAsC,CACzC,CAACF,IAAI,CAAC,IAAI,CAAC;IACZkB,aAAa,GAAG,oFAAoF3C,KAAK,CAACwB,EAAE,+HAA+H;EAC/O,CAAC,MACI;IACDG,KAAK,GAAG,CAAC,0BAA0B,EAAE,kBAAkB,CAAC,CAACF,IAAI,CAAC,IAAI,CAAC;IACnEkB,aAAa,GAAG,oFAAoF3C,KAAK,CAACwB,EAAE,GAAG;EACnH;EACA,OAAO,CACH;IACIE,KAAK,EAAE,kBAAkB;IACzBC,KAAK,EAAEA,KAAK;IACZC,OAAO,EAAE,CAAC,0CAA0C5B,KAAK,CAACwB,EAAE,GAAG,EAAEkB,UAAU;EAC/E,CAAC,EACD;IACIhB,KAAK,EAAE,mBAAmB;IAC1BC,KAAK,EAAE,CACH,gCAAgC,EAChC,2CAA2C,EAC3C,8BAA8B,EAC9B,sDAAsD,EACtD,8CAA8C,EAC9C,kBAAkB,EAClB,gBAAgB,EAChB,6BAA6B,EAC7B,YAAY3B,KAAK,CAACwB,EAAE,EAAE,CACzB,CAACC,IAAI,CAAC,IAAI,CAAC;IACZG,OAAO,EAAE,CAACe,aAAa,EAAED,UAAU;EACvC,CAAC,CACJ;AACL,CAAC;AACD,MAAME,UAAU,GAAI5C,KAAK,IAAK;EAC1B,MAAM0C,UAAU,GAAG,CACf,+BAA+B,EAC/B,6DAA6D,EAC7D,yCAAyC,EACzC,YAAY,EACZ,eAAe1C,KAAK,CAACwB,EAAE,IAAI,EAC3B,iBAAiB,EACjB,kEAAkE,EAClE,KAAK,EACL,KAAK,CACR;EACD,OAAO,CACH;IACIE,KAAK,EAAE,mBAAmB;IAC1BC,KAAK,EAAE,CACH,gCAAgC,EAChC,0BAA0B,EAC1B,sDAAsD,EACtD,6BAA6B,EAC7B,gBAAgB,EAChB,0DAA0D,EAC1D,eAAe3B,KAAK,CAACwB,EAAE,EAAE,CAC5B,CAACC,IAAI,CAAC,IAAI,CAAC;IACZG,OAAO,EAAE,CAACc,UAAU,CAACjB,IAAI,CAAC,IAAI,CAAC;EACnC,CAAC,CACJ;AACL,CAAC;AACD,MAAMoB,YAAY,GAAI7C,KAAK,IAAK;EAC5B,MAAM8C,UAAU,GAAG,CACf,kDAAkD,EAClD,6DAA6D,EAC7D,2CAA2C,EAC3C,cAAc,EACd,kBAAkB9C,KAAK,CAACwB,EAAE,IAAI,EAC9B,oBAAoB,EACpB,6CAA6C,EAC7C,QAAQ,EACR,OAAO,CACV;EACD,OAAO,CACH;IACIE,KAAK,EAAE,kCAAkC;IACzCC,KAAK,EAAE,CAAC,kBAAkB,EAAE,wBAAwB,CAAC,CAACF,IAAI,CAAC,IAAI,CAAC;IAChEG,OAAO,EAAE,CACL,IAAI5B,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,GACnC,CAAC,yBAAyB,EAAE,wBAAwBT,KAAK,CAACwB,EAAE,GAAG,CAAC,GAChE,CAAC,sBAAsB,EAAE,4BAA4BxB,KAAK,CAACwB,EAAE,+BAA+B,CAAC,CAAC,CACvG,CAACC,IAAI,CAAC,IAAI;EACf,CAAC,EACD,IAAIzB,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,gBAAgB,CAAC,GACnC,CACE;IACIiB,KAAK,EAAE,iCAAiC;IACxCC,KAAK,EAAE,CAAC,kBAAkB,EAAE,wBAAwB,CAAC,CAACF,IAAI,CAAC,IAAI,CAAC;IAChEG,OAAO,EAAE,CAAC,oBAAoB,EAAE,0BAA0B5B,KAAK,CAACwB,EAAE,GAAG,EAAE,GAAGsB,UAAU,CAAC,CAACrB,IAAI,CAAC,IAAI;EACnG,CAAC,CACJ,GACC,EAAE,CAAC,CACZ;AACL,CAAC;AACD,MAAMsB,wBAAwB,GAAGA,CAAC/C,KAAK,EAAEiB,QAAQ,KAAK;EAClD,OAAO,0BAA0BjB,KAAK,CAACwB,EAAE,GAAGR,WAAW,CAACC,QAAQ,CAAC,EAAE;AACvE,CAAC;AACD,MAAM+B,eAAe,GAAGA,CAAChD,KAAK,EAAEiB,QAAQ,KAAK;EACzC,MAAMa,OAAO,GAAGd,WAAW,CAACC,QAAQ,CAAC;EACrC,MAAMgC,SAAS,GAAGjD,KAAK,CAACwB,EAAE,CAACf,QAAQ,CAAC,GAAG,CAAC,GAAGT,KAAK,CAACwB,EAAE,CAAC0B,KAAK,CAAC,GAAG,CAAC,CAAC,CAAC,CAAC,GAAGlD,KAAK,CAACwB,EAAE;EAC5E;EACA,IAAI2B,mBAAmB;EACvB,IAAIC,MAAM;EACV,IAAIC,UAAU;EACd,IAAIC,YAAY;EAChB,IAAItD,KAAK,CAACQ,IAAI,CAAC+C,IAAI,CAAEC,GAAG,IAAK,CAAC,aAAa,EAAE,gBAAgB,CAAC,CAAC/C,QAAQ,CAAC+C,GAAG,CAAC,CAAC,EAAE;IAC3EJ,MAAM,GAAGpD,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,aAAa,CAAC,GAAG,SAAS,GAAG,YAAY;IACtE4C,UAAU,GAAGrD,KAAK,CAACwB,EAAE;IACrB8B,YAAY,GAAG,gCAAgC;IAC/CH,mBAAmB,GAAGF,SAAS,CAACC,KAAK,CAAC,OAAO,CAAC,CAAC,CAAC,CAAC;IACjDC,mBAAmB,IAAIC,MAAM,KAAK,SAAS,GAAG,MAAM,GAAG,SAAS;EACpE,CAAC,MACI;IACDA,MAAM,GAAG,UAAU;IACnBC,UAAU,GAAG,GAAGrD,KAAK,CAACwB,EAAE,GAAGM,OAAO,EAAE;IACpCwB,YAAY,GAAG,EAAE;IACjBH,mBAAmB,GAAGF,SAAS;EACnC;EACA,OAAO,CACH;IACIvB,KAAK,EAAE,gBAAgB;IACvBC,KAAK,EAAE,sDAAsD;IAC7DC,OAAO,EAAE,CACL,6BAA6BuB,mBAAmB,iBAAiBE,UAAU,aAAaD,MAAM,EAAE,EAChG,oFAAoF,CACvF,CAAC3B,IAAI,CAAC,IAAI;EACf,CAAC,EACD;IACIC,KAAK,EAAE,8BAA8B4B,YAAY,EAAE;IACnD1B,OAAO,EAAE,4BAA4BuB,mBAAmB;EAC5D,CAAC,EACD;IACIzB,KAAK,EAAE,2BAA2B;IAClCE,OAAO,EAAE;EACb,CAAC,CACJ;AACL,CAAC;AACD;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA,OAAO,MAAM6B,UAAU,GAAG;EACtB,WAAW,EAAE;IACTC,WAAW,EAAE,WAAW;IACxBC,OAAO,EAAE,wCAAwC;IACjDC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCY,OAAO,EAAEH;EACb,CAAC;EACD,gBAAgB,EAAE;IACdsC,WAAW,EAAE,gBAAgB;IAC7BC,OAAO,EAAE,mCAAmC;IAC5CC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCY,OAAO,EAAEM;EACb,CAAC;EACDiC,IAAI,EAAE;IACFJ,WAAW,EAAE,MAAM;IACnBC,OAAO,EAAE,sBAAsB;IAC/BC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAG7D,KAAK,IAAK,CAACD,UAAU,CAACC,KAAK,CAAC,IAC7CI,WAAW,CAACJ,KAAK,CAAC,IAClBK,WAAW,CAACL,KAAK,CAAC,IAClBM,aAAa,CAACN,KAAK,CAAC,IACpBW,mBAAmB,CAACX,KAAK,CAAC,IAC1BO,mBAAmB,CAACP,KAAK,CAAC,MACzBA,KAAK,CAAC+D,YAAY,KAAK,iBAAiB,IAAI/D,KAAK,CAAC+D,YAAY,KAAK,oBAAoB,CAAC;IAC7FxC,OAAO,EAAEU;EACb,CAAC;EACD,QAAQ,EAAE;IACNyB,WAAW,EAAE,QAAQ;IACrBC,OAAO,EAAE,sCAAsC;IAC/CC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAG7D,KAAK,IAAKA,KAAK,CAAC+D,YAAY,KAAK,iBAAiB,IAAIhD,UAAU,CAACf,KAAK,CAAC;IAC5FuB,OAAO,EAAEsB;EACb,CAAC;EACDmB,GAAG,EAAE;IACDN,WAAW,EAAE,KAAK;IAClBC,OAAO,EAAE,wDAAwD;IACjEC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAEnD,UAAU;IAC9Ba,OAAO,EAAEqB;EACb,CAAC;EACDqB,QAAQ,EAAE;IACNP,WAAW,EAAE,WAAW;IACxBC,OAAO,EAAE,qBAAqB;IAC9BC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAG7D,KAAK,IAAKW,mBAAmB,CAACX,KAAK,CAAC,IAAIe,UAAU,CAACf,KAAK,CAAC;IAC9EkE,QAAQ,EAAEA,CAAClE,KAAK,EAAEiB,QAAQ,KAAK,IAAIkD,GAAG,CAAC,iCAAiCnE,KAAK,CAACwB,EAAE,GAAGP,QAAQ,GAAG,SAASA,QAAQ,EAAE,GAAG,EAAE,EAAE;EAC5H,CAAC;EACDmD,OAAO,EAAE;IACLV,WAAW,EAAE,SAAS;IACtBC,OAAO,EAAE,mCAAmC;IAC5CC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCY,OAAO,EAAES;EACb,CAAC;EACDqC,GAAG,EAAE;IACDX,WAAW,EAAE,KAAK;IAClBC,OAAO,EAAE,gBAAgB;IACzBC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCuD,QAAQ,EAAGlE,KAAK,IAAK,IAAImE,GAAG,CAAC,4BAA4BnE,KAAK,CAACwB,EAAE,EAAE;EACvE,CAAC;EACD8C,QAAQ,EAAE;IACNZ,WAAW,EAAE,aAAa;IAC1BC,OAAO,EAAE,qBAAqB;IAC9BC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCuD,QAAQ,EAAGlE,KAAK,IAAK,IAAImE,GAAG,CAAC,gCAAgCnE,KAAK,CAACwB,EAAE,EAAE;EAC3E,CAAC;EACD+C,OAAO,EAAE;IACLb,WAAW,EAAE,SAAS;IACtBC,OAAO,EAAE,oBAAoB;IAC7BC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCuD,QAAQ,EAAGlE,KAAK,IAAK,IAAImE,GAAG,CAAC,gCAAgCnE,KAAK,CAACwB,EAAE,EAAE;EAC3E,CAAC;EACDgD,QAAQ,EAAE;IACNd,WAAW,EAAE,UAAU;IACvBC,OAAO,EAAE,sBAAsB;IAC/BC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAG7D,KAAK,IAAKW,mBAAmB,CAACX,KAAK,CAAC,IACpDA,KAAK,CAACyE,YAAY,KAAK,WAAW,IAC/BzE,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,aAAa,CAAC,KACjCT,KAAK,CAAC+D,YAAY,KAAK,eAAe,IAAI/D,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,MAAM,CAAC,CAAE;IAChFyD,QAAQ,EAAGlE,KAAK,IAAK;MACjB,IAAIW,mBAAmB,CAACX,KAAK,CAAC,EAAE;QAC5B,OAAO,IAAImE,GAAG,CAAC,yCAAyCnE,KAAK,CAACwB,EAAE,EAAE,CAAC;MACvE,CAAC,MACI,IAAIxB,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,MAAM,CAAC,EAAE;QAClC,OAAO,IAAI0D,GAAG,CAAC,iDAAiDnE,KAAK,CAACwB,EAAE,EAAE,CAAC;MAC/E,CAAC,MACI;QACD,OAAO,IAAI2C,GAAG,CAAC,6CAA6CnE,KAAK,CAACwB,EAAE,EAAE,CAAC;MAC3E;IACJ;EACJ,CAAC;EACDkD,IAAI,EAAE;IACFhB,WAAW,EAAE,MAAM;IACnBC,OAAO,EAAE,kBAAkB;IAC3BC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCuD,QAAQ,EAAGlE,KAAK,IAAK,IAAImE,GAAG,CAAC,2BAA2BnE,KAAK,CAACwB,EAAE,EAAE;EACtE,CAAC;EACDmD,WAAW,EAAE;IACTjB,WAAW,EAAE,aAAa;IAC1BC,OAAO,EAAE,sBAAsB;IAC/BC,QAAQ,EAAE,iBAAiB;IAC3BgB,SAAS,EAAE,IAAI;IACff,kBAAkB,EAAElD,mBAAmB;IACvCuD,QAAQ,EAAGlE,KAAK,IAAK,IAAImE,GAAG,CAAC,+CAA+CnE,KAAK,CAACwB,EAAE,EAAE;EAC1F,CAAC;EACDqD,UAAU,EAAE;IACRnB,WAAW,EAAE,aAAa;IAC1BC,OAAO,EAAE,uBAAuB;IAChCC,QAAQ,EAAE,eAAe;IACzBgB,SAAS,EAAE,IAAI;IACff,kBAAkB,EAAG7D,KAAK,IAAKA,KAAK,CAACyE,YAAY,KAAK,WAAW,KAAKzE,KAAK,CAAC+D,YAAY,KAAK,eAAe,IAAI/D,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,MAAM,CAAC,CAAC;IAC5IyD,QAAQ,EAAGlE,KAAK,IAAK;MACjB,IAAIA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,MAAM,CAAC,EAAE;QAC7B,OAAO,IAAI0D,GAAG,CAAC,6EAA6EnE,KAAK,CAACwB,EAAE,EAAE,CAAC;MAC3G,CAAC,MACI;QACD,OAAO,IAAI2C,GAAG,CAAC,2EAA2EnE,KAAK,CAACwB,EAAE,EAAE,CAAC;MACzG;IACJ;EACJ,CAAC;EACDsD,YAAY,EAAE;IACVpB,WAAW,EAAE,cAAc;IAC3BC,OAAO,EAAE,0BAA0B;IACnCC,QAAQ,EAAE,eAAe;IACzBgB,SAAS,EAAE,IAAI;IACff,kBAAkB,EAAG7D,KAAK,IAAKA,KAAK,CAACyE,YAAY,KAAK,WAAW,IAAIzE,KAAK,CAAC+D,YAAY,KAAK,eAAe;IAC3GG,QAAQ,EAAGlE,KAAK,IAAK,IAAImE,GAAG,CAAC,wDAAwDnE,KAAK,CAACwB,EAAE,EAAE;EACnG,CAAC;EACDuD,SAAS,EAAE;IACPrB,WAAW,EAAE,WAAW;IACxBC,OAAO,EAAE,uBAAuB;IAChCC,QAAQ,EAAE,eAAe;IACzBgB,SAAS,EAAE,IAAI;IACff,kBAAkB,EAAG7D,KAAK,IAAKA,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,QAAQ,CAAC,IAAIT,KAAK,CAACQ,IAAI,CAACC,QAAQ,CAAC,WAAW,CAAC,IAAIT,KAAK,CAAC+D,YAAY,KAAK,eAAe;IAC1IG,QAAQ,EAAGlE,KAAK,IAAK,IAAImE,GAAG,CAAC,gDAAgDnE,KAAK,CAACwB,EAAE,EAAE;EAC3F,CAAC;EACDwD,MAAM,EAAE;IACJtB,WAAW,EAAE,QAAQ;IACrBC,OAAO,EAAE,uCAAuC;IAChDC,QAAQ,EAAE,eAAe;IACzBC,kBAAkB,EAAG7D,KAAK,IAAKA,KAAK,CAACyE,YAAY,KAAK,WAAW,IAAIzE,KAAK,CAAC+D,YAAY,KAAK,eAAe;IAC3GG,QAAQ,EAAGlE,KAAK,IAAK,IAAImE,GAAG,CAAC,wCAAwCnE,KAAK,CAACwB,EAAE,EAAE;EACnF,CAAC;EACDyD,MAAM,EAAE;IACJvB,WAAW,EAAE,QAAQ;IACrBC,OAAO,EAAE,oBAAoB;IAC7BC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCY,OAAO,EAAEQ;EACb,CAAC;EACD,qBAAqB,EAAE;IACnB2B,WAAW,EAAE,qBAAqB;IAClCC,OAAO,EAAE,0CAA0C;IACnDC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAElD,mBAAmB;IACvCY,OAAO,EAAEwB;EACb,CAAC;EACDmC,QAAQ,EAAE;IACNxB,WAAW,EAAE,UAAU;IACvBC,OAAO,EAAE,4BAA4B;IACrCC,QAAQ,EAAE,iBAAiB;IAC3BC,kBAAkB,EAAG7D,KAAK,IAAKW,mBAAmB,CAACX,KAAK,CAAC,IAAIc,eAAe,CAACd,KAAK,CAAC;IACnFuB,OAAO,EAAEyB;EACb;AACJ,CAAC","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}