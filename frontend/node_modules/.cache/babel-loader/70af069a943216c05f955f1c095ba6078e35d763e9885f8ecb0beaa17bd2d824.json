{"ast":null,"code":"const taskData = {\n  datasets: [{\n    description: \"Multiple-choice questions and answers about videos.\",\n    id: \"lmms-lab/Video-MME\"\n  }, {\n    description: \"A dataset of instructions and question-answer pairs about videos.\",\n    id: \"lmms-lab/VideoChatGPT\"\n  }, {\n    description: \"Large video understanding dataset.\",\n    id: \"HuggingFaceFV/finevideo\"\n  }],\n  demo: {\n    inputs: [{\n      filename: \"video-text-to-text-input.gif\",\n      type: \"img\"\n    }, {\n      label: \"Text Prompt\",\n      content: \"What is happening in this video?\",\n      type: \"text\"\n    }],\n    outputs: [{\n      label: \"Answer\",\n      content: \"The video shows a series of images showing a fountain with water jets and a variety of colorful flowers and butterflies in the background.\",\n      type: \"text\"\n    }]\n  },\n  metrics: [],\n  models: [{\n    description: \"A robust video-text-to-text model.\",\n    id: \"Vision-CAIR/LongVU_Qwen2_7B\"\n  }, {\n    description: \"Strong video-text-to-text model with reasoning capabilities.\",\n    id: \"GoodiesHere/Apollo-LMMs-Apollo-7B-t32\"\n  }, {\n    description: \"Strong video-text-to-text model.\",\n    id: \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n  }],\n  spaces: [{\n    description: \"An application to chat with a video-text-to-text model.\",\n    id: \"llava-hf/video-llava\"\n  }, {\n    description: \"A leaderboard for various video-text-to-text models.\",\n    id: \"opencompass/openvlm_video_leaderboard\"\n  }, {\n    description: \"An application to generate highlights from a video.\",\n    id: \"HuggingFaceTB/SmolVLM2-HighlightGenerator\"\n  }],\n  summary: \"Video-text-to-text models take in a video and a text prompt and output text. These models are also called video-language models.\",\n  widgetModels: [\"\"],\n  youtubeId: \"\"\n};\nexport default taskData;","map":{"version":3,"names":["taskData","datasets","description","id","demo","inputs","filename","type","label","content","outputs","metrics","models","spaces","summary","widgetModels","youtubeId"],"sources":["/Users/agmacbook/Documents/Courses/Meta - Full Stack/Exercises/meta_fullstack_exercises/6_react_basics/12_review/13_chef_claude/node_modules/@huggingface/tasks/dist/esm/tasks/video-text-to-text/data.js"],"sourcesContent":["const taskData = {\n    datasets: [\n        {\n            description: \"Multiple-choice questions and answers about videos.\",\n            id: \"lmms-lab/Video-MME\",\n        },\n        {\n            description: \"A dataset of instructions and question-answer pairs about videos.\",\n            id: \"lmms-lab/VideoChatGPT\",\n        },\n        {\n            description: \"Large video understanding dataset.\",\n            id: \"HuggingFaceFV/finevideo\",\n        },\n    ],\n    demo: {\n        inputs: [\n            {\n                filename: \"video-text-to-text-input.gif\",\n                type: \"img\",\n            },\n            {\n                label: \"Text Prompt\",\n                content: \"What is happening in this video?\",\n                type: \"text\",\n            },\n        ],\n        outputs: [\n            {\n                label: \"Answer\",\n                content: \"The video shows a series of images showing a fountain with water jets and a variety of colorful flowers and butterflies in the background.\",\n                type: \"text\",\n            },\n        ],\n    },\n    metrics: [],\n    models: [\n        {\n            description: \"A robust video-text-to-text model.\",\n            id: \"Vision-CAIR/LongVU_Qwen2_7B\",\n        },\n        {\n            description: \"Strong video-text-to-text model with reasoning capabilities.\",\n            id: \"GoodiesHere/Apollo-LMMs-Apollo-7B-t32\",\n        },\n        {\n            description: \"Strong video-text-to-text model.\",\n            id: \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\",\n        },\n    ],\n    spaces: [\n        {\n            description: \"An application to chat with a video-text-to-text model.\",\n            id: \"llava-hf/video-llava\",\n        },\n        {\n            description: \"A leaderboard for various video-text-to-text models.\",\n            id: \"opencompass/openvlm_video_leaderboard\",\n        },\n        {\n            description: \"An application to generate highlights from a video.\",\n            id: \"HuggingFaceTB/SmolVLM2-HighlightGenerator\",\n        },\n    ],\n    summary: \"Video-text-to-text models take in a video and a text prompt and output text. These models are also called video-language models.\",\n    widgetModels: [\"\"],\n    youtubeId: \"\",\n};\nexport default taskData;\n"],"mappings":"AAAA,MAAMA,QAAQ,GAAG;EACbC,QAAQ,EAAE,CACN;IACIC,WAAW,EAAE,qDAAqD;IAClEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,mEAAmE;IAChFC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,oCAAoC;IACjDC,EAAE,EAAE;EACR,CAAC,CACJ;EACDC,IAAI,EAAE;IACFC,MAAM,EAAE,CACJ;MACIC,QAAQ,EAAE,8BAA8B;MACxCC,IAAI,EAAE;IACV,CAAC,EACD;MACIC,KAAK,EAAE,aAAa;MACpBC,OAAO,EAAE,kCAAkC;MAC3CF,IAAI,EAAE;IACV,CAAC,CACJ;IACDG,OAAO,EAAE,CACL;MACIF,KAAK,EAAE,QAAQ;MACfC,OAAO,EAAE,4IAA4I;MACrJF,IAAI,EAAE;IACV,CAAC;EAET,CAAC;EACDI,OAAO,EAAE,EAAE;EACXC,MAAM,EAAE,CACJ;IACIV,WAAW,EAAE,oCAAoC;IACjDC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,8DAA8D;IAC3EC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,kCAAkC;IAC/CC,EAAE,EAAE;EACR,CAAC,CACJ;EACDU,MAAM,EAAE,CACJ;IACIX,WAAW,EAAE,yDAAyD;IACtEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,sDAAsD;IACnEC,EAAE,EAAE;EACR,CAAC,EACD;IACID,WAAW,EAAE,qDAAqD;IAClEC,EAAE,EAAE;EACR,CAAC,CACJ;EACDW,OAAO,EAAE,kIAAkI;EAC3IC,YAAY,EAAE,CAAC,EAAE,CAAC;EAClBC,SAAS,EAAE;AACf,CAAC;AACD,eAAehB,QAAQ","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}